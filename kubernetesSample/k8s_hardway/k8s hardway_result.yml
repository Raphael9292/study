Using username "root".
Authenticating with public key "rsa-key-20181025"
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-46-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

20 packages can be updated.
0 updates are security updates.

Last login: Thu Mar  7 22:44:16 2019 from 39.115.227.76
root@master:~# cat /etc/*~release
cat: '/etc/*~release': No such file or directory
root@master:~# cat /etc/*release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION="Ubuntu 16.04.5 LTS"
NAME="Ubuntu"
VERSION="16.04.5 LTS (Xenial Xerus)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 16.04.5 LTS"
VERSION_ID="16.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
root@master:~# kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d22h
root@master:~# # Create environment variable for correct distribution
root@master:~# export CLOUD_SDK_REPO="cloud-sdk-$(lsb_release -c -s)"
root@master:~#
root@master:~# # Add the Cloud SDK distribution URI as a package source
root@master:~# echo "deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
deb http://packages.cloud.google.com/apt cloud-sdk-xenial main
root@master:~#
root@master:~# # Import the Google Cloud Platform public key
root@master:~# curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1326  100  1326    0     0   3574      0 --:--:-- --:--:-- --:--:--  3583
OK
root@master:~#
root@master:~# # Update the package list and install the Cloud SDK
root@master:~# sudo apt-get update && sudo apt-get install google-cloud-sdk
Hit:1 http://kr.archive.ubuntu.com/ubuntu xenial InRelease
Get:2 http://kr.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Get:3 http://kr.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Hit:4 https://download.docker.com/linux/ubuntu xenial InRelease
Get:5 http://kr.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [920 kB]
Get:6 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease [6,372 B]
Get:7 http://kr.archive.ubuntu.com/ubuntu xenial-updates/main i386 Packages [804 kB]
Get:8 http://kr.archive.ubuntu.com/ubuntu xenial-updates/main amd64 DEP-11 Metadata [318 kB]
Get:9 http://kr.archive.ubuntu.com/ubuntu xenial-updates/main DEP-11 64x64 Icons [237 kB]
Get:10 http://kr.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [737 kB]
Get:11 http://kr.archive.ubuntu.com/ubuntu xenial-updates/universe i386 Packages [675 kB]
Get:12 http://kr.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 DEP-11 Metadata [252 kB]
Get:13 http://kr.archive.ubuntu.com/ubuntu xenial-updates/universe DEP-11 64x64 Icons [355 kB]
Get:14 http://kr.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 DEP-11 Metadata [5,964 B]
Get:15 http://kr.archive.ubuntu.com/ubuntu xenial-updates/multiverse DEP-11 64x64 Icons [14.3 kB]
Get:16 http://kr.archive.ubuntu.com/ubuntu xenial-backports/main amd64 DEP-11 Metadata [3,328 B]
Get:17 http://kr.archive.ubuntu.com/ubuntu xenial-backports/universe amd64 DEP-11 Metadata [5,104 B]
Get:18 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]
Get:19 http://packages.cloud.google.com/apt cloud-sdk-xenial/main amd64 Packages [65.4 kB]
Get:20 http://packages.cloud.google.com/apt cloud-sdk-xenial/main i386 Packages [65.1 kB]
Hit:21 https://apt.kubernetes.io kubernetes-xenial InRelease
Get:22 http://security.ubuntu.com/ubuntu xenial-security/main amd64 DEP-11 Metadata [67.9 kB]
Get:23 http://security.ubuntu.com/ubuntu xenial-security/main DEP-11 64x64 Icons [67.1 kB]
Get:24 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 DEP-11 Metadata [116 kB]
Get:25 http://security.ubuntu.com/ubuntu xenial-security/universe DEP-11 64x64 Icons [173 kB]
Get:26 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 DEP-11 Metadata [2,464 B]
Fetched 5,215 kB in 2s (1,884 kB/s)
Reading package lists... Done
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages were automatically installed and are no longer required:
  linux-headers-4.15.0-29 linux-headers-4.15.0-29-generic linux-image-4.15.0-29-generic linux-modules-4.15.0-29-generic linux-modules-extra-4.15.0-29-generic
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  python-crcmod
Suggested packages:
  google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-python google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-datastore-emulator
The following NEW packages will be installed:
  google-cloud-sdk python-crcmod
0 upgraded, 2 newly installed, 0 to remove and 19 not upgraded.
Need to get 21.1 MB of archives.
After this operation, 152 MB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://kr.archive.ubuntu.com/ubuntu xenial/universe amd64 python-crcmod amd64 1.7-2build2 [17.4 kB]
Get:2 http://packages.cloud.google.com/apt cloud-sdk-xenial/main amd64 google-cloud-sdk all 237.0.0-0 [21.0 MB]
Fetched 21.1 MB in 1s (12.8 MB/s)
Selecting previously unselected package python-crcmod.
(Reading database ... 254952 files and directories currently installed.)
Preparing to unpack .../python-crcmod_1.7-2build2_amd64.deb ...
Unpacking python-crcmod (1.7-2build2) ...
Selecting previously unselected package google-cloud-sdk.
Preparing to unpack .../google-cloud-sdk_237.0.0-0_all.deb ...
Unpacking google-cloud-sdk (237.0.0-0) ...
Processing triggers for man-db (2.7.5-1) ...
Setting up python-crcmod (1.7-2build2) ...
Setting up google-cloud-sdk (237.0.0-0) ...
root@master:~#
root@master:~#
root@master:~#
root@master:~# gcloud version
Google Cloud SDK 237.0.0
alpha 2019.03.01
beta 2019.03.01
bq 2.0.42
core 2019.03.01
gsutil 4.37
kubectl 2019.03.01
root@master:~#
root@master:~#
root@master:~#
root@master:~# gcloud init
Welcome! This command will take you through the configuration of gcloud.

Your current configuration has been set to: [default]

You can skip diagnostics next time by using the following flag:
  gcloud init --skip-diagnostics

Network diagnostic detects and fixes local network connection issues.
Checking network connection...done.
Reachability Check passed.
Network diagnostic passed (1/1 checks passed).

You must log in to continue. Would you like to log in (Y/n)?  Y

Go to the following link in your browser:

    https://accounts.google.com/o/oauth2/auth?redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&prompt=select_account&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2info.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reautess_type=offline


Enter verification code: 4/BwH-0TO79tSGJHLbdF7frdsG8xif9J5qeMdbSjJnSHh2yRea_DrvsqU
You are logged in as: [hnterkiller@gmail.com].

Pick cloud project to use:
 [1] velvety-column-233813
 [2] Create a new project
Please enter numeric choice or text value (must exactly match list
item):  1

Your current project has been set to: [velvety-column-233813].

Do you want to configure a default Compute Region and Zone? (Y/n)?  Y

Which Google Compute Engine zone would you like to use as project
default?
If you do not specify a zone via a command line flag while working
with Compute Engine resources, the default is assumed.
 [1] us-east1-b
 [2] us-east1-c
 [3] us-east1-d
 [4] us-east4-c
 [5] us-east4-b
 [6] us-east4-a
 [7] us-central1-c
 [8] us-central1-a
 [9] us-central1-f
 [10] us-central1-b
 [11] us-west1-b
 [12] us-west1-c
 [13] us-west1-a
 [14] europe-west4-a
 [15] europe-west4-b
 [16] europe-west4-c
 [17] europe-west1-b
 [18] europe-west1-d
 [19] europe-west1-c
 [20] europe-west3-c
 [21] europe-west3-a
 [22] europe-west3-b
 [23] europe-west2-c
 [24] europe-west2-b
 [25] europe-west2-a
 [26] asia-east1-b
 [27] asia-east1-a
 [28] asia-east1-c
 [29] asia-southeast1-b
 [30] asia-southeast1-a
 [31] asia-southeast1-c
 [32] asia-northeast1-b
 [33] asia-northeast1-c
 [34] asia-northeast1-a
 [35] asia-south1-c
 [36] asia-south1-b
 [37] asia-south1-a
 [38] australia-southeast1-b
 [39] australia-southeast1-c
 [40] australia-southeast1-a
 [41] southamerica-east1-b
 [42] southamerica-east1-c
 [43] southamerica-east1-a
 [44] asia-east2-a
 [45] asia-east2-b
 [46] asia-east2-c
 [47] europe-north1-a
 [48] europe-north1-b
 [49] europe-north1-c
 [50] europe-west6-a
Did not print [9] options.
Too many options [59]. Enter "list" at prompt to print choices fully.
Please enter numeric choice or text value (must exactly match list
item):  list
 [1] us-east1-b
 [2] us-east1-c
 [3] us-east1-d
 [4] us-east4-c
 [5] us-east4-b
 [6] us-east4-a
 [7] us-central1-c
 [8] us-central1-a
 [9] us-central1-f
 [10] us-central1-b
 [11] us-west1-b
 [12] us-west1-c
 [13] us-west1-a
 [14] europe-west4-a
 [15] europe-west4-b
 [16] europe-west4-c
 [17] europe-west1-b
 [18] europe-west1-d
 [19] europe-west1-c
 [20] europe-west3-c
 [21] europe-west3-a
 [22] europe-west3-b
 [23] europe-west2-c
 [24] europe-west2-b
 [25] europe-west2-a
 [26] asia-east1-b
 [27] asia-east1-a
 [28] asia-east1-c
 [29] asia-southeast1-b
 [30] asia-southeast1-a
 [31] asia-southeast1-c
 [32] asia-northeast1-b
 [33] asia-northeast1-c
 [34] asia-northeast1-a
 [35] asia-south1-c
 [36] asia-south1-b
 [37] asia-south1-a
 [38] australia-southeast1-b
 [39] australia-southeast1-c
 [40] australia-southeast1-a
 [41] southamerica-east1-b
 [42] southamerica-east1-c
 [43] southamerica-east1-a
 [44] asia-east2-a
 [45] asia-east2-b
 [46] asia-east2-c
 [47] europe-north1-a
 [48] europe-north1-b
 [49] europe-north1-c
 [50] europe-west6-a
 [51] europe-west6-b
 [52] europe-west6-c
 [53] northamerica-northeast1-a
 [54] northamerica-northeast1-b
 [55] northamerica-northeast1-c
 [56] us-west2-a
 [57] us-west2-b
 [58] us-west2-c
 [59] Do not set default zone
Please enter numeric choice or text value (must exactly match list
item):  26

Your project default Compute Engine zone has been set to [asia-east1-b].
You can change it by running [gcloud config set compute/zone NAME].

Your project default Compute Engine region has been set to [asia-east1].
You can change it by running [gcloud config set compute/region NAME].

Created a default .boto configuration file at [/root/.boto]. See this file and
[https://cloud.google.com/storage/docs/gsutil/commands/config] for more
information about configuring Google Cloud Storage.
Your Google Cloud SDK is configured and ready to use!

* Commands that require authentication will use hnterkiller@gmail.com by default
* Commands will reference project `velvety-column-233813` by default
* Compute Engine commands will use region `asia-east1` by default
* Compute Engine commands will use zone `asia-east1-b` by default

Run `gcloud help config` to learn how to change individual settings

This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.
Run `gcloud topic configurations` to learn more.

Some things to try next:

* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.
* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting
root@master:~# gcloud compute zones list
NAME                       REGION                   STATUS  NEXT_MAINTENANCE  TURNDOWN_DATE
us-east1-b                 us-east1                 UP
us-east1-c                 us-east1                 UP
us-east1-d                 us-east1                 UP
us-east4-c                 us-east4                 UP
us-east4-b                 us-east4                 UP
us-east4-a                 us-east4                 UP
us-central1-c              us-central1              UP
us-central1-a              us-central1              UP
us-central1-f              us-central1              UP
us-central1-b              us-central1              UP
us-west1-b                 us-west1                 UP
us-west1-c                 us-west1                 UP
us-west1-a                 us-west1                 UP
europe-west4-a             europe-west4             UP
europe-west4-b             europe-west4             UP
europe-west4-c             europe-west4             UP
europe-west1-b             europe-west1             UP
europe-west1-d             europe-west1             UP
europe-west1-c             europe-west1             UP
europe-west3-c             europe-west3             UP
europe-west3-a             europe-west3             UP
europe-west3-b             europe-west3             UP
europe-west2-c             europe-west2             UP
europe-west2-b             europe-west2             UP
europe-west2-a             europe-west2             UP
asia-east1-b               asia-east1               UP
asia-east1-a               asia-east1               UP
asia-east1-c               asia-east1               UP
asia-southeast1-b          asia-southeast1          UP
asia-southeast1-a          asia-southeast1          UP
asia-southeast1-c          asia-southeast1          UP
asia-northeast1-b          asia-northeast1          UP
asia-northeast1-c          asia-northeast1          UP
asia-northeast1-a          asia-northeast1          UP
asia-south1-c              asia-south1              UP
asia-south1-b              asia-south1              UP
asia-south1-a              asia-south1              UP
australia-southeast1-b     australia-southeast1     UP
australia-southeast1-c     australia-southeast1     UP
australia-southeast1-a     australia-southeast1     UP
southamerica-east1-b       southamerica-east1       UP
southamerica-east1-c       southamerica-east1       UP
southamerica-east1-a       southamerica-east1       UP
asia-east2-a               asia-east2               UP
asia-east2-b               asia-east2               UP
asia-east2-c               asia-east2               UP
europe-north1-a            europe-north1            UP
europe-north1-b            europe-north1            UP
europe-north1-c            europe-north1            UP
europe-west6-a             europe-west6             UP
europe-west6-b             europe-west6             UP
europe-west6-c             europe-west6             UP
northamerica-northeast1-a  northamerica-northeast1  UP
northamerica-northeast1-b  northamerica-northeast1  UP
northamerica-northeast1-c  northamerica-northeast1  UP
us-west2-a                 us-west2                 UP
us-west2-b                 us-west2                 UP
us-west2-c                 us-west2                 UP
root@master:~#
root@master:~#
root@master:~# gcloud config configurations activate
ERROR: (gcloud.config.configurations.activate) argument CONFIGURATION_NAME: Must be specified.
Usage: gcloud config configurations activate CONFIGURATION_NAME [optional flags]
  optional flags may be  --help

For detailed information on this command and its flags, run:
  gcloud config configurations activate --help
root@master:~#
root@master:~#
root@master:~# gcloud config configurations list
NAME     IS_ACTIVE  ACCOUNT                PROJECT                DEFAULT_ZONE  DEFAULT_REGION
default  True       hnterkiller@gmail.com  velvety-column-233813  asia-east1-b  asia-east1
root@master:~# date
2019. 03. 08. (금) 09:51:35 KST
root@master:~# wget -q --show-progress --https-only --timestamping \
>   https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \
>   https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
cfssl_linux-amd64                                           100%[========================================================================================================================================>]   9.90M  5.84MB/s    in 1.7s
cfssljson_linux-amd64                                       100%[========================================================================================================================================>]   2.17M  1.89MB/s    in 1.1s
root@master:~# ls
\  calico.yaml  cfssljson_linux-amd64  cfssl_linux-amd64  default.etcd  go  go1.11.5.linux-amd64.tar.gz  kubeadm-config.yaml  kubernetes-src.tar.gz  pkg  snap  test  yaml
root@master:~# ll
total 177424
drwx------ 15 root root      4096  3월  8 09:52 ./
drwxr-xr-x 24 root root      4096  3월  6 06:23 ../
-rw-r--r--  1 root root      3162  2월 22 13:22 \
-rw-------  1 root root     61861  3월  7 22:46 .bash_history
-rw-r--r--  1 root root        76  2월 13 20:12 .bash_profile
-rw-r--r--  1 root root      3162  2월 22 13:22 .bashrc
-rw-------  1 root root     18028  3월  8 09:50 .boto
drwx------  3 root root      4096  2월 19 15:36 .cache/
-rw-r--r--  1 root root     13501  3월  4 00:41 calico.yaml
-rw-r--r--  1 root root   2277873  3월 30  2016 cfssljson_linux-amd64
-rw-r--r--  1 root root  10376657  3월 30  2016 cfssl_linux-amd64
drwxr-xr-x  3 root root      4096  3월  8 09:46 .config/
drwx------  3 root root      4096  2월 22 16:50 default.etcd/
drwxr-xr-x  4 root root      4096  2월 19 18:45 go/
-rw-r--r--  1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
drwxr-xr-x  4 root root      4096  2월 18 16:12 .kube/
-rw-r--r--  1 root root       299  2월 27 12:54 kubeadm-config.yaml
-rw-r--r--  1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
drwx------  3 root root      4096  2월 28 11:23 .local/
drwxr-xr-x  2 root root      4096  2월 28 10:51 .nano/
drwxr-xr-x  3 root root      4096  2월 20 14:49 pkg/
-rw-r--r--  1 root root       148  8월 18  2015 .profile
drwxr-xr-x  4 root root      4096  2월 28 11:20 snap/
drwxr-xr-x  2 root root      4096  3월  4 16:28 .ssh/
drwxr-xr-x  2 root root      4096  3월  3 23:34 test/
drwxr-xr-x  2 root root      4096  2월 26 18:24 .vim/
-rw-------  1 root root      7785  3월  4 11:21 .viminfo
-rw-r--r--  1 root root       167  3월  8 09:52 .wget-hsts
drwxr-xr-x  2 root root      4096  2월 27 12:46 yaml/
root@master:~# chmod +x cfssl*
root@master:~# ll
total 177424
drwx------ 15 root root      4096  3월  8 09:52 ./
drwxr-xr-x 24 root root      4096  3월  6 06:23 ../
-rw-r--r--  1 root root      3162  2월 22 13:22 \
-rw-------  1 root root     61861  3월  7 22:46 .bash_history
-rw-r--r--  1 root root        76  2월 13 20:12 .bash_profile
-rw-r--r--  1 root root      3162  2월 22 13:22 .bashrc
-rw-------  1 root root     18028  3월  8 09:50 .boto
drwx------  3 root root      4096  2월 19 15:36 .cache/
-rw-r--r--  1 root root     13501  3월  4 00:41 calico.yaml
-rwxr-xr-x  1 root root   2277873  3월 30  2016 cfssljson_linux-amd64*
-rwxr-xr-x  1 root root  10376657  3월 30  2016 cfssl_linux-amd64*
drwxr-xr-x  3 root root      4096  3월  8 09:46 .config/
drwx------  3 root root      4096  2월 22 16:50 default.etcd/
drwxr-xr-x  4 root root      4096  2월 19 18:45 go/
-rw-r--r--  1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
drwxr-xr-x  4 root root      4096  2월 18 16:12 .kube/
-rw-r--r--  1 root root       299  2월 27 12:54 kubeadm-config.yaml
-rw-r--r--  1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
drwx------  3 root root      4096  2월 28 11:23 .local/
drwxr-xr-x  2 root root      4096  2월 28 10:51 .nano/
drwxr-xr-x  3 root root      4096  2월 20 14:49 pkg/
-rw-r--r--  1 root root       148  8월 18  2015 .profile
drwxr-xr-x  4 root root      4096  2월 28 11:20 snap/
drwxr-xr-x  2 root root      4096  3월  4 16:28 .ssh/
drwxr-xr-x  2 root root      4096  3월  3 23:34 test/
drwxr-xr-x  2 root root      4096  2월 26 18:24 .vim/
-rw-------  1 root root      7785  3월  4 11:21 .viminfo
-rw-r--r--  1 root root       167  3월  8 09:52 .wget-hsts
drwxr-xr-x  2 root root      4096  2월 27 12:46 yaml/
root@master:~# mv cfssl_linux-amd64 /usr/local/bin/cfssl
root@master:~# mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
root@master:~#
root@master:~#
root@master:~# cfssl version
Version: 1.2.0
Revision: dev
Runtime: go1.6
root@master:~#
root@master:~#
root@master:~# kubeadm reset
[reset] WARNING: changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] are you sure you want to proceed? [y/N]: y
[preflight] running pre-flight checks
[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[reset] stopping the kubelet service
[reset] unmounting mounted directories in "/var/lib/kubelet"
[reset] deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
[reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually.
For example:
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

root@master:~# iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
root@master:~# systemctl restart kubelet docker
root@master:~# ssh master2
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

20 packages can be updated.
0 updates are security updates.

*** System restart required ***
Last login: Mon Mar  4 11:23:54 2019 from 10.0.0.77



root@master2:~#
root@master2:~#
root@master2:~#
root@master2:~# kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d22h
root@master2:~# kubectl get nodes
NAME      STATUS   ROLES         AGE     VERSION
master    Ready    etcd,master   3d22h   v1.13.3
master2   Ready    etcd,master   3d22h   v1.13.3
master3   Ready    etcd,master   3d22h   v1.13.3
worker    Ready    worker        3d22h   v1.13.3
root@master2:~# kubectl describe node master
Name:               master
Roles:              etcd,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    etcd=true
                    kubernetes.io/hostname=master
                    master=true
                    node-role.kubernetes.io/etcd=true
                    node-role.kubernetes.io/master=true
                    role=master
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 10.0.0.77/24
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 04 Mar 2019 11:22:33 +0900
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Wed, 06 Mar 2019 09:38:48 +0900   Wed, 06 Mar 2019 09:38:48 +0900   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 08 Mar 2019 09:53:34 +0900   Mon, 04 Mar 2019 11:22:30 +0900   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 08 Mar 2019 09:53:34 +0900   Mon, 04 Mar 2019 11:22:30 +0900   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 08 Mar 2019 09:53:34 +0900   Mon, 04 Mar 2019 11:22:30 +0900   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 08 Mar 2019 09:53:34 +0900   Mon, 04 Mar 2019 11:25:28 +0900   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  10.0.0.77
  Hostname:    master
Capacity:
 cpu:                8
 ephemeral-storage:  50487988Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             8167648Ki
 pods:               110
Allocatable:
 cpu:                8
 ephemeral-storage:  46529729664
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             8065248Ki
 pods:               110
System Info:
 Machine ID:                 737d2eec0f704e08857c42edfcb3f668
 System UUID:                21614D56-2790-AC45-5790-24B41713FDD5
 Boot ID:                    dcbd88ea-2211-4870-83a3-f41a0c62e9c7
 Kernel Version:             4.15.0-46-generic
 OS Image:                   Ubuntu 16.04.5 LTS
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://18.9.2
 Kubelet Version:            v1.13.3
 Kube-Proxy Version:         v1.13.3
PodCIDR:                     192.168.0.0/24
Non-terminated Pods:         (8 in total)
  Namespace                  Name                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                  ----                              ------------  ----------  ---------------  -------------  ---
  kube-system                calico-node-n2wkq                 250m (3%)     0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                coredns-86c58d9df4-488kd          100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     3d22h
  kube-system                coredns-86c58d9df4-nz7dc          100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     3d22h
  kube-system                etcd-master                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                kube-apiserver-master             250m (3%)     0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                kube-controller-manager-master    200m (2%)     0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                kube-proxy-rm6j2                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                kube-scheduler-master             100m (1%)     0 (0%)      0 (0%)           0 (0%)         3d22h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1 (12%)     0 (0%)
  memory             140Mi (1%)  340Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
Events:              <none>
root@master2:~# exit
logout
Connection to master2 closed.
root@master:~# kubectl get all
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@master:~# ssh master2

Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

20 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

*** System restart required ***
Last login: Fri Mar  8 09:53:59 2019 from 10.0.0.77

root@master2:~#
root@master2:~#
root@master2:~#
root@master2:~# kubectl cluster-info
Kubernetes master is running at https://10.0.0.89:16443
KubeDNS is running at https://10.0.0.89:16443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
root@master2:~# ssh master3
root@master3's password:
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-46-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

20 packages can be updated.
0 updates are security updates.

Last login: Mon Mar  4 11:24:19 2019 from 10.0.0.77
root@master3:~#
root@master3:~#
root@master3:~# exit
logout
Connection to master3 closed.
root@master2:~# reboot
Connection to master2 closed by remote host.
Connection to master2 closed.
root@master:~# kubectl version
Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:08:12Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"lamd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@master:~# kubectl version --client
Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:08:12Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"lamd64"}
root@master:~# gcloud compute networks create kubernetes-the-hard-way --subnet-mode custom

Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/networks/kubernetes-the-hard-way].
NAME                     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4
kubernetes-the-hard-way  CUSTOM       REGIONAL

Instances on this network will not be reachable until firewall rules
are created. As an example, you can allow all internal traffic between
instances as well as SSH, RDP, and ICMP by running:

$ gcloud compute firewall-rules create <FIREWALL_NAME> --network kubernetes-the-hard-way --allow tcp,udp,icmp --source-ranges <IP_RANGE>
$ gcloud compute firewall-rules create <FIREWALL_NAME> --network kubernetes-the-hard-way --allow tcp:22,tcp:3389,icmp

root@master:~#
root@master:~# gcloud compute networks subnets create kubernetes \
>   --network kubernetes-the-hard-way \
>   --range 10.240.0.0/24
Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/regions/asia-east1/subnetworks/kubernetes].
NAME        REGION      NETWORK                  RANGE
kubernetes  asia-east1  kubernetes-the-hard-way  10.240.0.0/24
root@master:~#
root@master:~#
root@master:~# gcloud compute firewall-rules create kubernetes-the-hard-way-allow-external \
>   --allow tcp:22,tcp:6443,icmp \
>   --network kubernetes-the-hard-way \
>   --source-ranges 0.0.0.0/0
Creating firewall...⠼Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/firewalls/kubernetes-the-hard-way-allow-external].
Creating firewall...done.
NAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW                 DENY  DISABLED
kubernetes-the-hard-way-allow-external  kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp        False
root@master:~# gcloud compute firewall-rules list --filter="network:kubernetes-the-hard-way"
NAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW                 DENY  DISABLED
kubernetes-the-hard-way-allow-external  kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp        False

To show all fields of the firewall, please show in JSON format: --format=json
To show all fields in table format, please see the examples in --help.

root@master:~#
root@master:~#
root@master:~#
root@master:~# gcloud compute addresses create kubernetes-the-hard-way \
>   --region $(gcloud config get-value compute/region)
Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/regions/asia-east1/addresses/kubernetes-the-hard-way].
root@master:~# gcloud compute addresses list --filter="name=('kubernetes-the-hard-way')"
NAME                     ADDRESS/RANGE  TYPE  PURPOSE  NETWORK  REGION      SUBNET  STATUS
kubernetes-the-hard-way  34.80.89.118                           asia-east1          RESERVED
root@master:~# for i in 0 1 2; do
>   gcloud compute instances create controller-${i} \
>     --async \
>     --boot-disk-size 200GB \
>     --can-ip-forward \
>     --image-family ubuntu-1604-lts \
>     --image-project ubuntu-os-cloud \
>     --machine-type n1-standard-1 \
>     --private-network-ip 10.240.0.1${i} \
>     --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
>     --subnet kubernetes \
>     --tags kubernetes-the-hard-way,controller
> done
Instance creation in progress for [controller-0]: https://www.googleapis.com/compute/v1/projects/velvety-column-233813/zones/asia-east1-b/operations/operation-1552006892217-5838ac37fe2c3-0f6675cf-6596c707
Use [gcloud compute operations describe URI] command to check the status of the operation(s).
Instance creation in progress for [controller-1]: https://www.googleapis.com/compute/v1/projects/velvety-column-233813/zones/asia-east1-b/operations/operation-1552006895554-5838ac3b2d0e3-8408c03c-6f50757c
Use [gcloud compute operations describe URI] command to check the status of the operation(s).
Instance creation in progress for [controller-2]: https://www.googleapis.com/compute/v1/projects/velvety-column-233813/zones/asia-east1-b/operations/operation-1552006898246-5838ac3dbe438-b54fd390-6bd0af75
Use [gcloud compute operations describe URI] command to check the status of the operation(s).
root@master:~# for i in 0 1 2; do
>   gcloud compute instances create worker-${i} \
>     --async \
>     --boot-disk-size 200GB \
>     --can-ip-forward \
>     --image-family ubuntu-1804-lts \
>     --image-project ubuntu-os-cloud \
>     --machine-type n1-standard-1 \
>     --metadata pod-cidr=10.200.${i}.0/24 \
>     --private-network-ip 10.240.0.2${i} \
>     --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
>     --subnet kubernetes \
>     --tags kubernetes-the-hard-way,worker
> done
Instance creation in progress for [worker-0]: https://www.googleapis.com/compute/v1/projects/velvety-column-233813/zones/asia-east1-b/operations/operation-1552006915858-5838ac4e89ed5-38d83496-86ec6ff0
Use [gcloud compute operations describe URI] command to check the status of the operation(s).
Instance creation in progress for [worker-1]: https://www.googleapis.com/compute/v1/projects/velvety-column-233813/zones/asia-east1-b/operations/operation-1552006918447-5838ac5102238-16bdb85c-e0127810
Use [gcloud compute operations describe URI] command to check the status of the operation(s).
Instance creation in progress for [worker-2]: https://www.googleapis.com/compute/v1/projects/velvety-column-233813/zones/asia-east1-b/operations/operation-1552006921080-5838ac5384de9-ca63af21-39c61385
Use [gcloud compute operations describe URI] command to check the status of the operation(s).
root@master:~#
root@master:~#
root@master:~# gcloud compute instances list
NAME          ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS
controller-0  asia-east1-b  n1-standard-1               10.240.0.10  34.80.25.2    RUNNING
controller-1  asia-east1-b  n1-standard-1               10.240.0.11  34.80.85.175  RUNNING
controller-2  asia-east1-b  n1-standard-1               10.240.0.12  34.80.89.164  RUNNING
worker-0      asia-east1-b  n1-standard-1               10.240.0.20  34.80.0.204   RUNNING
worker-1      asia-east1-b  n1-standard-1               10.240.0.21  34.80.34.179  RUNNING
worker-2      asia-east1-b  n1-standard-1               10.240.0.22  34.80.89.105  RUNNING
root@master:~# gcloud compute instances list -o wide
WARNING: Argument `NAME` is deprecated. Use `--filter="name=( 'NAME' ... )"` instead.
ERROR: (gcloud.compute.instances.list) unrecognized arguments: -o

To search the help text of gcloud commands, run:
  gcloud help -- SEARCH_TERMS
root@master:~#
root@master:~#
root@master:~# gcloud compute ssh controller-0
WARNING: The public SSH key file for gcloud does not exist.
WARNING: The private SSH key file for gcloud does not exist.
WARNING: You do not have an SSH key for gcloud.
WARNING: SSH keygen will be executed to generate a key.
Generating public/private rsa key pair.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/google_compute_engine.
Your public key has been saved in /root/.ssh/google_compute_engine.pub.
The key fingerprint is:
SHA256:sCP9CujzG2suDi/6Qf/uZTGZ8SjqAj5depZkBjzlWQo root@master
The key's randomart image is:
+---[RSA 2048]----+
|                 |
|   E . .         |
|  . + = .        |
|   + = o *       |
|  . + = S .      |
|.. o B + o       |
|o.+ @ . +        |
|.*+=.O +         |
|o+*BO+=          |
+----[SHA256]-----+
Updating project ssh metadata...⠏Updated [https://www.googleapis.com/compute/v1/projects/velvety-column-233813].
Updating project ssh metadata...done.
Waiting for SSH key to propagate.
Warning: Permanently added 'compute.1938167585789706243' (ECDSA) to the list of known hosts.
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.


root@controller-0:~# ls
root@controller-0:~# free -h
              total        used        free      shared  buff/cache   available
Mem:           3.6G        124M        3.2G        5.2M        241M        3.2G
Swap:            0B          0B          0B
root@controller-0:~# grep . /etc/*~release
grep: /etc/*~release: No such file or directory
root@controller-0:~# grep . /etc/*release
/etc/lsb-release:DISTRIB_ID=Ubuntu
/etc/lsb-release:DISTRIB_RELEASE=16.04
/etc/lsb-release:DISTRIB_CODENAME=xenial
/etc/lsb-release:DISTRIB_DESCRIPTION="Ubuntu 16.04.6 LTS"
/etc/os-release:NAME="Ubuntu"
/etc/os-release:VERSION="16.04.6 LTS (Xenial Xerus)"
/etc/os-release:ID=ubuntu
/etc/os-release:ID_LIKE=debian
/etc/os-release:PRETTY_NAME="Ubuntu 16.04.6 LTS"
/etc/os-release:VERSION_ID="16.04"
/etc/os-release:HOME_URL="http://www.ubuntu.com/"
/etc/os-release:SUPPORT_URL="http://help.ubuntu.com/"
/etc/os-release:BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
/etc/os-release:VERSION_CODENAME=xenial
/etc/os-release:UBUNTU_CODENAME=xenial
root@controller-0:~# exit
logout
Connection to 34.80.25.2 closed.
root@master:~# gcloud compute list
ERROR: (gcloud.compute) Invalid choice: 'list'.
Maybe you meant:
  gcloud compute instances list
  gcloud compute addresses list
  gcloud compute firewall-rules list
  gcloud compute zones list
  gcloud compute networks list
  gcloud compute regions list
  gcloud compute accelerator-types list
  gcloud compute backend-buckets list
  gcloud compute backend-services list
  gcloud compute commitments list

Showing 10 out of 493 suggestions.

To search the help text of gcloud commands, run:
  gcloud help -- SEARCH_TERMS
root@master:~# gcloud compute instances list
NAME          ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS
controller-0  asia-east1-b  n1-standard-1               10.240.0.10  34.80.25.2    RUNNING
controller-1  asia-east1-b  n1-standard-1               10.240.0.11  34.80.85.175  RUNNING
controller-2  asia-east1-b  n1-standard-1               10.240.0.12  34.80.89.164  RUNNING
worker-0      asia-east1-b  n1-standard-1               10.240.0.20  34.80.0.204   RUNNING
worker-1      asia-east1-b  n1-standard-1               10.240.0.21  34.80.34.179  RUNNING
worker-2      asia-east1-b  n1-standard-1               10.240.0.22  34.80.89.105  RUNNING
root@master:~# {
>
> cat > ca-config.json <<EOF
> {
>   "signing": {
>     "default": {
>       "expiry": "8760h"
>     },
>     "profiles": {
>       "kubernetes": {
>         "usages": ["signing", "key encipherment", "server auth", "client auth"],
>         "expiry": "8760h"
>       }
>     }
>   }
> }
> EOF
>
> cat > ca-csr.json <<EOF
> {
>   "CN": "Kubernetes",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "Kubernetes",
>       "OU": "CA",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> cfssl gencert -initca ca-csr.json | cfssljson -bare ca
>
> }
2019/03/08 10:08:17 [INFO] generating a new CA key and certificate from CSR
2019/03/08 10:08:17 [INFO] generate received request
2019/03/08 10:08:17 [INFO] received CSR
2019/03/08 10:08:17 [INFO] generating key: rsa-2048
2019/03/08 10:08:18 [INFO] encoded CSR
2019/03/08 10:08:18 [INFO] signed certificate with serial number 63537674933801368838975150603305963650221741053
root@master:~# ls
\  ca-config.json  ca.csr  ca-csr.json  ca-key.pem  calico.yaml  ca.pem  default.etcd  go  go1.11.5.linux-amd64.tar.gz  kubeadm-config.yaml  kubernetes-src.tar.gz  pkg  snap  test  yaml
root@master:~# ll
total 165080
drwx------ 15 root root      4096  3월  8 10:08 ./
drwxr-xr-x 24 root root      4096  3월  6 06:23 ../
-rw-r--r--  1 root root      3162  2월 22 13:22 \
-rw-------  1 root root     61861  3월  7 22:46 .bash_history
-rw-r--r--  1 root root        76  2월 13 20:12 .bash_profile
-rw-r--r--  1 root root      3162  2월 22 13:22 .bashrc
-rw-------  1 root root     18028  3월  8 09:50 .boto
drwx------  3 root root      4096  2월 19 15:36 .cache/
-rw-r--r--  1 root root       232  3월  8 10:08 ca-config.json
-rw-r--r--  1 root root      1005  3월  8 10:08 ca.csr
-rw-r--r--  1 root root       211  3월  8 10:08 ca-csr.json
-rw-------  1 root root      1679  3월  8 10:08 ca-key.pem
-rw-r--r--  1 root root     13501  3월  4 00:41 calico.yaml
-rw-r--r--  1 root root      1367  3월  8 10:08 ca.pem
drwxr-xr-x  3 root root      4096  3월  8 09:46 .config/
drwx------  3 root root      4096  2월 22 16:50 default.etcd/
drwxr-xr-x  4 root root      4096  2월 19 18:45 go/
-rw-r--r--  1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
drwxr-xr-x  4 root root      4096  2월 18 16:12 .kube/
-rw-r--r--  1 root root       299  2월 27 12:54 kubeadm-config.yaml
-rw-r--r--  1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
drwx------  3 root root      4096  2월 28 11:23 .local/
drwxr-xr-x  2 root root      4096  2월 28 10:51 .nano/
drwxr-xr-x  3 root root      4096  2월 20 14:49 pkg/
-rw-r--r--  1 root root       148  8월 18  2015 .profile
drwxr-xr-x  4 root root      4096  2월 28 11:20 snap/
drwxr-xr-x  2 root root      4096  3월  8 10:03 .ssh/
drwxr-xr-x  2 root root      4096  3월  3 23:34 test/
drwxr-xr-x  2 root root      4096  2월 26 18:24 .vim/
-rw-------  1 root root      7785  3월  4 11:21 .viminfo
-rw-r--r--  1 root root       167  3월  8 09:52 .wget-hsts
drwxr-xr-x  2 root root      4096  2월 27 12:46 yaml/
root@master:~# ls
\  ca-config.json  ca.csr  ca-csr.json  ca-key.pem  calico.yaml  ca.pem  default.etcd  go  go1.11.5.linux-amd64.tar.gz  kubeadm-config.yaml  kubernetes-src.tar.gz  pkg  snap  test  yaml
root@master:~# ls -l
total 164936
-rw-r--r-- 1 root root      3162  2월 22 13:22 \
-rw-r--r-- 1 root root       232  3월  8 10:08 ca-config.json
-rw-r--r-- 1 root root      1005  3월  8 10:08 ca.csr
-rw-r--r-- 1 root root       211  3월  8 10:08 ca-csr.json
-rw------- 1 root root      1679  3월  8 10:08 ca-key.pem
-rw-r--r-- 1 root root     13501  3월  4 00:41 calico.yaml
-rw-r--r-- 1 root root      1367  3월  8 10:08 ca.pem
drwx------ 3 root root      4096  2월 22 16:50 default.etcd
drwxr-xr-x 4 root root      4096  2월 19 18:45 go
-rw-r--r-- 1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
-rw-r--r-- 1 root root       299  2월 27 12:54 kubeadm-config.yaml
-rw-r--r-- 1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
drwxr-xr-x 3 root root      4096  2월 20 14:49 pkg
drwxr-xr-x 4 root root      4096  2월 28 11:20 snap
drwxr-xr-x 2 root root      4096  3월  3 23:34 test
drwxr-xr-x 2 root root      4096  2월 27 12:46 yaml
root@master:~# mv calico.yaml yaml/
root@master:~# ls -l
total 164920
-rw-r--r-- 1 root root      3162  2월 22 13:22 \
-rw-r--r-- 1 root root       232  3월  8 10:08 ca-config.json
-rw-r--r-- 1 root root      1005  3월  8 10:08 ca.csr
-rw-r--r-- 1 root root       211  3월  8 10:08 ca-csr.json
-rw------- 1 root root      1679  3월  8 10:08 ca-key.pem
-rw-r--r-- 1 root root      1367  3월  8 10:08 ca.pem
drwx------ 3 root root      4096  2월 22 16:50 default.etcd
drwxr-xr-x 4 root root      4096  2월 19 18:45 go
-rw-r--r-- 1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
-rw-r--r-- 1 root root       299  2월 27 12:54 kubeadm-config.yaml
-rw-r--r-- 1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
drwxr-xr-x 3 root root      4096  2월 20 14:49 pkg
drwxr-xr-x 4 root root      4096  2월 28 11:20 snap
drwxr-xr-x 2 root root      4096  3월  3 23:34 test
drwxr-xr-x 2 root root      4096  3월  8 10:08 yaml
root@master:~# mv kubeadm-config.yaml yaml/
root@master:~# ls -l
total 164916
-rw-r--r-- 1 root root      3162  2월 22 13:22 \
-rw-r--r-- 1 root root       232  3월  8 10:08 ca-config.json
-rw-r--r-- 1 root root      1005  3월  8 10:08 ca.csr
-rw-r--r-- 1 root root       211  3월  8 10:08 ca-csr.json
-rw------- 1 root root      1679  3월  8 10:08 ca-key.pem
-rw-r--r-- 1 root root      1367  3월  8 10:08 ca.pem
drwx------ 3 root root      4096  2월 22 16:50 default.etcd
drwxr-xr-x 4 root root      4096  2월 19 18:45 go
-rw-r--r-- 1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
-rw-r--r-- 1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
drwxr-xr-x 3 root root      4096  2월 20 14:49 pkg
drwxr-xr-x 4 root root      4096  2월 28 11:20 snap
drwxr-xr-x 2 root root      4096  3월  3 23:34 test
drwxr-xr-x 2 root root      4096  3월  8 10:09 yaml
root@master:~#
root@master:~#
root@master:~#
root@master:~# cat ca-config.json
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
root@master:~# cat ca.csr
-----BEGIN CERTIFICATE REQUEST-----
MIICrTCCAZUCAQAwaDELMAkGA1UEBhMCVVMxDzANBgNVBAgTBk9yZWdvbjERMA8G
A1UEBxMIUG9ydGxhbmQxEzARBgNVBAoTCkt1YmVybmV0ZXMxCzAJBgNVBAsTAkNB
MRMwEQYDVQQDEwpLdWJlcm5ldGVzMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB
CgKCAQEA0x/gFGi/b7eH8nfqu1R1nGgyyomEcc7WH7wDopYX2U1Azr5pIDdkmfRt
9yAr4H+ct4/0UgUS9TysdPmPmxK5GqYI9JJRs95cxRnoKVB9nvW7RLMSHik6hWPS
KVz2HqGiRLaoIKTEWcaFpiPPaG+rM63JL501DqqEo6oiBF5vbMIX9sHYYWY8et7E
D4AuPAmbddlZX9aV78hDPL7yS1qGObt6cs1Ji9U5RvltZvkUe8HzQBEc2l20gVZ0
RO08UEic/IAN0bP6gKvibMRh9uh6c9LpdpPM22mIXHNzXX+2veyttSFJ2UFziAJy
n0FEGyrog+A+Ih7PDbPEiNQRhFsGxQIDAQABoAAwDQYJKoZIhvcNAQELBQADggEB
AA7GNefgWLB4wvZ1UJCvLWMNNhxffg89zlV/NP+WNPXGzqCLSLhwEMVqks3HUUHy
QAjgXvfQvPmYa1uchpBwMPcsFIjLeZb01mZ0g0pYmI4ubYxNTjK0zrWMJP+tn5y8
rXxQXysAEaUKpMTemrkvygAWm5xpXD6fOS2ciBnOmKQ6eNZ4XkzzP7QX0PvhvtiL
0uyodGgcq1o+qbGoMOusUDPaiYgRI2iQXhEC9nRZc7cjO8WNC4dpZB/M06/dF4dG
/VdF1+/UcHDg5YgTmqk8FhcP/6H2wsoLGBY8pIgVNydruFAiEM8hJWI/DhbFBuVi
D1YUKPmwYJEXi/hnf2OOfyg=
-----END CERTIFICATE REQUEST-----
root@master:~# {
>
> cat > admin-csr.json <<EOF
> {
>   "CN": "admin",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "system:masters",
>       "OU": "Kubernetes The Hard Way",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> cfssl gencert \
>   -ca=ca.pem \
>   -ca-key=ca-key.pem \
>   -config=ca-config.json \
>   -profile=kubernetes \
>   admin-csr.json | cfssljson -bare admin
>
> }
2019/03/08 10:10:09 [INFO] generate received request
2019/03/08 10:10:09 [INFO] received CSR
2019/03/08 10:10:09 [INFO] generating key: rsa-2048
2019/03/08 10:10:09 [INFO] encoded CSR
2019/03/08 10:10:09 [INFO] signed certificate with serial number 322110566896226027412747738559340882871675434834
2019/03/08 10:10:09 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
root@master:~# ls
\  admin.csr  admin-csr.json  admin-key.pem  admin.pem  ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  default.etcd  go  go1.11.5.linux-amd64.tar.gz  kubernetes-src.tar.gz  pkg  snap  test  yaml
root@master:~# for instance in worker-0 worker-1 worker-2; do
> cat > ${instance}-csr.json <<EOF
> {
>   "CN": "system:node:${instance}",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "system:nodes",
>       "OU": "Kubernetes The Hard Way",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> EXTERNAL_IP=$(gcloud compute instances describe ${instance} \
>   --format 'value(networkInterfaces[0].accessConfigs[0].natIP)')
>
> INTERNAL_IP=$(gcloud compute instances describe ${instance} \
>   --format 'value(networkInterfaces[0].networkIP)')
>
> cfssl gencert \
>   -ca=ca.pem \
>   -ca-key=ca-key.pem \
>   -config=ca-config.json \
>   -hostname=${instance},${EXTERNAL_IP},${INTERNAL_IP} \
>   -profile=kubernetes \
>   ${instance}-csr.json | cfssljson -bare ${instance}
> done
2019/03/08 10:12:13 [INFO] generate received request
2019/03/08 10:12:13 [INFO] received CSR
2019/03/08 10:12:13 [INFO] generating key: rsa-2048
2019/03/08 10:12:13 [INFO] encoded CSR
2019/03/08 10:12:13 [INFO] signed certificate with serial number 442828253075337352345974711135833119022944559529
2019/03/08 10:12:15 [INFO] generate received request
2019/03/08 10:12:15 [INFO] received CSR
2019/03/08 10:12:15 [INFO] generating key: rsa-2048
2019/03/08 10:12:15 [INFO] encoded CSR
2019/03/08 10:12:15 [INFO] signed certificate with serial number 631273401067634293040104195957703073112849775185
2019/03/08 10:12:17 [INFO] generate received request
2019/03/08 10:12:17 [INFO] received CSR
2019/03/08 10:12:17 [INFO] generating key: rsa-2048
2019/03/08 10:12:17 [INFO] encoded CSR
2019/03/08 10:12:17 [INFO] signed certificate with serial number 140080151565825713005957047024294622197764164562
root@master:~# ls
\          admin-csr.json  admin.pem       ca.csr       ca-key.pem  default.etcd  go1.11.5.linux-amd64.tar.gz  pkg   test          worker-0-csr.json  worker-0.pem  worker-1-csr.json  worker-1.pem  worker-2-csr.json  worker-2.pem
admin.csr  admin-key.pem   ca-config.json  ca-csr.json  ca.pem      go            kubernetes-src.tar.gz        snap  worker-0.csr  worker-0-key.pem   worker-1.csr  worker-1-key.pem   worker-2.csr  worker-2-key.pem   yaml
root@master:~#
root@master:~#
root@master:~# {
>
> cat > kube-controller-manager-csr.json <<EOF
> {
>   "CN": "system:kube-controller-manager",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "system:kube-controller-manager",
>       "OU": "Kubernetes The Hard Way",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> cfssl gencert \
>   -ca=ca.pem \
>   -ca-key=ca-key.pem \
>   -config=ca-config.json \
>   -profile=kubernetes \
>   kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
>
> }
2019/03/08 10:13:31 [INFO] generate received request
2019/03/08 10:13:31 [INFO] received CSR
2019/03/08 10:13:31 [INFO] generating key: rsa-2048
2019/03/08 10:13:32 [INFO] encoded CSR
2019/03/08 10:13:32 [INFO] signed certificate with serial number 276365051591965301943671847976250361222291810425
2019/03/08 10:13:32 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
root@master:~# ls
\               admin-key.pem   ca.csr       ca.pem        go1.11.5.linux-amd64.tar.gz       kube-controller-manager-key.pem  pkg   worker-0.csr       worker-0.pem       worker-1-key.pem  worker-2-csr.json  yaml
admin.csr       admin.pem       ca-csr.json  default.etcd  kube-controller-manager.csr       kube-controller-manager.pem      snap  worker-0-csr.json  worker-1.csr       worker-1.pem      worker-2-key.pem
admin-csr.json  ca-config.json  ca-key.pem   go            kube-controller-manager-csr.json  kubernetes-src.tar.gz            test  worker-0-key.pem   worker-1-csr.json  worker-2.csr      worker-2.pem
root@master:~# {
>
> cat > kube-proxy-csr.json <<EOF
> {
>   "CN": "system:kube-proxy",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "system:node-proxier",
>       "OU": "Kubernetes The Hard Way",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> cfssl gencert \
>   -ca=ca.pem \
>   -ca-key=ca-key.pem \
>   -config=ca-config.json \
>   -profile=kubernetes \
>   kube-proxy-csr.json | cfssljson -bare kube-proxy
>
> }
2019/03/08 10:14:04 [INFO] generate received request
2019/03/08 10:14:04 [INFO] received CSR
2019/03/08 10:14:04 [INFO] generating key: rsa-2048
2019/03/08 10:14:05 [INFO] encoded CSR
2019/03/08 10:14:05 [INFO] signed certificate with serial number 390438905741114352664814202539815269185527237880
2019/03/08 10:14:05 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
root@master:~# {
>
> cat > kube-scheduler-csr.json <<EOF
> {
>   "CN": "system:kube-scheduler",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "system:kube-scheduler",
>       "OU": "Kubernetes The Hard Way",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> cfssl gencert \
>   -ca=ca.pem \
>   -ca-key=ca-key.pem \
>   -config=ca-config.json \
>   -profile=kubernetes \
>   kube-scheduler-csr.json | cfssljson -bare kube-scheduler
>
> }
2019/03/08 10:14:26 [INFO] generate received request
2019/03/08 10:14:26 [INFO] received CSR
2019/03/08 10:14:26 [INFO] generating key: rsa-2048
2019/03/08 10:14:27 [INFO] encoded CSR
2019/03/08 10:14:27 [INFO] signed certificate with serial number 529039919246709871687750868331781217283379148984
2019/03/08 10:14:27 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
root@master:~# {
>
> KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
>   --region $(gcloud config get-value compute/region) \
>   --format 'value(address)')
>
> cat > kubernetes-csr.json <<EOF
> {
>   "CN": "kubernetes",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "Kubernetes",
>       "OU": "Kubernetes The Hard Way",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> cfssl gencert \
>   -ca=ca.pem \
>   -ca-key=ca-key.pem \
>   -config=ca-config.json \
>   -hostname=10.32.0.1,10.240.0.10,10.240.0.11,10.240.0.12,${KUBERNETES_PUBLIC_ADDRESS},127.0.0.1,kubernetes.default \
>   -profile=kubernetes \
>   kubernetes-csr.json | cfssljson -bare kubernetes
>
> }
2019/03/08 10:14:54 [INFO] generate received request
2019/03/08 10:14:54 [INFO] received CSR
2019/03/08 10:14:54 [INFO] generating key: rsa-2048
2019/03/08 10:14:55 [INFO] encoded CSR
2019/03/08 10:14:55 [INFO] signed certificate with serial number 489280849685248540135986873140221872232255676351
root@master:~# ls -l
total 165044
-rw-r--r-- 1 root root      3162  2월 22 13:22 \
-rw-r--r-- 1 root root      1033  3월  8 10:10 admin.csr
-rw-r--r-- 1 root root       231  3월  8 10:10 admin-csr.json
-rw------- 1 root root      1675  3월  8 10:10 admin-key.pem
-rw-r--r-- 1 root root      1428  3월  8 10:10 admin.pem
-rw-r--r-- 1 root root       232  3월  8 10:08 ca-config.json
-rw-r--r-- 1 root root      1005  3월  8 10:08 ca.csr
-rw-r--r-- 1 root root       211  3월  8 10:08 ca-csr.json
-rw------- 1 root root      1679  3월  8 10:08 ca-key.pem
-rw-r--r-- 1 root root      1367  3월  8 10:08 ca.pem
drwx------ 3 root root      4096  2월 22 16:50 default.etcd
drwxr-xr-x 4 root root      4096  2월 19 18:45 go
-rw-r--r-- 1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
-rw-r--r-- 1 root root      1090  3월  8 10:13 kube-controller-manager.csr
-rw-r--r-- 1 root root       272  3월  8 10:13 kube-controller-manager-csr.json
-rw------- 1 root root      1675  3월  8 10:13 kube-controller-manager-key.pem
-rw-r--r-- 1 root root      1484  3월  8 10:13 kube-controller-manager.pem
-rw-r--r-- 1 root root      1058  3월  8 10:14 kube-proxy.csr
-rw-r--r-- 1 root root       248  3월  8 10:14 kube-proxy-csr.json
-rw------- 1 root root      1675  3월  8 10:14 kube-proxy-key.pem
-rw-r--r-- 1 root root      1452  3월  8 10:14 kube-proxy.pem
-rw-r--r-- 1 root root      1033  3월  8 10:14 kubernetes.csr
-rw-r--r-- 1 root root       232  3월  8 10:14 kubernetes-csr.json
-rw------- 1 root root      1679  3월  8 10:14 kubernetes-key.pem
-rw-r--r-- 1 root root      1521  3월  8 10:14 kubernetes.pem
-rw-r--r-- 1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
-rw-r--r-- 1 root root      1066  3월  8 10:14 kube-scheduler.csr
-rw-r--r-- 1 root root       254  3월  8 10:14 kube-scheduler-csr.json
-rw------- 1 root root      1675  3월  8 10:14 kube-scheduler-key.pem
-rw-r--r-- 1 root root      1460  3월  8 10:14 kube-scheduler.pem
drwxr-xr-x 3 root root      4096  2월 20 14:49 pkg
drwxr-xr-x 4 root root      4096  2월 28 11:20 snap
drwxr-xr-x 2 root root      4096  3월  3 23:34 test
-rw-r--r-- 1 root root      1050  3월  8 10:12 worker-0.csr
-rw-r--r-- 1 root root       244  3월  8 10:12 worker-0-csr.json
-rw------- 1 root root      1679  3월  8 10:12 worker-0-key.pem
-rw-r--r-- 1 root root      1493  3월  8 10:12 worker-0.pem
-rw-r--r-- 1 root root      1050  3월  8 10:12 worker-1.csr
-rw-r--r-- 1 root root       244  3월  8 10:12 worker-1-csr.json
-rw------- 1 root root      1675  3월  8 10:12 worker-1-key.pem
-rw-r--r-- 1 root root      1493  3월  8 10:12 worker-1.pem
-rw-r--r-- 1 root root      1050  3월  8 10:12 worker-2.csr
-rw-r--r-- 1 root root       244  3월  8 10:12 worker-2-csr.json
-rw------- 1 root root      1679  3월  8 10:12 worker-2-key.pem
-rw-r--r-- 1 root root      1493  3월  8 10:12 worker-2.pem
drwxr-xr-x 2 root root      4096  3월  8 10:09 yaml
root@master:~#
root@master:~#
root@master:~# {
>
> cat > service-account-csr.json <<EOF
> {
>   "CN": "service-accounts",
>   "key": {
>     "algo": "rsa",
>     "size": 2048
>   },
>   "names": [
>     {
>       "C": "US",
>       "L": "Portland",
>       "O": "Kubernetes",
>       "OU": "Kubernetes The Hard Way",
>       "ST": "Oregon"
>     }
>   ]
> }
> EOF
>
> cfssl gencert \
>   -ca=ca.pem \
>   -ca-key=ca-key.pem \
>   -config=ca-config.json \
>   -profile=kubernetes \
>   service-account-csr.json | cfssljson -bare service-account
>
> }
2019/03/08 10:15:36 [INFO] generate received request
2019/03/08 10:15:36 [INFO] received CSR
2019/03/08 10:15:36 [INFO] generating key: rsa-2048
2019/03/08 10:15:36 [INFO] encoded CSR
2019/03/08 10:15:36 [INFO] signed certificate with serial number 356350419682405825972237514641988410320444130655
2019/03/08 10:15:36 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
root@master:~# for instance in worker-0 worker-1 worker-2; do
>   gcloud compute scp ca.pem ${instance}-key.pem ${instance}.pem ${instance}:~/
> done
Warning: Permanently added 'compute.4741376659807797739' (ECDSA) to the list of known hosts.
ca.pem                                                                                                                                                                                                100% 1367     1.3KB/s   00:00
worker-0-key.pem                                                                                                                                                                                      100% 1679     1.6KB/s   00:00
worker-0.pem                                                                                                                                                                                          100% 1493     1.5KB/s   00:00
Warning: Permanently added 'compute.393066346732847593' (ECDSA) to the list of known hosts.
ca.pem                                                                                                                                                                                                100% 1367     1.3KB/s   00:00
worker-1-key.pem                                                                                                                                                                                      100% 1675     1.6KB/s   00:00
worker-1.pem                                                                                                                                                                                          100% 1493     1.5KB/s   00:00
Warning: Permanently added 'compute.3477245605660328422' (ECDSA) to the list of known hosts.
ca.pem                                                                                                                                                                                                100% 1367     1.3KB/s   00:00
worker-2-key.pem                                                                                                                                                                                      100% 1679     1.6KB/s   00:00
worker-2.pem                                                                                                                                                                                          100% 1493     1.5KB/s   00:00
root@master:~#
root@master:~# for instance in controller-0 controller-1 controller-2; do
>   gcloud compute scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
>     service-account-key.pem service-account.pem ${instance}:~/
> done
ca.pem                                                                                                                                                                                                100% 1367     1.3KB/s   00:00
ca-key.pem                                                                                                                                                                                            100% 1679     1.6KB/s   00:00
kubernetes-key.pem                                                                                                                                                                                    100% 1679     1.6KB/s   00:00
kubernetes.pem                                                                                                                                                                                        100% 1521     1.5KB/s   00:00
service-account-key.pem                                                                                                                                                                               100% 1675     1.6KB/s   00:00
service-account.pem                                                                                                                                                                                   100% 1440     1.4KB/s   00:00
Warning: Permanently added 'compute.8753926435639132160' (ECDSA) to the list of known hosts.
ca.pem                                                                                                                                                                                                100% 1367     1.3KB/s   00:00
ca-key.pem                                                                                                                                                                                            100% 1679     1.6KB/s   00:00
kubernetes-key.pem                                                                                                                                                                                    100% 1679     1.6KB/s   00:00
kubernetes.pem                                                                                                                                                                                        100% 1521     1.5KB/s   00:00
service-account-key.pem                                                                                                                                                                               100% 1675     1.6KB/s   00:00
service-account.pem                                                                                                                                                                                   100% 1440     1.4KB/s   00:00
Warning: Permanently added 'compute.8585794825531162653' (ECDSA) to the list of known hosts.
ca.pem                                                                                                                                                                                                100% 1367     1.3KB/s   00:00
ca-key.pem                                                                                                                                                                                            100% 1679     1.6KB/s   00:00
kubernetes-key.pem                                                                                                                                                                                    100% 1679     1.6KB/s   00:00
kubernetes.pem                                                                                                                                                                                        100% 1521     1.5KB/s   00:00
service-account-key.pem                                                                                                                                                                               100% 1675     1.6KB/s   00:00
service-account.pem                                                                                                                                                                                   100% 1440     1.4KB/s   00:00
root@master:~# cfssljson -bare --help
Usage of cfssljson:
  -bare
        the response from CFSSL is not wrapped in the API standard response
  -f string
        JSON input (default "-")
  -stdout
        output the response instead of saving to a file
root@master:~#
root@master:~#
root@master:~# KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
>   --region $(gcloud config get-value compute/region) \
>   --format 'value(address)')
root@master:~#
root@master:~#
root@master:~# echo $KUBERNETES_PUBLIC_ADDRESS
34.80.89.118
root@master:~#
root@master:~# for instance in worker-0 worker-1 worker-2; do
>   kubectl config set-cluster kubernetes-the-hard-way \
>     --certificate-authority=ca.pem \
>     --embed-certs=true \
>     --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
>     --kubeconfig=${instance}.kubeconfig
>
>   kubectl config set-credentials system:node:${instance} \
>     --client-certificate=${instance}.pem \
>     --client-key=${instance}-key.pem \
>     --embed-certs=true \
>     --kubeconfig=${instance}.kubeconfig
>
>   kubectl config set-context default \
>     --cluster=kubernetes-the-hard-way \
>     --user=system:node:${instance} \
>     --kubeconfig=${instance}.kubeconfig
>
>   kubectl config use-context default --kubeconfig=${instance}.kubeconfig
> done
Cluster "kubernetes-the-hard-way" set.
User "system:node:worker-0" set.
Context "default" created.
Switched to context "default".
Cluster "kubernetes-the-hard-way" set.
User "system:node:worker-1" set.
Context "default" created.
Switched to context "default".
Cluster "kubernetes-the-hard-way" set.
User "system:node:worker-2" set.
Context "default" created.
Switched to context "default".
root@master:~# ls
\               ca.csr        go1.11.5.linux-amd64.tar.gz       kube-proxy-csr.json  kubernetes.pem           pkg                       test                 worker-1.csr         worker-2-csr.json
admin.csr       ca-csr.json   kube-controller-manager.csr       kube-proxy-key.pem   kubernetes-src.tar.gz    service-account.csr       worker-0.csr         worker-1-csr.json    worker-2-key.pem
admin-csr.json  ca-key.pem    kube-controller-manager-csr.json  kube-proxy.pem       kube-scheduler.csr       service-account-csr.json  worker-0-csr.json    worker-1-key.pem     worker-2.kubeconfig
admin-key.pem   ca.pem        kube-controller-manager-key.pem   kubernetes.csr       kube-scheduler-csr.json  service-account-key.pem   worker-0-key.pem     worker-1.kubeconfig  worker-2.pem
admin.pem       default.etcd  kube-controller-manager.pem       kubernetes-csr.json  kube-scheduler-key.pem   service-account.pem       worker-0.kubeconfig  worker-1.pem         yaml
ca-config.json  go            kube-proxy.csr                    kubernetes-key.pem   kube-scheduler.pem       snap                      worker-0.pem         worker-2.csr
root@master:~# ls -l
total 165084
-rw-r--r-- 1 root root      3162  2월 22 13:22 \
-rw-r--r-- 1 root root      1033  3월  8 10:10 admin.csr
-rw-r--r-- 1 root root       231  3월  8 10:10 admin-csr.json
-rw------- 1 root root      1675  3월  8 10:10 admin-key.pem
-rw-r--r-- 1 root root      1428  3월  8 10:10 admin.pem
-rw-r--r-- 1 root root       232  3월  8 10:08 ca-config.json
-rw-r--r-- 1 root root      1005  3월  8 10:08 ca.csr
-rw-r--r-- 1 root root       211  3월  8 10:08 ca-csr.json
-rw------- 1 root root      1679  3월  8 10:08 ca-key.pem
-rw-r--r-- 1 root root      1367  3월  8 10:08 ca.pem
drwx------ 3 root root      4096  2월 22 16:50 default.etcd
drwxr-xr-x 4 root root      4096  2월 19 18:45 go
-rw-r--r-- 1 root root 140132627  1월 24 06:32 go1.11.5.linux-amd64.tar.gz
-rw-r--r-- 1 root root      1090  3월  8 10:13 kube-controller-manager.csr
-rw-r--r-- 1 root root       272  3월  8 10:13 kube-controller-manager-csr.json
-rw------- 1 root root      1675  3월  8 10:13 kube-controller-manager-key.pem
-rw-r--r-- 1 root root      1484  3월  8 10:13 kube-controller-manager.pem
-rw-r--r-- 1 root root      1058  3월  8 10:14 kube-proxy.csr
-rw-r--r-- 1 root root       248  3월  8 10:14 kube-proxy-csr.json
-rw------- 1 root root      1675  3월  8 10:14 kube-proxy-key.pem
-rw-r--r-- 1 root root      1452  3월  8 10:14 kube-proxy.pem
-rw-r--r-- 1 root root      1033  3월  8 10:14 kubernetes.csr
-rw-r--r-- 1 root root       232  3월  8 10:14 kubernetes-csr.json
-rw------- 1 root root      1679  3월  8 10:14 kubernetes-key.pem
-rw-r--r-- 1 root root      1521  3월  8 10:14 kubernetes.pem
-rw-r--r-- 1 root root  28685456  3월  6 09:36 kubernetes-src.tar.gz
-rw-r--r-- 1 root root      1066  3월  8 10:14 kube-scheduler.csr
-rw-r--r-- 1 root root       254  3월  8 10:14 kube-scheduler-csr.json
-rw------- 1 root root      1675  3월  8 10:14 kube-scheduler-key.pem
-rw-r--r-- 1 root root      1460  3월  8 10:14 kube-scheduler.pem
drwxr-xr-x 3 root root      4096  2월 20 14:49 pkg
-rw-r--r-- 1 root root      1041  3월  8 10:15 service-account.csr
-rw-r--r-- 1 root root       238  3월  8 10:15 service-account-csr.json
-rw------- 1 root root      1675  3월  8 10:15 service-account-key.pem
-rw-r--r-- 1 root root      1440  3월  8 10:15 service-account.pem
drwxr-xr-x 4 root root      4096  2월 28 11:20 snap
drwxr-xr-x 2 root root      4096  3월  3 23:34 test
-rw-r--r-- 1 root root      1050  3월  8 10:12 worker-0.csr
-rw-r--r-- 1 root root       244  3월  8 10:12 worker-0-csr.json
-rw------- 1 root root      1679  3월  8 10:12 worker-0-key.pem
-rw------- 1 root root      6450  3월  8 10:31 worker-0.kubeconfig
-rw-r--r-- 1 root root      1493  3월  8 10:12 worker-0.pem
-rw-r--r-- 1 root root      1050  3월  8 10:12 worker-1.csr
-rw-r--r-- 1 root root       244  3월  8 10:12 worker-1-csr.json
-rw------- 1 root root      1675  3월  8 10:12 worker-1-key.pem
-rw------- 1 root root      6446  3월  8 10:31 worker-1.kubeconfig
-rw-r--r-- 1 root root      1493  3월  8 10:12 worker-1.pem
-rw-r--r-- 1 root root      1050  3월  8 10:12 worker-2.csr
-rw-r--r-- 1 root root       244  3월  8 10:12 worker-2-csr.json
-rw------- 1 root root      1679  3월  8 10:12 worker-2-key.pem
-rw------- 1 root root      6450  3월  8 10:31 worker-2.kubeconfig
-rw-r--r-- 1 root root      1493  3월  8 10:12 worker-2.pem
drwxr-xr-x 2 root root      4096  3월  8 10:09 yaml
root@master:~#
root@master:~#
root@master:~#
root@master:~# {
>   kubectl config set-cluster kubernetes-the-hard-way \
>     --certificate-authority=ca.pem \
>     --embed-certs=true \
>     --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
>     --kubeconfig=kube-proxy.kubeconfig
>
>   kubectl config set-credentials system:kube-proxy \
>     --client-certificate=kube-proxy.pem \
>     --client-key=kube-proxy-key.pem \
>     --embed-certs=true \
>     --kubeconfig=kube-proxy.kubeconfig
>
>   kubectl config set-context default \
>     --cluster=kubernetes-the-hard-way \
>     --user=system:kube-proxy \
>     --kubeconfig=kube-proxy.kubeconfig
>
>   kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
> }
Cluster "kubernetes-the-hard-way" set.
User "system:kube-proxy" set.
Context "default" created.
Switched to context "default".
root@master:~#
root@master:~#
root@master:~# {
>   kubectl config set-cluster kubernetes-the-hard-way \
>     --certificate-authority=ca.pem \
>     --embed-certs=true \
>     --server=https://127.0.0.1:6443 \
>     --kubeconfig=kube-scheduler.kubeconfig
>
>   kubectl config set-credentials system:kube-scheduler \
>     --client-certificate=kube-scheduler.pem \
>     --client-key=kube-scheduler-key.pem \
>     --embed-certs=true \
>     --kubeconfig=kube-scheduler.kubeconfig
>
>   kubectl config set-context default \
>     --cluster=kubernetes-the-hard-way \
>     --user=system:kube-scheduler \
>     --kubeconfig=kube-scheduler.kubeconfig
>
>   kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
> }
Cluster "kubernetes-the-hard-way" set.
User "system:kube-scheduler" set.
Context "default" created.
Switched to context "default".
root@master:~#
root@master:~#
root@master:~#
root@master:~# {
>   kubectl config set-cluster kubernetes-the-hard-way \
>     --certificate-authority=ca.pem \
>     --embed-certs=true \
>     --server=https://127.0.0.1:6443 \
>     --kubeconfig=admin.kubeconfig
>
>   kubectl config set-credentials admin \
>     --client-certificate=admin.pem \
>     --client-key=admin-key.pem \
>     --embed-certs=true \
>     --kubeconfig=admin.kubeconfig
>
>   kubectl config set-context default \
>     --cluster=kubernetes-the-hard-way \
>     --user=admin \
>     --kubeconfig=admin.kubeconfig
>
>   kubectl config use-context default --kubeconfig=admin.kubeconfig
> }
Cluster "kubernetes-the-hard-way" set.
User "admin" set.
Context "default" created.
Switched to context "default".
root@master:~#
root@master:~#
root@master:~#
root@master:~# for instance in worker-0 worker-1 worker-2; do
>   gcloud compute scp ${instance}.kubeconfig kube-proxy.kubeconfig ${instance}:~/
> done
worker-0.kubeconfig                                                                                                                                                                                   100% 6450     6.3KB/s   00:00
kube-proxy.kubeconfig                                                                                                                                                                                 100% 6384     6.2KB/s   00:00
worker-1.kubeconfig                                                                                                                                                                                   100% 6446     6.3KB/s   00:00
kube-proxy.kubeconfig                                                                                                                                                                                 100% 6384     6.2KB/s   00:00
worker-2.kubeconfig                                                                                                                                                                                   100% 6450     6.3KB/s   00:00
kube-proxy.kubeconfig                                                                                                                                                                                 100% 6384     6.2KB/s   00:00
root@master:~#
root@master:~#
root@master:~#
root@master:~# for instance in controller-0 controller-1 controller-2; do
>   gcloud compute scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
> done
admin.kubeconfig                                                                                                                                                                                      100% 6325     6.2KB/s   00:00
kube-controller-manager.kubeconfig: No such file or directory
kube-scheduler.kubeconfig                                                                                                                                                                             100% 6401     6.3KB/s   00:00
ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1].
admin.kubeconfig                                                                                                                                                                                      100% 6325     6.2KB/s   00:00
kube-controller-manager.kubeconfig: No such file or directory
kube-scheduler.kubeconfig                                                                                                                                                                             100% 6401     6.3KB/s   00:00
ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1].
admin.kubeconfig                                                                                                                                                                                      100% 6325     6.2KB/s   00:00
kube-controller-manager.kubeconfig: No such file or directory
kube-scheduler.kubeconfig                                                                                                                                                                             100% 6401     6.3KB/s   00:00
ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1].
root@master:~# ls -l *.kubeconfig
-rw------- 1 root root 6325  3월  8 10:33 admin.kubeconfig
-rw------- 1 root root 6384  3월  8 10:32 kube-proxy.kubeconfig
-rw------- 1 root root 6401  3월  8 10:32 kube-scheduler.kubeconfig
-rw------- 1 root root 6450  3월  8 10:31 worker-0.kubeconfig
-rw------- 1 root root 6446  3월  8 10:31 worker-1.kubeconfig
-rw------- 1 root root 6450  3월  8 10:31 worker-2.kubeconfig
root@master:~# {
>   kubectl config set-cluster kubernetes-the-hard-way \
>     --certificate-authority=ca.pem \
>     --embed-certs=true \
>     --server=https://127.0.0.1:6443 \
>     --kubeconfig=kube-controller-manager.kubeconfig
>
>   kubectl config set-credentials system:kube-controller-manager \
>     --client-certificate=kube-controller-manager.pem \
>     --client-key=kube-controller-manager-key.pem \
>     --embed-certs=true \
>     --kubeconfig=kube-controller-manager.kubeconfig
>
>   kubectl config set-context default \
>     --cluster=kubernetes-the-hard-way \
>     --user=system:kube-controller-manager \
>     --kubeconfig=kube-controller-manager.kubeconfig
>
>   kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
> }
Cluster "kubernetes-the-hard-way" set.
User "system:kube-controller-manager" set.
Context "default" created.
Switched to context "default".
root@master:~# for instance in controller-0 controller-1 controller-2; do
>   gcloud compute scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
> done
admin.kubeconfig                                                                                                                                                                                      100% 6325     6.2KB/s   00:00
kube-controller-manager.kubeconfig                                                                                                                                                                    100% 6451     6.3KB/s   00:00
kube-scheduler.kubeconfig                                                                                                                                                                             100% 6401     6.3KB/s   00:00
admin.kubeconfig                                                                                                                                                                                      100% 6325     6.2KB/s   00:00
kube-controller-manager.kubeconfig                                                                                                                                                                    100% 6451     6.3KB/s   00:00
kube-scheduler.kubeconfig                                                                                                                                                                             100% 6401     6.3KB/s   00:00
admin.kubeconfig                                                                                                                                                                                      100% 6325     6.2KB/s   00:00
kube-controller-manager.kubeconfig                                                                                                                                                                    100% 6451     6.3KB/s   00:00
kube-scheduler.kubeconfig                                                                                                                                                                             100% 6401     6.3KB/s   00:00
root@master:~#
root@master:~#
root@master:~# ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
root@master:~# ls
\                 ca.csr                       kube-controller-manager.csr         kube-proxy-key.pem     kubernetes-src.tar.gz      service-account.csr       worker-0-csr.json    worker-1.kubeconfig  yaml
admin.csr         ca-csr.json                  kube-controller-manager-csr.json    kube-proxy.kubeconfig  kube-scheduler.csr         service-account-csr.json  worker-0-key.pem     worker-1.pem
admin-csr.json    ca-key.pem                   kube-controller-manager-key.pem     kube-proxy.pem         kube-scheduler-csr.json    service-account-key.pem   worker-0.kubeconfig  worker-2.csr
admin-key.pem     ca.pem                       kube-controller-manager.kubeconfig  kubernetes.csr         kube-scheduler-key.pem     service-account.pem       worker-0.pem         worker-2-csr.json
admin.kubeconfig  default.etcd                 kube-controller-manager.pem         kubernetes-csr.json    kube-scheduler.kubeconfig  snap                      worker-1.csr         worker-2-key.pem
admin.pem         go                           kube-proxy.csr                      kubernetes-key.pem     kube-scheduler.pem         test                      worker-1-csr.json    worker-2.kubeconfig
ca-config.json    go1.11.5.linux-amd64.tar.gz  kube-proxy-csr.json                 kubernetes.pem         pkg                        worker-0.csr              worker-1-key.pem     worker-2.pem
root@master:~#
root@master:~#
root@master:~# cat > encryption-config.yaml <<EOF
> kind: EncryptionConfig
> apiVersion: v1
> resources:
>   - resources:
>       - secrets
>     providers:
>       - aescbc:
>           keys:
>             - name: key1
>               secret: ${ENCRYPTION_KEY}
>       - identity: {}
> EOF
root@master:~# for instance in controller-0 controller-1 controller-2; do
>   gcloud compute scp encryption-config.yaml ${instance}:~/
> done
encryption-config.yaml                                                                                                                                                                                100%  240     0.2KB/s   00:00
encryption-config.yaml                                                                                                                                                                                100%  240     0.2KB/s   00:00
encryption-config.yaml                                                                                                                                                                                100%  240     0.2KB/s   00:00
root@master:~# gcloud compute ssh controller-0
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 01:03:10 2019 from 112.171.112.143
root@controller-0:~#
root@controller-0:~# wget -q --show-progress --https-only --timestamping \
>   "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"
etcd-v3.3.9-linux-amd64.tar.gz                            100%[=====================================================================================================================================>]  10.73M  2.11MB/s    in 10s
root@controller-0:~# ls
admin.kubeconfig  ca.pem                  etcd-v3.3.9-linux-amd64.tar.gz      kube-scheduler.kubeconfig  kubernetes.pem           service-account.pem
ca-key.pem        encryption-config.yaml  kube-controller-manager.kubeconfig  kubernetes-key.pem         service-account-key.pem
root@controller-0:~# tar xvf etcd-v3.3.9-linux-amd64.tar.gz
etcd-v3.3.9-linux-amd64/
etcd-v3.3.9-linux-amd64/Documentation/
etcd-v3.3.9-linux-amd64/Documentation/v2/
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_2.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_1.md
etcd-v3.3.9-linux-amd64/Documentation/v2/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/security.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/v2/proxy.md
etcd-v3.3.9-linux-amd64/Documentation/v2/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/v2/other_apis.md
etcd-v3.3.9-linux-amd64/Documentation/v2/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/v2/members_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/internal-protocol-versioning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/v2/faq.md
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/v2/errorcode.md
etcd-v3.3.9-linux-amd64/Documentation/v2/docker_guide.md
etcd-v3.3.9-linux-amd64/Documentation/v2/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/release.md
etcd-v3.3.9-linux-amd64/Documentation/v2/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/backward_compatibility.md
etcd-v3.3.9-linux-amd64/Documentation/v2/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/v2/auth_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api_v3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/04_to_2_snapshot_migration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/libraries-and-tools.md
etcd-v3.3.9-linux-amd64/Documentation/v2/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/v2/admin_guide.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_1.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_0.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrading-etcd.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_4.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_3.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_2.md
etcd-v3.3.9-linux-amd64/Documentation/rfc/
etcd-v3.3.9-linux-amd64/Documentation/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/
etcd-v3.3.9-linux-amd64/Documentation/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/container-linux-systemd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/aws.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/
etcd-v3.3.9-linux-amd64/Documentation/op-guide/versioning.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/v2-migration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/supported-platform.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/hardware.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/gateway.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/failures.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png
etcd-v3.3.9-linux-amd64/Documentation/op-guide/container.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/security.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/recovery.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/performance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/monitoring.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/maintenance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grpc_proxy.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grafana.json
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/op-guide/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/learning/
etcd-v3.3.9-linux-amd64/Documentation/learning/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api.md
etcd-v3.3.9-linux-amd64/Documentation/learning/why.md
etcd-v3.3.9-linux-amd64/Documentation/learning/data_model.md
etcd-v3.3.9-linux-amd64/Documentation/learning/auth_design.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api_guarantees.md
etcd-v3.3.9-linux-amd64/Documentation/faq.md
etcd-v3.3.9-linux-amd64/Documentation/docs.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/logging.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/release.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/local_cluster.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/experimental_apis.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/limit.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/interacting_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/grpc_naming.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/demo.md
etcd-v3.3.9-linux-amd64/Documentation/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/README.md
etcd-v3.3.9-linux-amd64/Documentation/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/integrations.md
etcd-v3.3.9-linux-amd64/Documentation/dl_build.md
etcd-v3.3.9-linux-amd64/READMEv2-etcdctl.md
etcd-v3.3.9-linux-amd64/README-etcdctl.md
etcd-v3.3.9-linux-amd64/README.md
etcd-v3.3.9-linux-amd64/etcdctl
etcd-v3.3.9-linux-amd64/etcd
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# mv etcd-v3.3.9-linux-amd64/etcd /usr/local/bin
root@controller-0:~#
root@controller-0:~# mkdir -p /etc/etcd /var/lib/etcd
root@controller-0:~# cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd
root@controller-0:~#
root@controller-0:~# INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
root@controller-0:~#
root@controller-0:~# ETCD_NAME=$(hostname -s)
root@controller-0:~# echo $ETCD_NAME
controller-0
root@controller-0:~#
root@controller-0:~# cat <<EOF | sudo tee /etc/systemd/system/etcd.service
> [Unit]
> Description=etcd
> Documentation=https://github.com/coreos
>
> [Service]
> ExecStart=/usr/local/bin/etcd \\
>   --name ${ETCD_NAME} \\
>   --cert-file=/etc/etcd/kubernetes.pem \\
>   --key-file=/etc/etcd/kubernetes-key.pem \\
>   --peer-cert-file=/etc/etcd/kubernetes.pem \\
>   --peer-key-file=/etc/etcd/kubernetes-key.pem \\
>   --trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-client-cert-auth \\
>   --client-cert-auth \\
>   --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
>   --advertise-client-urls https://${INTERNAL_IP}:2379 \\
>   --initial-cluster-token etcd-cluster-0 \\
>   --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \\
>   --initial-cluster-state new \\
>   --data-dir=/var/lib/etcd
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-0 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.10:2380 \
  --listen-peer-urls https://10.240.0.10:2380 \
  --listen-client-urls https://10.240.0.10:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.10:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-0:~#
root@controller-0:~# systemctl daemon-reload
root@controller-0:~# systemctl enable etcd
Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /etc/systemd/system/etcd.service.
root@controller-0:~#
root@controller-0:~# systemctl start etcd
root@controller-0:~#
root@controller-0:~# sudo ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
sudo: etcdctl: command not found
root@controller-0:~# ls
admin.kubeconfig  ca.pem                  etcd-v3.3.9-linux-amd64         kube-controller-manager.kubeconfig  kubernetes-key.pem  service-account-key.pem
ca-key.pem        encryption-config.yaml  etcd-v3.3.9-linux-amd64.tar.gz  kube-scheduler.kubeconfig           kubernetes.pem      service-account.pem
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# cd etcd-v3.3.9-linux-amd64/
root@controller-0:~/etcd-v3.3.9-linux-amd64# ls
Documentation  README-etcdctl.md  README.md  READMEv2-etcdctl.md  etcdctl
root@controller-0:~/etcd-v3.3.9-linux-amd64# cd ..
root@controller-0:~# mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin
root@controller-0:~#
root@controller-0:~# sudo ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-0:~# systemctl restart etcd
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# sudo ETCDCTL_API=3 etcdctl member list   --endpoints=https://127.0.0.1:2379   --cacert=/etc/etcd/ca.pem   --cert=/etc/etcd/kubernetes.pem   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-0:~# cd /usr/local/bin
root@controller-0:/usr/local/bin# ls
etcd  etcdctl
root@controller-0:/usr/local/bin# ll
total 33940
drwxr-xr-x  2 root   root       4096 Mar  8 01:58 ./
drwxr-xr-x 10 root   root       4096 Mar  6 08:09 ../
-rwxr-xr-x  1 ubuntu ubuntu 18934016 Jul 24  2018 etcd*
-rwxr-xr-x  1 ubuntu ubuntu 15809280 Jul 24  2018 etcdctl*
root@controller-0:/usr/local/bin# rm -rf etcd*
root@controller-0:/usr/local/bin# cd
root@controller-0:~# ls
admin.kubeconfig  ca.pem                  etcd-v3.3.9-linux-amd64         kube-controller-manager.kubeconfig  kubernetes-key.pem  service-account-key.pem
ca-key.pem        encryption-config.yaml  etcd-v3.3.9-linux-amd64.tar.gz  kube-scheduler.kubeconfig           kubernetes.pem      service-account.pem
root@controller-0:~# rm -rf etcd-v3.3.9-linux-amd64
root@controller-0:~# tar xvf etcd-v3.3.9-linux-amd64.tar.gz
etcd-v3.3.9-linux-amd64/
etcd-v3.3.9-linux-amd64/Documentation/
etcd-v3.3.9-linux-amd64/Documentation/v2/
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_2.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_1.md
etcd-v3.3.9-linux-amd64/Documentation/v2/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/security.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/v2/proxy.md
etcd-v3.3.9-linux-amd64/Documentation/v2/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/v2/other_apis.md
etcd-v3.3.9-linux-amd64/Documentation/v2/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/v2/members_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/internal-protocol-versioning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/v2/faq.md
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/v2/errorcode.md
etcd-v3.3.9-linux-amd64/Documentation/v2/docker_guide.md
etcd-v3.3.9-linux-amd64/Documentation/v2/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/release.md
etcd-v3.3.9-linux-amd64/Documentation/v2/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/backward_compatibility.md
etcd-v3.3.9-linux-amd64/Documentation/v2/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/v2/auth_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api_v3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/04_to_2_snapshot_migration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/libraries-and-tools.md
etcd-v3.3.9-linux-amd64/Documentation/v2/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/v2/admin_guide.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_1.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_0.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrading-etcd.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_4.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_3.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_2.md
etcd-v3.3.9-linux-amd64/Documentation/rfc/
etcd-v3.3.9-linux-amd64/Documentation/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/
etcd-v3.3.9-linux-amd64/Documentation/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/container-linux-systemd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/aws.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/
etcd-v3.3.9-linux-amd64/Documentation/op-guide/versioning.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/v2-migration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/supported-platform.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/hardware.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/gateway.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/failures.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png
etcd-v3.3.9-linux-amd64/Documentation/op-guide/container.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/security.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/recovery.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/performance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/monitoring.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/maintenance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grpc_proxy.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grafana.json
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/op-guide/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/learning/
etcd-v3.3.9-linux-amd64/Documentation/learning/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api.md
etcd-v3.3.9-linux-amd64/Documentation/learning/why.md
etcd-v3.3.9-linux-amd64/Documentation/learning/data_model.md
etcd-v3.3.9-linux-amd64/Documentation/learning/auth_design.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api_guarantees.md
etcd-v3.3.9-linux-amd64/Documentation/faq.md
etcd-v3.3.9-linux-amd64/Documentation/docs.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/logging.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/release.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/local_cluster.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/experimental_apis.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/limit.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/interacting_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/grpc_naming.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/demo.md
etcd-v3.3.9-linux-amd64/Documentation/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/README.md
etcd-v3.3.9-linux-amd64/Documentation/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/integrations.md
etcd-v3.3.9-linux-amd64/Documentation/dl_build.md
etcd-v3.3.9-linux-amd64/READMEv2-etcdctl.md
etcd-v3.3.9-linux-amd64/README-etcdctl.md
etcd-v3.3.9-linux-amd64/README.md
etcd-v3.3.9-linux-amd64/etcdctl
etcd-v3.3.9-linux-amd64/etcd
root@controller-0:~# ls
admin.kubeconfig  ca.pem                  etcd-v3.3.9-linux-amd64         kube-controller-manager.kubeconfig  kubernetes-key.pem  service-account-key.pem
ca-key.pem        encryption-config.yaml  etcd-v3.3.9-linux-amd64.tar.gz  kube-scheduler.kubeconfig           kubernetes.pem      service-account.pem
root@controller-0:~#
root@controller-0:~# mkdir -p /etc/etcd /var/lib/etcd
root@controller-0:~# cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
root@controller-0:~# mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/
root@controller-0:~# echo $INTERNAL_IP
10.240.0.10
root@controller-0:~# systemctl restart etcd
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# sudo ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-0:~# rm -rf /etc/systemd/system/etcd.service
root@controller-0:~# cat <<EOF | sudo tee /etc/systemd/system/etcd.service
> [Unit]
> Description=etcd
> Documentation=https://github.com/coreos
>
> [Service]
> ExecStart=/usr/local/bin/etcd \\
>   --name ${ETCD_NAME} \\
>   --cert-file=/etc/etcd/kubernetes.pem \\
>   --key-file=/etc/etcd/kubernetes-key.pem \\
>   --peer-cert-file=/etc/etcd/kubernetes.pem \\
>   --peer-key-file=/etc/etcd/kubernetes-key.pem \\
>   --trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-client-cert-auth \\
>   --client-cert-auth \\
>   --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
>   --advertise-client-urls https://${INTERNAL_IP}:2379 \\
>   --initial-cluster-token etcd-cluster-0 \\
>   --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \\
>   --initial-cluster-state new \\
>   --data-dir=/var/lib/etcd
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-0 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.10:2380 \
  --listen-peer-urls https://10.240.0.10:2380 \
  --listen-client-urls https://10.240.0.10:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.10:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-0:~#
root@controller-0:~# systemctl daemon-reload
root@controller-0:~# systemctl enable etcd
root@controller-0:~# systemctl start etcd
root@controller-0:~#
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# sudo ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-0:~# exit
logout
Connection to 34.80.25.2 closed.
root@master:~# gcloud compute ssh controller-1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


root@controller-1:~#
root@controller-1:~# wget -q --show-progress --https-only --timestamping \
>   "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"
etcd-v3.3.9-linux-amd64.tar.gz                            100%[=====================================================================================================================================>]  10.73M  5.84MB/s    in 1.8s
root@controller-1:~# tar -xvf etcd-v3.3.9-linux-amd64.tar.gz
etcd-v3.3.9-linux-amd64/
etcd-v3.3.9-linux-amd64/Documentation/
etcd-v3.3.9-linux-amd64/Documentation/v2/
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_2.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_1.md
etcd-v3.3.9-linux-amd64/Documentation/v2/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/security.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/v2/proxy.md
etcd-v3.3.9-linux-amd64/Documentation/v2/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/v2/other_apis.md
etcd-v3.3.9-linux-amd64/Documentation/v2/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/v2/members_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/internal-protocol-versioning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/v2/faq.md
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/v2/errorcode.md
etcd-v3.3.9-linux-amd64/Documentation/v2/docker_guide.md
etcd-v3.3.9-linux-amd64/Documentation/v2/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/release.md
etcd-v3.3.9-linux-amd64/Documentation/v2/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/backward_compatibility.md
etcd-v3.3.9-linux-amd64/Documentation/v2/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/v2/auth_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api_v3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/04_to_2_snapshot_migration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/libraries-and-tools.md
etcd-v3.3.9-linux-amd64/Documentation/v2/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/v2/admin_guide.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_1.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_0.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrading-etcd.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_4.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_3.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_2.md
etcd-v3.3.9-linux-amd64/Documentation/rfc/
etcd-v3.3.9-linux-amd64/Documentation/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/
etcd-v3.3.9-linux-amd64/Documentation/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/container-linux-systemd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/aws.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/
etcd-v3.3.9-linux-amd64/Documentation/op-guide/versioning.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/v2-migration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/supported-platform.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/hardware.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/gateway.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/failures.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png
etcd-v3.3.9-linux-amd64/Documentation/op-guide/container.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/security.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/recovery.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/performance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/monitoring.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/maintenance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grpc_proxy.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grafana.json
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/op-guide/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/learning/
etcd-v3.3.9-linux-amd64/Documentation/learning/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api.md
etcd-v3.3.9-linux-amd64/Documentation/learning/why.md
etcd-v3.3.9-linux-amd64/Documentation/learning/data_model.md
etcd-v3.3.9-linux-amd64/Documentation/learning/auth_design.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api_guarantees.md
etcd-v3.3.9-linux-amd64/Documentation/faq.md
etcd-v3.3.9-linux-amd64/Documentation/docs.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/logging.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/release.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/local_cluster.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/experimental_apis.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/limit.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/interacting_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/grpc_naming.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/demo.md
etcd-v3.3.9-linux-amd64/Documentation/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/README.md
etcd-v3.3.9-linux-amd64/Documentation/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/integrations.md
etcd-v3.3.9-linux-amd64/Documentation/dl_build.md
etcd-v3.3.9-linux-amd64/READMEv2-etcdctl.md
etcd-v3.3.9-linux-amd64/README-etcdctl.md
etcd-v3.3.9-linux-amd64/README.md
etcd-v3.3.9-linux-amd64/etcdctl
etcd-v3.3.9-linux-amd64/etcd
root@controller-1:~#   sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/
root@controller-1:~#   sudo mkdir -p /etc/etcd /var/lib/etcd
root@controller-1:~#   sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
root@controller-1:~# INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
root@controller-1:~#
root@controller-1:~# ETCD_NAME=$(hostname -s)
root@controller-1:~#
root@controller-1:~# cat <<EOF | sudo tee /etc/systemd/system/etcd.service
> [Unit]
> Description=etcd
> Documentation=https://github.com/coreos
>
> [Service]
> ExecStart=/usr/local/bin/etcd \\
>   --name ${ETCD_NAME} \\
>   --cert-file=/etc/etcd/kubernetes.pem \\
>   --key-file=/etc/etcd/kubernetes-key.pem \\
>   --peer-cert-file=/etc/etcd/kubernetes.pem \\
>   --peer-key-file=/etc/etcd/kubernetes-key.pem \\
>   --trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-client-cert-auth \\
>   --client-cert-auth \\
>   --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
>   --advertise-client-urls https://${INTERNAL_IP}:2379 \\
>   --initial-cluster-token etcd-cluster-0 \\
>   --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \\
>   --initial-cluster-state new \\
>   --data-dir=/var/lib/etcd
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-1 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.11:2380 \
  --listen-peer-urls https://10.240.0.11:2380 \
  --listen-client-urls https://10.240.0.11:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.11:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-1:~#   sudo systemctl daemon-reload
root@controller-1:~#   sudo systemctl enable etcd
Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /etc/systemd/system/etcd.service.
root@controller-1:~#   sudo systemctl start etcd
root@controller-1:~#
root@controller-1:~#
root@controller-1:~# exit
logout
Connection to 34.80.85.175 closed.
root@master:~# gcloud compute ssh controller-2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


root@controller-2:~# wget -q --show-progress --https-only --timestamping \
>   "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"
etcd-v3.3.9-linux-amd64.tar.gz                            100%[=====================================================================================================================================>]  10.73M  5.84MB/s    in 1.8s
root@controller-2:~#   tar -xvf etcd-v3.3.9-linux-amd64.tar.gz
etcd-v3.3.9-linux-amd64/
etcd-v3.3.9-linux-amd64/Documentation/
etcd-v3.3.9-linux-amd64/Documentation/v2/
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_2.md
etcd-v3.3.9-linux-amd64/Documentation/v2/upgrade_2_1.md
etcd-v3.3.9-linux-amd64/Documentation/v2/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/security.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/v2/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/
etcd-v3.3.9-linux-amd64/Documentation/v2/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/v2/proxy.md
etcd-v3.3.9-linux-amd64/Documentation/v2/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/
etcd-v3.3.9-linux-amd64/Documentation/v2/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/v2/other_apis.md
etcd-v3.3.9-linux-amd64/Documentation/v2/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/v2/members_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/internal-protocol-versioning.md
etcd-v3.3.9-linux-amd64/Documentation/v2/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/v2/faq.md
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/v2/etcd_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/v2/errorcode.md
etcd-v3.3.9-linux-amd64/Documentation/v2/docker_guide.md
etcd-v3.3.9-linux-amd64/Documentation/v2/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/
etcd-v3.3.9-linux-amd64/Documentation/v2/dev/release.md
etcd-v3.3.9-linux-amd64/Documentation/v2/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/v2/backward_compatibility.md
etcd-v3.3.9-linux-amd64/Documentation/v2/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/v2/auth_api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api_v3.md
etcd-v3.3.9-linux-amd64/Documentation/v2/api.md
etcd-v3.3.9-linux-amd64/Documentation/v2/README.md
etcd-v3.3.9-linux-amd64/Documentation/v2/04_to_2_snapshot_migration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/libraries-and-tools.md
etcd-v3.3.9-linux-amd64/Documentation/v2/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/v2/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/v2/admin_guide.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_1.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_0.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrading-etcd.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_4.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_3.md
etcd-v3.3.9-linux-amd64/Documentation/upgrades/upgrade_3_2.md
etcd-v3.3.9-linux-amd64/Documentation/rfc/
etcd-v3.3.9-linux-amd64/Documentation/rfc/v3api.md
etcd-v3.3.9-linux-amd64/Documentation/reporting_bugs.md
etcd-v3.3.9-linux-amd64/Documentation/production-users.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/
etcd-v3.3.9-linux-amd64/Documentation/platforms/freebsd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/container-linux-systemd.md
etcd-v3.3.9-linux-amd64/Documentation/platforms/aws.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/
etcd-v3.3.9-linux-amd64/Documentation/op-guide/versioning.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/v2-migration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/supported-platform.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/hardware.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/gateway.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/failures.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png
etcd-v3.3.9-linux-amd64/Documentation/op-guide/container.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/security.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/runtime-reconf-design.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/recovery.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/performance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/monitoring.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/maintenance.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grpc_proxy.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/grafana.json
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules.yml
etcd-v3.3.9-linux-amd64/Documentation/op-guide/etcd3_alert.rules
etcd-v3.3.9-linux-amd64/Documentation/op-guide/configuration.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/clustering.md
etcd-v3.3.9-linux-amd64/Documentation/op-guide/authentication.md
etcd-v3.3.9-linux-amd64/Documentation/metrics.md
etcd-v3.3.9-linux-amd64/Documentation/learning/
etcd-v3.3.9-linux-amd64/Documentation/learning/glossary.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api.md
etcd-v3.3.9-linux-amd64/Documentation/learning/why.md
etcd-v3.3.9-linux-amd64/Documentation/learning/data_model.md
etcd-v3.3.9-linux-amd64/Documentation/learning/auth_design.md
etcd-v3.3.9-linux-amd64/Documentation/learning/api_guarantees.md
etcd-v3.3.9-linux-amd64/Documentation/faq.md
etcd-v3.3.9-linux-amd64/Documentation/docs.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/logging.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/discovery_protocol.md
etcd-v3.3.9-linux-amd64/Documentation/dev-internal/release.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/local_cluster.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/experimental_apis.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/limit.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/interacting_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/grpc_naming.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md
etcd-v3.3.9-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md
etcd-v3.3.9-linux-amd64/Documentation/demo.md
etcd-v3.3.9-linux-amd64/Documentation/branch_management.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/README.md
etcd-v3.3.9-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.9-linux-amd64/Documentation/README.md
etcd-v3.3.9-linux-amd64/Documentation/tuning.md
etcd-v3.3.9-linux-amd64/Documentation/integrations.md
etcd-v3.3.9-linux-amd64/Documentation/dl_build.md
etcd-v3.3.9-linux-amd64/READMEv2-etcdctl.md
etcd-v3.3.9-linux-amd64/README-etcdctl.md
etcd-v3.3.9-linux-amd64/README.md
etcd-v3.3.9-linux-amd64/etcdctl
etcd-v3.3.9-linux-amd64/etcd
root@controller-2:~#   sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/
root@controller-2:~#   sudo mkdir -p /etc/etcd /var/lib/etcd
root@controller-2:~#   sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
root@controller-2:~# INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
root@controller-2:~#
root@controller-2:~# ETCD_NAME=$(hostname -s)
root@controller-2:~#
root@controller-2:~# cat <<EOF | sudo tee /etc/systemd/system/etcd.service
> [Unit]
> Description=etcd
> Documentation=https://github.com/coreos
>
> [Service]
> ExecStart=/usr/local/bin/etcd \\
>   --name ${ETCD_NAME} \\
>   --cert-file=/etc/etcd/kubernetes.pem \\
>   --key-file=/etc/etcd/kubernetes-key.pem \\
>   --peer-cert-file=/etc/etcd/kubernetes.pem \\
>   --peer-key-file=/etc/etcd/kubernetes-key.pem \\
>   --trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-trusted-ca-file=/etc/etcd/ca.pem \\
>   --peer-client-cert-auth \\
>   --client-cert-auth \\
>   --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-peer-urls https://${INTERNAL_IP}:2380 \\
>   --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
>   --advertise-client-urls https://${INTERNAL_IP}:2379 \\
>   --initial-cluster-token etcd-cluster-0 \\
>   --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \\
>   --initial-cluster-state new \\
>   --data-dir=/var/lib/etcd
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-2 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.12:2380 \
  --listen-peer-urls https://10.240.0.12:2380 \
  --listen-client-urls https://10.240.0.12:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.12:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-2:~#   sudo systemctl daemon-reload
root@controller-2:~#   sudo systemctl enable etcd
Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /etc/systemd/system/etcd.service.
root@controller-2:~#   sudo systemctl start etcd
root@controller-2:~# sudo ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-2:~# exit
logout
Connection to 34.80.89.164 closed.
root@master:~# sudo ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
sudo: etcdctl: command not found
root@master:~# gcloud compute ssh controller-2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:03:13 2019 from 112.171.112.143
root@controller-2:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:03:52 UTC; 50s ago
     Docs: https://github.com/coreos
 Main PID: 3070 (etcd)
    Tasks: 7
   Memory: 5.2M
      CPU: 290ms
   CGroup: /system.slice/etcd.service
           └─3070 /usr/local/bin/etcd --name controller-2 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:04:40 controller-2 etcd[3070]: 3a57933972cb5131 is starting a new election at term 32
Mar 08 02:04:40 controller-2 etcd[3070]: 3a57933972cb5131 became candidate at term 33
Mar 08 02:04:40 controller-2 etcd[3070]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 33
Mar 08 02:04:40 controller-2 etcd[3070]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 33
Mar 08 02:04:40 controller-2 etcd[3070]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 33
Mar 08 02:04:41 controller-2 etcd[3070]: 3a57933972cb5131 is starting a new election at term 33
Mar 08 02:04:41 controller-2 etcd[3070]: 3a57933972cb5131 became candidate at term 34
Mar 08 02:04:41 controller-2 etcd[3070]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 34
Mar 08 02:04:41 controller-2 etcd[3070]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 34
Mar 08 02:04:41 controller-2 etcd[3070]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 34
root@controller-2:~# ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-2:~# etcdctl
NAME:
   etcdctl - A simple command line client for etcd.

WARNING:
   Environment variable ETCDCTL_API is not set; defaults to etcdctl v2.
   Set environment variable ETCDCTL_API=3 to use v3 API or ETCDCTL_API=2 to use v2 API.

USAGE:
   etcdctl [global options] command [command options] [arguments...]

VERSION:
   3.3.9

COMMANDS:
     backup          backup an etcd directory
     cluster-health  check the health of the etcd cluster
     mk              make a new key with a given value
     mkdir           make a new directory
     rm              remove a key or a directory
     rmdir           removes the key if it is an empty directory or a key-value pair
     get             retrieve the value of a key
     ls              retrieve a directory
     set             set the value of a key
     setdir          create a new directory or update an existing directory TTL
     update          update an existing key with a given value
     updatedir       update an existing directory
     watch           watch a key for changes
     exec-watch      watch a key for changes and exec an executable
     member          member add, remove and list subcommands
     user            user add, grant and revoke subcommands
     role            role add, grant and revoke subcommands
     auth            overall auth controls
     help, h         Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                          output cURL commands which can be used to reproduce the request
   --no-sync                        don't synchronize cluster information before sending request
   --output simple, -o simple       output response in the given format (simple, `extended` or `json`) (default: "simple")
   --discovery-srv value, -D value  domain name to query for SRV records describing cluster endpoints
   --insecure-discovery             accept insecure SRV records describing cluster endpoints
   --peers value, -C value          DEPRECATED - "--endpoints" should be used instead
   --endpoint value                 DEPRECATED - "--endpoints" should be used instead
   --endpoints value                a comma-delimited list of machine addresses in the cluster (default: "http://127.0.0.1:2379,http://127.0.0.1:4001")
   --cert-file value                identify HTTPS client using this SSL certificate file
   --key-file value                 identify HTTPS client using this SSL key file
   --ca-file value                  verify certificates of HTTPS-enabled servers using this CA bundle
   --username value, -u value       provide username[:password] and prompt if password is not supplied.
   --timeout value                  connection timeout per request (default: 2s)
   --total-timeout value            timeout for the command execution (except watch) (default: 5s)
   --help, -h                       show help
   --version, -v                    print the version

root@controller-2:~# exit
logout
Connection to 34.80.89.164 closed.
root@master:~# gcloud compute ssh controller-0

Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 01:52:51 2019 from 112.171.112.143
root@controller-0:~#
root@controller-0:~# systemctl stauts etcd
Unknown operation stauts.
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:00:09 UTC; 6min ago
     Docs: https://github.com/coreos
 Main PID: 3192 (etcd)
   CGroup: /system.slice/etcd.service
           └─3192 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:06:38 controller-0 etcd[3192]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 453
Mar 08 02:06:38 controller-0 etcd[3192]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 453
Mar 08 02:06:38 controller-0 etcd[3192]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 453
Mar 08 02:06:39 controller-0 etcd[3192]: health check for peer 3a57933972cb5131 could not connect: dial tcp 10.240.0.12:2380: i/o timeout
Mar 08 02:06:39 controller-0 etcd[3192]: health check for peer ffed16798470cab5 could not connect: dial tcp 10.240.0.11:2380: i/o timeout
Mar 08 02:06:40 controller-0 etcd[3192]: f98dc20bce6225a0 is starting a new election at term 453
Mar 08 02:06:40 controller-0 etcd[3192]: f98dc20bce6225a0 became candidate at term 454
Mar 08 02:06:40 controller-0 etcd[3192]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 454
Mar 08 02:06:40 controller-0 etcd[3192]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 454
Mar 08 02:06:40 controller-0 etcd[3192]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 454
root@controller-0:~# systemctl restart etcd
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:07:35 UTC; 1s ago
     Docs: https://github.com/coreos
 Main PID: 3453 (etcd)
    Tasks: 6
   Memory: 3.9M
      CPU: 47ms
   CGroup: /system.slice/etcd.service
           └─3453 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:07:35 controller-0 etcd[3453]: started streaming with peer 3a57933972cb5131 (stream Message reader)
Mar 08 02:07:35 controller-0 etcd[3453]: started streaming with peer ffed16798470cab5 (writer)
Mar 08 02:07:35 controller-0 etcd[3453]: started streaming with peer ffed16798470cab5 (writer)
Mar 08 02:07:35 controller-0 etcd[3453]: started streaming with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:07:35 controller-0 etcd[3453]: started streaming with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:07:36 controller-0 etcd[3453]: f98dc20bce6225a0 is starting a new election at term 490
Mar 08 02:07:36 controller-0 etcd[3453]: f98dc20bce6225a0 became candidate at term 491
Mar 08 02:07:36 controller-0 etcd[3453]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 491
Mar 08 02:07:36 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 491
Mar 08 02:07:36 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 491
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:07:35 UTC; 2min 59s ago
     Docs: https://github.com/coreos
 Main PID: 3453 (etcd)
    Tasks: 6
   Memory: 7.6M
      CPU: 928ms
   CGroup: /system.slice/etcd.service
           └─3453 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:10:32 controller-0 etcd[3453]: f98dc20bce6225a0 is starting a new election at term 612
Mar 08 02:10:32 controller-0 etcd[3453]: f98dc20bce6225a0 became candidate at term 613
Mar 08 02:10:32 controller-0 etcd[3453]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 613
Mar 08 02:10:32 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 613
Mar 08 02:10:32 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 613
Mar 08 02:10:34 controller-0 etcd[3453]: f98dc20bce6225a0 is starting a new election at term 613
Mar 08 02:10:34 controller-0 etcd[3453]: f98dc20bce6225a0 became candidate at term 614
Mar 08 02:10:34 controller-0 etcd[3453]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 614
Mar 08 02:10:34 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 614
Mar 08 02:10:34 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 614
root@controller-0:~# exit
logout
Connection to 34.80.25.2 closed.
root@master:~# gcloud compute ssh controller-1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:02:19 2019 from 112.171.112.143
root@controller-1:~#
root@controller-1:~#
root@controller-1:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:03:04 UTC; 7min ago
     Docs: https://github.com/coreos
 Main PID: 3067 (etcd)
    Tasks: 7
   Memory: 8.0M
      CPU: 2.347s
   CGroup: /system.slice/etcd.service
           └─3067 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:10:47 controller-1 etcd[3067]: ffed16798470cab5 became candidate at term 320
Mar 08 02:10:47 controller-1 etcd[3067]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 320
Mar 08 02:10:47 controller-1 etcd[3067]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 320
Mar 08 02:10:47 controller-1 etcd[3067]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 320
Mar 08 02:10:48 controller-1 etcd[3067]: ffed16798470cab5 is starting a new election at term 320
Mar 08 02:10:48 controller-1 etcd[3067]: ffed16798470cab5 became candidate at term 321
Mar 08 02:10:48 controller-1 etcd[3067]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 321
Mar 08 02:10:48 controller-1 etcd[3067]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 321
Mar 08 02:10:48 controller-1 etcd[3067]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 321
Mar 08 02:10:48 controller-1 etcd[3067]: publish error: etcdserver: request timed out
root@controller-1:~# systemctl daemon-reload
root@controller-1:~# systemctl restart etcd
root@controller-1:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:11:05 UTC; 1s ago
     Docs: https://github.com/coreos
 Main PID: 3278 (etcd)
    Tasks: 6
   Memory: 3.7M
      CPU: 48ms
   CGroup: /system.slice/etcd.service
           └─3278 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer 3a57933972cb5131 (stream Message reader)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (writer)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (writer)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (stream MsgApp v2 reader)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (stream Message reader)
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 is starting a new election at term 333
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 became candidate at term 334
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 334
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 334
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 334
...skipping...
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:11:05 UTC; 1s ago
     Docs: https://github.com/coreos
 Main PID: 3278 (etcd)
    Tasks: 6
   Memory: 3.7M
      CPU: 48ms
   CGroup: /system.slice/etcd.service
           └─3278 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer 3a57933972cb5131 (stream Message reader)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (writer)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (writer)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (stream MsgApp v2 reader)
Mar 08 02:11:05 controller-1 etcd[3278]: started streaming with peer f98dc20bce6225a0 (stream Message reader)
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 is starting a new election at term 333
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 became candidate at term 334
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 334
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 334
Mar 08 02:11:07 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 334
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
...skipping...
export LESSOPEN="| /usr/bin/lesspipe %s";
export LESSCLOSE="/usr/bin/lesspipe %s %s";
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
--key=/etc/etcd/kubernetes-key.pemESC:q
root@controller-1:~# ^C
root@controller-1:~# ^C
root@controller-1:~# ^C
root@controller-1:~# ^C
root@controller-1:~# ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-1:~#
root@controller-1:~#
root@controller-1:~#
root@controller-1:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:11:05 UTC; 41s ago
     Docs: https://github.com/coreos
 Main PID: 3278 (etcd)
    Tasks: 6
   Memory: 4.5M
      CPU: 262ms
   CGroup: /system.slice/etcd.service
           └─3278 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:11:45 controller-1 etcd[3278]: ffed16798470cab5 is starting a new election at term 359
Mar 08 02:11:45 controller-1 etcd[3278]: ffed16798470cab5 became candidate at term 360
Mar 08 02:11:45 controller-1 etcd[3278]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 360
Mar 08 02:11:45 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 360
Mar 08 02:11:45 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 360
Mar 08 02:11:47 controller-1 etcd[3278]: ffed16798470cab5 is starting a new election at term 360
Mar 08 02:11:47 controller-1 etcd[3278]: ffed16798470cab5 became candidate at term 361
Mar 08 02:11:47 controller-1 etcd[3278]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 361
Mar 08 02:11:47 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 361
Mar 08 02:11:47 controller-1 etcd[3278]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 361
root@controller-1:~# ls
admin.kubeconfig  ca.pem                  etcd-v3.3.9-linux-amd64         kube-controller-manager.kubeconfig  kubernetes-key.pem  service-account-key.pem
ca-key.pem        encryption-config.yaml  etcd-v3.3.9-linux-amd64.tar.gz  kube-scheduler.kubeconfig           kubernetes.pem      service-account.pem
root@controller-1:~# exit
logout
Connection to 34.80.85.175 closed.
root@master:~# gcloud compute ssh controller-0
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:06:30 2019 from 112.171.112.143
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:07:35 UTC; 5min ago
     Docs: https://github.com/coreos
 Main PID: 3453 (etcd)
    Tasks: 6
   Memory: 8.2M
      CPU: 1.538s
   CGroup: /system.slice/etcd.service
           └─3453 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:12:41 controller-0 etcd[3453]: f98dc20bce6225a0 is starting a new election at term 703
Mar 08 02:12:41 controller-0 etcd[3453]: f98dc20bce6225a0 became candidate at term 704
Mar 08 02:12:41 controller-0 etcd[3453]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 704
Mar 08 02:12:41 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 704
Mar 08 02:12:41 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 704
Mar 08 02:12:42 controller-0 etcd[3453]: f98dc20bce6225a0 is starting a new election at term 704
Mar 08 02:12:42 controller-0 etcd[3453]: f98dc20bce6225a0 became candidate at term 705
Mar 08 02:12:42 controller-0 etcd[3453]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 705
Mar 08 02:12:42 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 705
Mar 08 02:12:42 controller-0 etcd[3453]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 705
root@controller-0:~# ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-0:~# ll /etc/etcd/
total 20
drwxr-xr-x  2 root root 4096 Mar  8 01:54 ./
drwxr-xr-x 95 root root 4096 Mar  8 01:54 ../
-rw-r--r--  1 root root 1367 Mar  8 01:59 ca.pem
-rw-------  1 root root 1679 Mar  8 01:59 kubernetes-key.pem
-rw-r--r--  1 root root 1521 Mar  8 01:59 kubernetes.pem
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# cat /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-0 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.10:2380 \
  --listen-peer-urls https://10.240.0.10:2380 \
  --listen-client-urls https://10.240.0.10:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.10:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-0:~# systemctl restart etcd
root@controller-0:~# exit
logout
Connection to 34.80.25.2 closed.
root@master:~# gcloud compute ssh controller-1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:10:46 2019 from 112.171.112.143
root@controller-1:~#
root@controller-1:~#
root@controller-1:~# vim /etc/systemd/system/etcd.service
root@controller-1:~# systemctl restart etcd
Warning: etcd.service changed on disk. Run 'systemctl daemon-reload' to reload units.
root@controller-1:~# systemctl daemon-reload
]root@controller-1:~# systemctl restart etcd
root@controller-1:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:15:34 UTC; 4s ago
     Docs: https://github.com/coreos
 Main PID: 3542 (etcd)
    Tasks: 6
   Memory: 4.0M
      CPU: 65ms
   CGroup: /system.slice/etcd.service
           └─3542 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:15:35 controller-1 etcd[3542]: ffed16798470cab5 is starting a new election at term 517
Mar 08 02:15:35 controller-1 etcd[3542]: ffed16798470cab5 became candidate at term 518
Mar 08 02:15:35 controller-1 etcd[3542]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 518
Mar 08 02:15:35 controller-1 etcd[3542]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 518
Mar 08 02:15:35 controller-1 etcd[3542]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 518
Mar 08 02:15:37 controller-1 etcd[3542]: ffed16798470cab5 is starting a new election at term 518
Mar 08 02:15:37 controller-1 etcd[3542]: ffed16798470cab5 became candidate at term 519
Mar 08 02:15:37 controller-1 etcd[3542]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 519
Mar 08 02:15:37 controller-1 etcd[3542]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 519
Mar 08 02:15:37 controller-1 etcd[3542]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 519
root@controller-1:~# exit
logout
Connection to 34.80.85.175 closed.
root@master:~#
root@master:~# gcloud compute ssh controller-2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:04:39 2019 from 112.171.112.143
root@controller-2:~# vim /etc/systemd/system/etcd.service
root@controller-2:~# systemctl daemon-reload
root@controller-2:~# systemctl restart etcd
root@controller-2:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:16:15 UTC; 3s ago
     Docs: https://github.com/coreos
 Main PID: 3425 (etcd)
    Tasks: 6
   Memory: 4.0M
      CPU: 58ms
   CGroup: /system.slice/etcd.service
           └─3425 /usr/local/bin/etcd --name controller-2 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:16:16 controller-2 etcd[3425]: started streaming with peer f98dc20bce6225a0 (stream Message reader)
Mar 08 02:16:16 controller-2 etcd[3425]: started streaming with peer ffed16798470cab5 (writer)
Mar 08 02:16:16 controller-2 etcd[3425]: started streaming with peer ffed16798470cab5 (writer)
Mar 08 02:16:16 controller-2 etcd[3425]: started streaming with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:16:16 controller-2 etcd[3425]: started streaming with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:16:17 controller-2 etcd[3425]: 3a57933972cb5131 is starting a new election at term 510
Mar 08 02:16:17 controller-2 etcd[3425]: 3a57933972cb5131 became candidate at term 511
Mar 08 02:16:17 controller-2 etcd[3425]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 511
Mar 08 02:16:17 controller-2 etcd[3425]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 511
Mar 08 02:16:17 controller-2 etcd[3425]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 511
root@controller-2:~# exit
logout
Connection to 34.80.89.164 closed.
root@master:~# gcloud compute ssh controller-0
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:12:39 2019 from 112.171.112.143
root@controller-0:~#
root@controller-0:~# systemctl restart etcd
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:16:46 UTC; 3s ago
     Docs: https://github.com/coreos
 Main PID: 3772 (etcd)
    Tasks: 6
   Memory: 3.6M
      CPU: 58ms
   CGroup: /system.slice/etcd.service
           └─3772 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:16:48 controller-0 etcd[3772]: f98dc20bce6225a0 is starting a new election at term 874
Mar 08 02:16:48 controller-0 etcd[3772]: f98dc20bce6225a0 became candidate at term 875
Mar 08 02:16:48 controller-0 etcd[3772]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 875
Mar 08 02:16:48 controller-0 etcd[3772]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 875
Mar 08 02:16:48 controller-0 etcd[3772]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 875
Mar 08 02:16:49 controller-0 etcd[3772]: f98dc20bce6225a0 is starting a new election at term 875
Mar 08 02:16:49 controller-0 etcd[3772]: f98dc20bce6225a0 became candidate at term 876
Mar 08 02:16:49 controller-0 etcd[3772]: f98dc20bce6225a0 received MsgVoteResp from f98dc20bce6225a0 at term 876
Mar 08 02:16:49 controller-0 etcd[3772]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 876
Mar 08 02:16:49 controller-0 etcd[3772]: f98dc20bce6225a0 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 876
root@controller-0:~# ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
Error: context deadline exceeded
root@controller-0:~#
root@controller-0:~#
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# cat ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem^C
root@controller-0:~#
root@controller-0:~# cat /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-0 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.10:2380 \
  --listen-peer-urls https://10.240.0.10:2380 \
  --listen-client-urls https://10.240.0.10:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.10:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-0:~# ll /etc/etcd
total 20
drwxr-xr-x  2 root root 4096 Mar  8 01:54 ./
drwxr-xr-x 95 root root 4096 Mar  8 02:19 ../
-rw-r--r--  1 root root 1367 Mar  8 01:59 ca.pem
-rw-------  1 root root 1679 Mar  8 01:59 kubernetes-key.pem
-rw-r--r--  1 root root 1521 Mar  8 01:59 kubernetes.pem
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# systemctl restart etcd
root@controller-0:~# exit
logout
Connection to 34.80.25.2 closed.
root@master:~#
root@master:~# gcloud compute ssh controller-1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:15:02 2019 from 112.171.112.143
root@controller-1:~#
root@controller-1:~#
root@controller-1:~# cat /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-1 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.11:2380 \
  --listen-peer-urls https://10.240.0.11:2380 \
  --listen-client-urls https://10.240.0.11:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.11:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state existing \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-1:~# systemctl restart etcd ; systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:21:08 UTC; 15ms ago
     Docs: https://github.com/coreos
 Main PID: 3713 (etcd)
    Tasks: 1
   Memory: 384.0K
      CPU: 1ms
   CGroup: /system.slice/etcd.service
           └─3713 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:21:08 controller-1 systemd[1]: Stopped etcd.
Mar 08 02:21:08 controller-1 systemd[1]: Started etcd.
...skipping...
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:21:08 UTC; 15ms ago
     Docs: https://github.com/coreos
 Main PID: 3713 (etcd)
    Tasks: 1
   Memory: 384.0K
      CPU: 1ms
   CGroup: /system.slice/etcd.service
           └─3713 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:21:08 controller-1 systemd[1]: Stopped etcd.
Mar 08 02:21:08 controller-1 systemd[1]: Started etcd.
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
root@controller-1:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:21:08 UTC; 12s ago
     Docs: https://github.com/coreos
 Main PID: 3713 (etcd)
    Tasks: 6
   Memory: 4.1M
      CPU: 120ms
   CGroup: /system.slice/etcd.service
           └─3713 /usr/local/bin/etcd --name controller-1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:21:19 controller-1 etcd[3713]: ffed16798470cab5 is starting a new election at term 758
Mar 08 02:21:19 controller-1 etcd[3713]: ffed16798470cab5 became candidate at term 759
Mar 08 02:21:19 controller-1 etcd[3713]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 759
Mar 08 02:21:19 controller-1 etcd[3713]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 759
Mar 08 02:21:19 controller-1 etcd[3713]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 759
Mar 08 02:21:20 controller-1 etcd[3713]: ffed16798470cab5 is starting a new election at term 759
Mar 08 02:21:20 controller-1 etcd[3713]: ffed16798470cab5 became candidate at term 760
Mar 08 02:21:20 controller-1 etcd[3713]: ffed16798470cab5 received MsgVoteResp from ffed16798470cab5 at term 760
Mar 08 02:21:20 controller-1 etcd[3713]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to 3a57933972cb5131 at term 760
Mar 08 02:21:20 controller-1 etcd[3713]: ffed16798470cab5 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 760
root@controller-1:~# exit
logout
Connection to 34.80.85.175 closed.
root@master:~# gcloud compute ssh controller-2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:15:47 2019 from 112.171.112.143
root@controller-2:~#
root@controller-2:~#
root@controller-2:~# cat /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \
  --name controller-2 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://10.240.0.12:2380 \
  --listen-peer-urls https://10.240.0.12:2380 \
  --listen-client-urls https://10.240.0.12:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://10.240.0.12:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \
  --initial-cluster-state existing \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-2:~# systemctl restart etcd
root@controller-2:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:22:34 UTC; 4s ago
     Docs: https://github.com/coreos
 Main PID: 3644 (etcd)
    Tasks: 6
   Memory: 3.9M
      CPU: 73ms
   CGroup: /system.slice/etcd.service
           └─3644 /usr/local/bin/etcd --name controller-2 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:22:36 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 777
Mar 08 02:22:36 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 778
Mar 08 02:22:36 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 778
Mar 08 02:22:36 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 778
Mar 08 02:22:36 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 778
Mar 08 02:22:38 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 778
Mar 08 02:22:38 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 779
Mar 08 02:22:38 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 779
Mar 08 02:22:38 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 779
Mar 08 02:22:38 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 779
root@controller-2:~# journalctl -xe etcd
Failed to add match 'etcd': Invalid argument
root@controller-2:~# journalctl -xe
Mar 08 02:24:26 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 852
Mar 08 02:24:26 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 852
Mar 08 02:24:26 controller-2 etcd[3644]: publish error: etcdserver: request timed out
Mar 08 02:24:27 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 852
Mar 08 02:24:27 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 853
Mar 08 02:24:27 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 853
Mar 08 02:24:27 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 853
Mar 08 02:24:27 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 853
Mar 08 02:24:28 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 853
Mar 08 02:24:28 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 854
Mar 08 02:24:28 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 854
Mar 08 02:24:28 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 854
Mar 08 02:24:28 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 854
Mar 08 02:24:29 controller-2 etcd[3644]: health check for peer f98dc20bce6225a0 could not connect: dial tcp 10.240.0.10:2380: i/o timeout
Mar 08 02:24:29 controller-2 etcd[3644]: health check for peer ffed16798470cab5 could not connect: dial tcp 10.240.0.11:2380: i/o timeout
Mar 08 02:24:30 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 854
Mar 08 02:24:30 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 855
Mar 08 02:24:30 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 855
Mar 08 02:24:30 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 855
Mar 08 02:24:30 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 855
Mar 08 02:24:31 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 855
Mar 08 02:24:31 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 856
Mar 08 02:24:31 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 856
Mar 08 02:24:31 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 856
Mar 08 02:24:31 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 856
Mar 08 02:24:32 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 856
Mar 08 02:24:32 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 857
Mar 08 02:24:32 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 857
Mar 08 02:24:32 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 857
Mar 08 02:24:32 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 857
Mar 08 02:24:33 controller-2 etcd[3644]: publish error: etcdserver: request timed out
Mar 08 02:24:34 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 857
Mar 08 02:24:34 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 858
Mar 08 02:24:34 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 858
Mar 08 02:24:34 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 858
Mar 08 02:24:34 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 858
Mar 08 02:24:34 controller-2 etcd[3644]: health check for peer f98dc20bce6225a0 could not connect: dial tcp 10.240.0.10:2380: i/o timeout
Mar 08 02:24:34 controller-2 etcd[3644]: health check for peer ffed16798470cab5 could not connect: dial tcp 10.240.0.11:2380: i/o timeout
Mar 08 02:24:35 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 858
Mar 08 02:24:35 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 859
Mar 08 02:24:35 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 859
Mar 08 02:24:35 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 859
Mar 08 02:24:35 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 859
Mar 08 02:24:36 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 859
Mar 08 02:24:36 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 860
Mar 08 02:24:36 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 860
Mar 08 02:24:36 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 860
Mar 08 02:24:36 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 860
Mar 08 02:24:38 controller-2 etcd[3644]: 3a57933972cb5131 is starting a new election at term 860
Mar 08 02:24:38 controller-2 etcd[3644]: 3a57933972cb5131 became candidate at term 861
Mar 08 02:24:38 controller-2 etcd[3644]: 3a57933972cb5131 received MsgVoteResp from 3a57933972cb5131 at term 861
Mar 08 02:24:38 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to f98dc20bce6225a0 at term 861
Mar 08 02:24:38 controller-2 etcd[3644]: 3a57933972cb5131 [logterm: 1, index: 3] sent MsgVote request to ffed16798470cab5 at term 861

root@controller-2:~# exit
logout
Connection to 34.80.89.164 closed.
root@master:~# gcloud compute firewall-rules list --filter="network:kubernetes-the-hard-way"
NAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW                 DENY  DISABLED
kubernetes-the-hard-way-allow-external  kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp        False

To show all fields of the firewall, please show in JSON format: --format=json
To show all fields in table format, please see the examples in --help.

root@master:~# gcloud compute firewall-rules create kubernetes-the-hard-way-allow-internal \
>   --allow tcp,udp,icmp \
>   --network kubernetes-the-hard-way \
>   --source-ranges 10.240.0.0/24,10.200.0.0/16
Creating firewall...⠼Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/firewalls/kubernetes-the-hard-way-allow-internal].
Creating firewall...done.
NAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW         DENY  DISABLED
kubernetes-the-hard-way-allow-internal  kubernetes-the-hard-way  INGRESS    1000      tcp,udp,icmp        False
root@master:~#
root@master:~#
root@master:~# gcloud compute firewall-rules list --filter="network:kubernetes-the-hard-way"
NAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW                 DENY  DISABLED
kubernetes-the-hard-way-allow-external  kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp        False
kubernetes-the-hard-way-allow-internal  kubernetes-the-hard-way  INGRESS    1000      tcp,udp,icmp                False

To show all fields of the firewall, please show in JSON format: --format=json
To show all fields in table format, please see the examples in --help.

root@master:~# gcloud comppute ssh controller-0
ERROR: (gcloud) Invalid choice: 'comppute'.
Maybe you meant:
  gcloud compute ssh
  gcloud compute config-ssh

Showing 2 out of 8 suggestions.

To search the help text of gcloud commands, run:
  gcloud help -- SEARCH_TERMS
root@master:~# gcloud compute ssh controller-0
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:16:43 2019 from 112.171.112.143
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:20:25 UTC; 7min ago
     Docs: https://github.com/coreos
 Main PID: 3875 (etcd)
    Tasks: 8
   Memory: 10.4M
      CPU: 2.515s
   CGroup: /system.slice/etcd.service
           └─3875 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:26:56 controller-0 etcd[3875]: ready to serve client requests
Mar 08 02:26:56 controller-0 etcd[3875]: serving client requests on 127.0.0.1:2379
Mar 08 02:26:57 controller-0 etcd[3875]: failed to write ffed16798470cab5 on pipeline (dial tcp 10.240.0.11:2380: i/o timeout)
Mar 08 02:26:57 controller-0 etcd[3875]: peer ffed16798470cab5 became inactive
Mar 08 02:26:57 controller-0 etcd[3875]: setting up the initial cluster version to 3.3
Mar 08 02:26:57 controller-0 etcd[3875]: set the initial cluster version to 3.3
Mar 08 02:26:57 controller-0 etcd[3875]: enabled capabilities for version 3.3
Mar 08 02:26:57 controller-0 etcd[3875]: peer ffed16798470cab5 became active
Mar 08 02:26:57 controller-0 etcd[3875]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:26:57 controller-0 etcd[3875]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
root@controller-0:~# systemctl restart etcd
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:28:00 UTC; 4s ago
     Docs: https://github.com/coreos
 Main PID: 4049 (etcd)
    Tasks: 7
   Memory: 6.2M
      CPU: 173ms
   CGroup: /system.slice/etcd.service
           └─4049 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:28:01 controller-0 etcd[4049]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:28:01 controller-0 etcd[4049]: forgot to set Type=notify in systemd service file?
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 127.0.0.1:2379
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 10.240.0.10:2379
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message writer)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 writer)
Mar 08 02:28:01 controller-0 etcd[4049]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
...skipping...
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:28:00 UTC; 4s ago
     Docs: https://github.com/coreos
 Main PID: 4049 (etcd)
    Tasks: 7
   Memory: 6.2M
      CPU: 173ms
   CGroup: /system.slice/etcd.service
           └─4049 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:28:01 controller-0 etcd[4049]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:28:01 controller-0 etcd[4049]: forgot to set Type=notify in systemd service file?
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 127.0.0.1:2379
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 10.240.0.10:2379
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message writer)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 writer)
Mar 08 02:28:01 controller-0 etcd[4049]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
...skipping...
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:28:00 UTC; 4s ago
     Docs: https://github.com/coreos
 Main PID: 4049 (etcd)
    Tasks: 7
   Memory: 6.2M
      CPU: 173ms
   CGroup: /system.slice/etcd.service
           └─4049 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:28:01 controller-0 etcd[4049]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:28:01 controller-0 etcd[4049]: forgot to set Type=notify in systemd service file?
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 127.0.0.1:2379
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 10.240.0.10:2379
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message writer)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 writer)
Mar 08 02:28:01 controller-0 etcd[4049]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
root@controller-0:~# journalctl -xe etcd
Failed to add match 'etcd': Invalid argument
root@controller-0:~# journalctl -xe
Mar 08 02:28:00 controller-0 etcd[4049]: listening for client requests on 127.0.0.1:2379
Mar 08 02:28:00 controller-0 etcd[4049]: name = controller-0
Mar 08 02:28:00 controller-0 etcd[4049]: data dir = /var/lib/etcd
Mar 08 02:28:00 controller-0 etcd[4049]: member dir = /var/lib/etcd/member
Mar 08 02:28:00 controller-0 etcd[4049]: heartbeat = 100ms
Mar 08 02:28:00 controller-0 etcd[4049]: election = 1000ms
Mar 08 02:28:00 controller-0 etcd[4049]: snapshot count = 100000
Mar 08 02:28:00 controller-0 etcd[4049]: advertise client URLs = https://10.240.0.10:2379
Mar 08 02:28:00 controller-0 etcd[4049]: restarting member f98dc20bce6225a0 in cluster 3e7cc799faffb625 at commit index 9
Mar 08 02:28:00 controller-0 etcd[4049]: f98dc20bce6225a0 became follower at term 1298
Mar 08 02:28:00 controller-0 etcd[4049]: newRaft f98dc20bce6225a0 [peers: [], term: 1298, commit: 9, applied: 0, lastindex: 9, lastterm: 1298]
Mar 08 02:28:00 controller-0 etcd[4049]: simple token is not cryptographically signed
Mar 08 02:28:00 controller-0 etcd[4049]: starting server... [version: 3.3.9, cluster version: to_be_decided]
Mar 08 02:28:00 controller-0 etcd[4049]: ClientTLS: cert = /etc/etcd/kubernetes.pem, key = /etc/etcd/kubernetes-key.pem, ca = , trusted-ca = /etc/etcd/ca.pem, client-cert-auth = true, crl-file =
Mar 08 02:28:00 controller-0 etcd[4049]: added member 3a57933972cb5131 [https://10.240.0.12:2380] to cluster 3e7cc799faffb625
Mar 08 02:28:00 controller-0 etcd[4049]: starting peer 3a57933972cb5131...
Mar 08 02:28:00 controller-0 etcd[4049]: started HTTP pipelining with peer 3a57933972cb5131
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer 3a57933972cb5131 (writer)
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer 3a57933972cb5131 (writer)
Mar 08 02:28:00 controller-0 etcd[4049]: started peer 3a57933972cb5131
Mar 08 02:28:00 controller-0 etcd[4049]: added peer 3a57933972cb5131
Mar 08 02:28:00 controller-0 etcd[4049]: added member f98dc20bce6225a0 [https://10.240.0.10:2380] to cluster 3e7cc799faffb625
Mar 08 02:28:00 controller-0 etcd[4049]: added member ffed16798470cab5 [https://10.240.0.11:2380] to cluster 3e7cc799faffb625
Mar 08 02:28:00 controller-0 etcd[4049]: starting peer ffed16798470cab5...
Mar 08 02:28:00 controller-0 etcd[4049]: started HTTP pipelining with peer ffed16798470cab5
Mar 08 02:28:00 controller-0 etcd[4049]: started peer ffed16798470cab5
Mar 08 02:28:00 controller-0 etcd[4049]: added peer ffed16798470cab5
Mar 08 02:28:00 controller-0 etcd[4049]: set the initial cluster version to 3.3
Mar 08 02:28:00 controller-0 etcd[4049]: enabled capabilities for version 3.3
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer 3a57933972cb5131 (stream MsgApp v2 reader)
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer 3a57933972cb5131 (stream Message reader)
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer ffed16798470cab5 (writer)
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer ffed16798470cab5 (writer)
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:28:00 controller-0 etcd[4049]: started streaming with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:28:01 controller-0 etcd[4049]: peer 3a57933972cb5131 became active
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer 3a57933972cb5131 (stream Message writer)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer 3a57933972cb5131 (stream MsgApp v2 writer)
Mar 08 02:28:01 controller-0 etcd[4049]: raft.node: f98dc20bce6225a0 elected leader 3a57933972cb5131 at term 1298
Mar 08 02:28:01 controller-0 etcd[4049]: peer ffed16798470cab5 became active
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer 3a57933972cb5131 (stream MsgApp v2 reader)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer 3a57933972cb5131 (stream Message reader)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:28:01 controller-0 etcd[4049]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:28:01 controller-0 etcd[4049]: forgot to set Type=notify in systemd service file?
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 127.0.0.1:2379
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 10.240.0.10:2379
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message writer)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 writer)
Mar 08 02:28:01 controller-0 etcd[4049]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
root@controller-0:~# ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379
f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379
ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379
root@controller-0:~#
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:28:00 UTC; 3min 22s ago
     Docs: https://github.com/coreos
 Main PID: 4049 (etcd)
    Tasks: 7
   Memory: 9.7M
      CPU: 1.785s
   CGroup: /system.slice/etcd.service
           └─4049 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:28:01 controller-0 etcd[4049]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:28:01 controller-0 etcd[4049]: forgot to set Type=notify in systemd service file?
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 127.0.0.1:2379
Mar 08 02:28:01 controller-0 etcd[4049]: ready to serve client requests
Mar 08 02:28:01 controller-0 etcd[4049]: serving client requests on 10.240.0.10:2379
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message writer)
Mar 08 02:28:01 controller-0 etcd[4049]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 writer)
Mar 08 02:28:01 controller-0 etcd[4049]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# etcdctl put foo bar
No help topic for 'put'
root@controller-0:~# ETCDCTL_API=3 etcdctl put foo bar
Error: rpc error: code = Unavailable desc = transport is closing
root@controller-0:~# systemctl daemon-reload
root@controller-0:~# systemctl restart etcd
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:32:20 UTC; 2s ago
     Docs: https://github.com/coreos
 Main PID: 4153 (etcd)
    Tasks: 8
   Memory: 6.3M
      CPU: 136ms
   CGroup: /system.slice/etcd.service
           └─4153 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer 3a57933972cb5131 (stream MsgApp v2 reader)
Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:32:20 controller-0 etcd[4153]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:32:20 controller-0 etcd[4153]: forgot to set Type=notify in systemd service file?
Mar 08 02:32:20 controller-0 etcd[4153]: ready to serve client requests
Mar 08 02:32:20 controller-0 etcd[4153]: serving client requests on 10.240.0.10:2379
Mar 08 02:32:20 controller-0 etcd[4153]: ready to serve client requests
Mar 08 02:32:20 controller-0 etcd[4153]: serving client requests on 127.0.0.1:2379
Mar 08 02:32:20 controller-0 etcd[4153]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
...skipping...
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:32:20 UTC; 2s ago
     Docs: https://github.com/coreos
 Main PID: 4153 (etcd)
    Tasks: 8
   Memory: 6.3M
      CPU: 136ms
   CGroup: /system.slice/etcd.service
           └─4153 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer 3a57933972cb5131 (stream MsgApp v2 reader)
Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:32:20 controller-0 etcd[4153]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:32:20 controller-0 etcd[4153]: forgot to set Type=notify in systemd service file?
Mar 08 02:32:20 controller-0 etcd[4153]: ready to serve client requests
Mar 08 02:32:20 controller-0 etcd[4153]: serving client requests on 10.240.0.10:2379
Mar 08 02:32:20 controller-0 etcd[4153]: ready to serve client requests
Mar 08 02:32:20 controller-0 etcd[4153]: serving client requests on 127.0.0.1:2379
Mar 08 02:32:20 controller-0 etcd[4153]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
root@controller-0:~#
root@controller-0:~# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled)
   Active: active (running) since Fri 2019-03-08 02:32:20 UTC; 8s ago
     Docs: https://github.com/coreos
 Main PID: 4153 (etcd)
    Tasks: 8
   Memory: 7.0M
      CPU: 198ms
   CGroup: /system.slice/etcd.service
           └─4153 /usr/local/bin/etcd --name controller-0 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kubernetes-key.pem --trusted-

Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer ffed16798470cab5 (stream MsgApp v2 reader)
Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer 3a57933972cb5131 (stream MsgApp v2 reader)
Mar 08 02:32:20 controller-0 etcd[4153]: established a TCP streaming connection with peer ffed16798470cab5 (stream Message reader)
Mar 08 02:32:20 controller-0 etcd[4153]: published {Name:controller-0 ClientURLs:[https://10.240.0.10:2379]} to cluster 3e7cc799faffb625
Mar 08 02:32:20 controller-0 etcd[4153]: forgot to set Type=notify in systemd service file?
Mar 08 02:32:20 controller-0 etcd[4153]: ready to serve client requests
Mar 08 02:32:20 controller-0 etcd[4153]: serving client requests on 10.240.0.10:2379
Mar 08 02:32:20 controller-0 etcd[4153]: ready to serve client requests
Mar 08 02:32:20 controller-0 etcd[4153]: serving client requests on 127.0.0.1:2379
Mar 08 02:32:20 controller-0 etcd[4153]: f98dc20bce6225a0 initialzed peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
root@controller-0:~# ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem
3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379
f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379
ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379
root@controller-0:~# exit
logout
Connection to 34.80.25.2 closed.






root@master:~#
Using username "root".
Authenticating with public key "rsa-key-20181025"
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-46-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

20 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Fri Mar  8 09:40:28 2019 from 10.0.0.105
root@master:~# gcloud instance list
ERROR: (gcloud) Invalid choice: 'instance'.
Maybe you meant:
  gcloud compute instances list
  gcloud compute instance-groups list-instances
  gcloud app instances list
  gcloud bigtable instances list
  gcloud filestore instances list
  gcloud redis instances list
  gcloud spanner instances list
  gcloud sql instances list
  gcloud compute instance-groups list
  gcloud compute instance-templates list

Showing 10 out of 298 suggestions.

To search the help text of gcloud commands, run:
  gcloud help -- SEARCH_TERMS
root@master:~# gcloud compute instances list
NAME          ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS
controller-0  asia-east1-b  n1-standard-1               10.240.0.10  34.80.25.2    RUNNING
controller-1  asia-east1-b  n1-standard-1               10.240.0.11  34.80.85.175  RUNNING
controller-2  asia-east1-b  n1-standard-1               10.240.0.12  34.80.89.164  RUNNING
worker-0      asia-east1-b  n1-standard-1               10.240.0.20  34.80.0.204   RUNNING
worker-1      asia-east1-b  n1-standard-1               10.240.0.21  34.80.34.179  RUNNING
worker-2      asia-east1-b  n1-standard-1               10.240.0.22  34.80.89.105  RUNNING
root@master:~# gcloud compute ssh controller-0
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:27:39 2019 from 112.171.112.143
root@controller-0:~#
root@controller-0:~# mkdir -p /etc/kubernetes/config
root@controller-0:~# wget -q --show-progress --https-only --timestamping \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-apiserver" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-controller-manager" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl"
kube-apiserver                                            100%[=====================================================================================================================================>] 183.84M  57.8MB/s    in 3.2s
kube-controller-manager                                   100%[=====================================================================================================================================>] 155.39M  64.0MB/s    in 2.4s
kube-scheduler                                            100%[=====================================================================================================================================>]  54.53M  51.6MB/s    in 1.1s
kubectl                                                   100%[=====================================================================================================================================>]  54.69M  40.2MB/s    in 1.4s
root@controller-0:~#   chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
root@controller-0:~#   sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
root@controller-0:~#
root@controller-0:~#
root@controller-0:~#   sudo mkdir -p /var/lib/kubernetes/
root@controller-0:~# sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
>     service-account-key.pem service-account.pem \
>     encryption-config.yaml /var/lib/kubernetes/
root@controller-0:~#
root@controller-0:~# INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
root@controller-0:~#
root@controller-0:~# cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
> [Unit]
> Description=Kubernetes API Server
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-apiserver \\
>   --advertise-address=${INTERNAL_IP} \\
>   --allow-privileged=true \\
>   --apiserver-count=3 \\
>   --audit-log-maxage=30 \\
>   --audit-log-maxbackup=3 \\
>   --audit-log-maxsize=100 \\
>   --audit-log-path=/var/log/audit.log \\
>   --authorization-mode=Node,RBAC \\
>   --bind-address=0.0.0.0 \\
>   --client-ca-file=/var/lib/kubernetes/ca.pem \\
>   --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
>   --enable-swagger-ui=true \\
>   --etcd-cafile=/var/lib/kubernetes/ca.pem \\
>   --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
>   --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
>   --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \\
>   --event-ttl=1h \\
>   --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
>   --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
>   --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
>   --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
>   --kubelet-https=true \\
>   --runtime-config=api/all \\
>   --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
>   --service-cluster-ip-range=10.32.0.0/24 \\
>   --service-node-port-range=30000-32767 \\
>   --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
>   --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
  --advertise-address=10.240.0.10 \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/audit.log \
  --authorization-mode=Node,RBAC \
  --bind-address=0.0.0.0 \
  --client-ca-file=/var/lib/kubernetes/ca.pem \
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --enable-swagger-ui=true \
  --etcd-cafile=/var/lib/kubernetes/ca.pem \
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \
  --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \
  --event-ttl=1h \
  --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \
  --kubelet-https=true \
  --runtime-config=api/all \
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \
  --service-cluster-ip-range=10.32.0.0/24 \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
root@controller-0:~# cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
> [Unit]
> Description=Kubernetes Controller Manager
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-controller-manager \\
>   --address=0.0.0.0 \\
>   --cluster-cidr=10.200.0.0/16 \\
>   --cluster-name=kubernetes \\
>   --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
>   --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
>   --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
>   --leader-elect=true \\
>   --root-ca-file=/var/lib/kubernetes/ca.pem \\
>   --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
>   --service-cluster-ip-range=10.32.0.0/24 \\
>   --use-service-account-credentials=true \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --address=0.0.0.0 \
  --cluster-cidr=10.200.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --root-ca-file=/var/lib/kubernetes/ca.pem \
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \
  --service-cluster-ip-range=10.32.0.0/24 \
  --use-service-account-credentials=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-0:~#
root@controller-0:~# sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/
root@controller-0:~# cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
> apiVersion: componentconfig/v1alpha1
> kind: KubeSchedulerConfiguration
> clientConnection:
>   kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
> leaderElection:
>   leaderElect: true
> EOF
apiVersion: componentconfig/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
root@controller-0:~#
root@controller-0:~# cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
> [Unit]
> Description=Kubernetes Scheduler
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-scheduler \\
>   --config=/etc/kubernetes/config/kube-scheduler.yaml \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --config=/etc/kubernetes/config/kube-scheduler.yaml \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-0:~#
root@controller-0:~#   sudo systemctl daemon-reload
root@controller-0:~#   sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /etc/systemd/system/kube-apiserver.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /etc/systemd/system/kube-controller-manager.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /etc/systemd/system/kube-scheduler.service.
root@controller-0:~#   sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# sudo apt-get install -y nginx
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
  fontconfig-config fonts-dejavu-core libfontconfig1 libgd3 libjbig0 libjpeg-turbo8 libjpeg8 libtiff5 libvpx3 libxpm4 nginx-common nginx-core
Suggested packages:
  libgd-tools fcgiwrap nginx-doc ssl-cert
The following NEW packages will be installed:
  fontconfig-config fonts-dejavu-core libfontconfig1 libgd3 libjbig0 libjpeg-turbo8 libjpeg8 libtiff5 libvpx3 libxpm4 nginx nginx-common nginx-core
0 upgraded, 13 newly installed, 0 to remove and 0 not upgraded.
Need to get 2858 kB of archives.
After this operation, 9311 kB of additional disk space will be used.
Get:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libjpeg-turbo8 amd64 1.4.2-0ubuntu3.1 [111 kB]
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial/main amd64 libjbig0 amd64 2.1-3.1 [26.6 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial/main amd64 fonts-dejavu-core all 2.35-1 [1039 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 fontconfig-config all 2.11.94-0ubuntu1.1 [49.9 kB]
Get:5 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libfontconfig1 amd64 2.11.94-0ubuntu1.1 [131 kB]
Get:6 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial/main amd64 libjpeg8 amd64 8c-2ubuntu8 [2194 B]
Get:7 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libtiff5 amd64 4.0.6-1ubuntu0.5 [149 kB]
Get:8 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial/main amd64 libvpx3 amd64 1.5.0-2ubuntu1 [732 kB]
Get:9 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libxpm4 amd64 1:3.5.11-1ubuntu0.16.04.1 [33.8 kB]
Get:10 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libgd3 amd64 2.1.1-4ubuntu0.16.04.11 [126 kB]
Get:11 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 nginx-common all 1.10.3-0ubuntu0.16.04.3 [26.7 kB]
Get:12 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 nginx-core amd64 1.10.3-0ubuntu0.16.04.3 [429 kB]
Get:13 http://asia-east1.gce.archive.ubuntu.com/ubuntu xenial-updates/main amd64 nginx all 1.10.3-0ubuntu0.16.04.3 [3506 B]
Fetched 2858 kB in 5s (509 kB/s)
perl: warning: Setting locale failed.
perl: warning: Please check that your locale settings:
        LANGUAGE = (unset),
        LC_ALL = (unset),
        LC_TIME = "ko_KR.UTF-8",
        LC_MONETARY = "ko_KR.UTF-8",
        LC_ADDRESS = "ko_KR.UTF-8",
        LC_TELEPHONE = "ko_KR.UTF-8",
        LC_NAME = "ko_KR.UTF-8",
        LC_MEASUREMENT = "ko_KR.UTF-8",
        LC_IDENTIFICATION = "ko_KR.UTF-8",
        LC_NUMERIC = "ko_KR.UTF-8",
        LC_PAPER = "ko_KR.UTF-8",
        LANG = "en_US.UTF-8"
    are supported and installed on your system.
perl: warning: Falling back to a fallback locale ("en_US.UTF-8").
locale: Cannot set LC_ALL to default locale: No such file or directory
Preconfiguring packages ...
Selecting previously unselected package libjpeg-turbo8:amd64.
(Reading database ... 71068 files and directories currently installed.)
Preparing to unpack .../libjpeg-turbo8_1.4.2-0ubuntu3.1_amd64.deb ...
Unpacking libjpeg-turbo8:amd64 (1.4.2-0ubuntu3.1) ...
Selecting previously unselected package libjbig0:amd64.
Preparing to unpack .../libjbig0_2.1-3.1_amd64.deb ...
Unpacking libjbig0:amd64 (2.1-3.1) ...
Selecting previously unselected package fonts-dejavu-core.
Preparing to unpack .../fonts-dejavu-core_2.35-1_all.deb ...
Unpacking fonts-dejavu-core (2.35-1) ...
Selecting previously unselected package fontconfig-config.
Preparing to unpack .../fontconfig-config_2.11.94-0ubuntu1.1_all.deb ...
Unpacking fontconfig-config (2.11.94-0ubuntu1.1) ...
Selecting previously unselected package libfontconfig1:amd64.
Preparing to unpack .../libfontconfig1_2.11.94-0ubuntu1.1_amd64.deb ...
Unpacking libfontconfig1:amd64 (2.11.94-0ubuntu1.1) ...
Selecting previously unselected package libjpeg8:amd64.
Preparing to unpack .../libjpeg8_8c-2ubuntu8_amd64.deb ...
Unpacking libjpeg8:amd64 (8c-2ubuntu8) ...
Selecting previously unselected package libtiff5:amd64.
Preparing to unpack .../libtiff5_4.0.6-1ubuntu0.5_amd64.deb ...
Unpacking libtiff5:amd64 (4.0.6-1ubuntu0.5) ...
Selecting previously unselected package libvpx3:amd64.
Preparing to unpack .../libvpx3_1.5.0-2ubuntu1_amd64.deb ...
Unpacking libvpx3:amd64 (1.5.0-2ubuntu1) ...
Selecting previously unselected package libxpm4:amd64.
Preparing to unpack .../libxpm4_1%3a3.5.11-1ubuntu0.16.04.1_amd64.deb ...
Unpacking libxpm4:amd64 (1:3.5.11-1ubuntu0.16.04.1) ...
Selecting previously unselected package libgd3:amd64.
Preparing to unpack .../libgd3_2.1.1-4ubuntu0.16.04.11_amd64.deb ...
Unpacking libgd3:amd64 (2.1.1-4ubuntu0.16.04.11) ...
Selecting previously unselected package nginx-common.
Preparing to unpack .../nginx-common_1.10.3-0ubuntu0.16.04.3_all.deb ...
Unpacking nginx-common (1.10.3-0ubuntu0.16.04.3) ...
Selecting previously unselected package nginx-core.
Preparing to unpack .../nginx-core_1.10.3-0ubuntu0.16.04.3_amd64.deb ...
Unpacking nginx-core (1.10.3-0ubuntu0.16.04.3) ...
Selecting previously unselected package nginx.
Preparing to unpack .../nginx_1.10.3-0ubuntu0.16.04.3_all.deb ...
Unpacking nginx (1.10.3-0ubuntu0.16.04.3) ...
Processing triggers for libc-bin (2.23-0ubuntu11) ...
Processing triggers for man-db (2.7.5-1) ...
Processing triggers for systemd (229-4ubuntu21.16) ...
Processing triggers for ureadahead (0.100.0-19) ...
Processing triggers for ufw (0.35-0ubuntu2) ...
Setting up libjpeg-turbo8:amd64 (1.4.2-0ubuntu3.1) ...
Setting up libjbig0:amd64 (2.1-3.1) ...
Setting up fonts-dejavu-core (2.35-1) ...
Setting up fontconfig-config (2.11.94-0ubuntu1.1) ...
Setting up libfontconfig1:amd64 (2.11.94-0ubuntu1.1) ...
Setting up libjpeg8:amd64 (8c-2ubuntu8) ...
Setting up libtiff5:amd64 (4.0.6-1ubuntu0.5) ...
Setting up libvpx3:amd64 (1.5.0-2ubuntu1) ...
Setting up libxpm4:amd64 (1:3.5.11-1ubuntu0.16.04.1) ...
Setting up libgd3:amd64 (2.1.1-4ubuntu0.16.04.11) ...
Setting up nginx-common (1.10.3-0ubuntu0.16.04.3) ...
locale: Cannot set LC_ALL to default locale: No such file or directory
Setting up nginx-core (1.10.3-0ubuntu0.16.04.3) ...
Setting up nginx (1.10.3-0ubuntu0.16.04.3) ...
Processing triggers for libc-bin (2.23-0ubuntu11) ...
Processing triggers for systemd (229-4ubuntu21.16) ...
Processing triggers for ureadahead (0.100.0-19) ...
Processing triggers for ufw (0.35-0ubuntu2) ...
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# cat > kubernetes.default.svc.cluster.local <<EOF
> server {
>   listen      80;
>   server_name kubernetes.default.svc.cluster.local;
>
>   location /healthz {
>      proxy_pass                    https://127.0.0.1:6443/healthz;
>      proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem;
>   }
> }
> EOF
root@controller-0:~# {
>   sudo mv kubernetes.default.svc.cluster.local \
>     /etc/nginx/sites-available/kubernetes.default.svc.cluster.local
>
>   sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/
> }
root@controller-0:~# sudo systemctl restart nginx
root@controller-0:~# sudo systemctl enable nginx
Synchronizing state of nginx.service with SysV init with /lib/systemd/systemd-sysv-install...
Executing /lib/systemd/systemd-sysv-install enable nginx
perl: warning: Setting locale failed.
perl: warning: Please check that your locale settings:
        LANGUAGE = (unset),
        LC_ALL = (unset),
        LC_TIME = "ko_KR.UTF-8",
        LC_MONETARY = "ko_KR.UTF-8",
        LC_ADDRESS = "ko_KR.UTF-8",
        LC_TELEPHONE = "ko_KR.UTF-8",
        LC_NAME = "ko_KR.UTF-8",
        LC_MEASUREMENT = "ko_KR.UTF-8",
        LC_IDENTIFICATION = "ko_KR.UTF-8",
        LC_NUMERIC = "ko_KR.UTF-8",
        LC_PAPER = "ko_KR.UTF-8",
        LANG = "en_US.UTF-8"
    are supported and installed on your system.
perl: warning: Falling back to a fallback locale ("en_US.UTF-8").
perl: warning: Setting locale failed.
perl: warning: Please check that your locale settings:
        LANGUAGE = (unset),
        LC_ALL = (unset),
        LC_TIME = "ko_KR.UTF-8",
        LC_MONETARY = "ko_KR.UTF-8",
        LC_ADDRESS = "ko_KR.UTF-8",
        LC_TELEPHONE = "ko_KR.UTF-8",
        LC_NAME = "ko_KR.UTF-8",
        LC_MEASUREMENT = "ko_KR.UTF-8",
        LC_IDENTIFICATION = "ko_KR.UTF-8",
        LC_NUMERIC = "ko_KR.UTF-8",
        LC_PAPER = "ko_KR.UTF-8",
        LANG = "en_US.UTF-8"
    are supported and installed on your system.
perl: warning: Falling back to a fallback locale ("en_US.UTF-8").
root@controller-0:~# kubectl get componentstatuses --kubeconfig admin.kubeconfig
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-1               Healthy   {"health":"true"}
etcd-2               Healthy   {"health":"true"}
etcd-0               Healthy   {"health":"true"}
root@controller-0:~# curl -H "Host: kubernetes.default.svc.cluster.local" -i http://127.0.0.1/healthz
HTTP/1.1 200 OK
Server: nginx/1.10.3 (Ubuntu)
Date: Fri, 08 Mar 2019 04:16:42 GMT
Content-Type: text/plain; charset=utf-8
Content-Length: 2
Connection: keep-alive

okroot@controller-0:~# exit
logout
Connection to 34.80.25.2 closed.
root@master:~#
root@master:~# gcloud compute ssh controller-1
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:20:32 2019 from 112.171.112.143
root@controller-1:~#
root@controller-1:~# sudo mkdir -p /etc/kubernetes/config
root@controller-1:~# wget -q --show-progress --https-only --timestamping \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-apiserver" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-controller-manager" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl"
kube-apiserver                                            100%[=====================================================================================================================================>] 183.84M   131MB/s    in 1.4s
kube-controller-manager                                   100%[=====================================================================================================================================>] 155.39M   127MB/s    in 1.2s
kube-scheduler                                            100%[=====================================================================================================================================>]  54.53M   113MB/s    in 0.5s
kubectl                                                   100%[=====================================================================================================================================>]  54.69M   117MB/s    in 0.5s
root@controller-1:~# {
>   chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
>   sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
> }
root@controller-1:~#
root@controller-1:~#
root@controller-1:~# {
>   sudo mkdir -p /var/lib/kubernetes/
>
>   sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
>     service-account-key.pem service-account.pem \
>     encryption-config.yaml /var/lib/kubernetes/
> }
root@controller-1:~# INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
root@controller-1:~#
root@controller-1:~# cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
> [Unit]
> Description=Kubernetes API Server
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-apiserver \\
>   --advertise-address=${INTERNAL_IP} \\
>   --allow-privileged=true \\
>   --apiserver-count=3 \\
>   --audit-log-maxage=30 \\
>   --audit-log-maxbackup=3 \\
>   --audit-log-maxsize=100 \\
>   --audit-log-path=/var/log/audit.log \\
>   --authorization-mode=Node,RBAC \\
>   --bind-address=0.0.0.0 \\
>   --client-ca-file=/var/lib/kubernetes/ca.pem \\
>   --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
>   --enable-swagger-ui=true \\
>   --etcd-cafile=/var/lib/kubernetes/ca.pem \\
>   --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
>   --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
>   --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \\
>   --event-ttl=1h \\
>   --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
>   --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
>   --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
>   --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
>   --kubelet-https=true \\
>   --runtime-config=api/all \\
>   --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
>   --service-cluster-ip-range=10.32.0.0/24 \\
>   --service-node-port-range=30000-32767 \\
>   --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
>   --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
  --advertise-address=10.240.0.11 \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/audit.log \
  --authorization-mode=Node,RBAC \
  --bind-address=0.0.0.0 \
  --client-ca-file=/var/lib/kubernetes/ca.pem \
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --enable-swagger-ui=true \
  --etcd-cafile=/var/lib/kubernetes/ca.pem \
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \
  --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \
  --event-ttl=1h \
  --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \
  --kubelet-https=true \
  --runtime-config=api/all \
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \
  --service-cluster-ip-range=10.32.0.0/24 \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-1:~# sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
root@controller-1:~#
root@controller-1:~# cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
> [Unit]
> Description=Kubernetes Controller Manager
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-controller-manager \\
>   --address=0.0.0.0 \\
>   --cluster-cidr=10.200.0.0/16 \\
>   --cluster-name=kubernetes \\
>   --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
>   --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
>   --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
>   --leader-elect=true \\
>   --root-ca-file=/var/lib/kubernetes/ca.pem \\
>   --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
>   --service-cluster-ip-range=10.32.0.0/24 \\
>   --use-service-account-credentials=true \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --address=0.0.0.0 \
  --cluster-cidr=10.200.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --root-ca-file=/var/lib/kubernetes/ca.pem \
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \
  --service-cluster-ip-range=10.32.0.0/24 \
  --use-service-account-credentials=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-1:~#
root@controller-1:~# sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/
root@controller-1:~# cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
> apiVersion: componentconfig/v1alpha1
> kind: KubeSchedulerConfiguration
> clientConnection:
>   kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
> leaderElection:
>   leaderElect: true
> EOF
apiVersion: componentconfig/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
root@controller-1:~#
root@controller-1:~# cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
> [Unit]
> Description=Kubernetes Scheduler
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-scheduler \\
>   --config=/etc/kubernetes/config/kube-scheduler.yaml \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --config=/etc/kubernetes/config/kube-scheduler.yaml \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-1:~# {
>   sudo systemctl daemon-reload
>   sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
>   sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
> }
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /etc/systemd/system/kube-apiserver.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /etc/systemd/system/kube-controller-manager.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /etc/systemd/system/kube-scheduler.service.
root@controller-1:~# exit
logout
Connection to 34.80.85.175 closed.
root@master:~# gcloud compute ssh controller-2
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 02:22:17 2019 from 112.171.112.143
root@controller-2:~# sudo mkdir -p /etc/kubernetes/config
root@controller-2:~# wget -q --show-progress --https-only --timestamping \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-apiserver" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-controller-manager" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler" \
>   "https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl"
kube-apiserver                                            100%[=====================================================================================================================================>] 183.84M   114MB/s    in 1.6s
kube-controller-manager                                   100%[=====================================================================================================================================>] 155.39M   124MB/s    in 1.3s
kube-scheduler                                            100%[=====================================================================================================================================>]  54.53M   117MB/s    in 0.5s
kubectl                                                   100%[=====================================================================================================================================>]  54.69M   114MB/s    in 0.5s
root@controller-2:~# {
>   chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
>   sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
> }
root@controller-2:~# {
>   sudo mkdir -p /var/lib/kubernetes/
>
>   sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
>     service-account-key.pem service-account.pem \
>     encryption-config.yaml /var/lib/kubernetes/
> }
root@controller-2:~# INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
root@controller-2:~#
root@controller-2:~# cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
> [Unit]
> Description=Kubernetes API Server
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-apiserver \\
>   --advertise-address=${INTERNAL_IP} \\
>   --allow-privileged=true \\
>   --apiserver-count=3 \\
>   --audit-log-maxage=30 \\
>   --audit-log-maxbackup=3 \\
>   --audit-log-maxsize=100 \\
>   --audit-log-path=/var/log/audit.log \\
>   --authorization-mode=Node,RBAC \\
>   --bind-address=0.0.0.0 \\
>   --client-ca-file=/var/lib/kubernetes/ca.pem \\
>   --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
>   --enable-swagger-ui=true \\
>   --etcd-cafile=/var/lib/kubernetes/ca.pem \\
>   --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
>   --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
>   --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \\
>   --event-ttl=1h \\
>   --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
>   --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
>   --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
>   --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
>   --kubelet-https=true \\
>   --runtime-config=api/all \\
>   --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
>   --service-cluster-ip-range=10.32.0.0/24 \\
>   --service-node-port-range=30000-32767 \\
>   --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
>   --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
  --advertise-address=10.240.0.12 \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/audit.log \
  --authorization-mode=Node,RBAC \
  --bind-address=0.0.0.0 \
  --client-ca-file=/var/lib/kubernetes/ca.pem \
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --enable-swagger-ui=true \
  --etcd-cafile=/var/lib/kubernetes/ca.pem \
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \
  --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \
  --event-ttl=1h \
  --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \
  --kubelet-https=true \
  --runtime-config=api/all \
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \
  --service-cluster-ip-range=10.32.0.0/24 \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-2:~# sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
root@controller-2:~# cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
> [Unit]
> Description=Kubernetes Controller Manager
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-controller-manager \\
>   --address=0.0.0.0 \\
>   --cluster-cidr=10.200.0.0/16 \\
>   --cluster-name=kubernetes \\
>   --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
>   --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
>   --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
>   --leader-elect=true \\
>   --root-ca-file=/var/lib/kubernetes/ca.pem \\
>   --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
>   --service-cluster-ip-range=10.32.0.0/24 \\
>   --use-service-account-credentials=true \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --address=0.0.0.0 \
  --cluster-cidr=10.200.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --root-ca-file=/var/lib/kubernetes/ca.pem \
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \
  --service-cluster-ip-range=10.32.0.0/24 \
  --use-service-account-credentials=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-2:~# sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/
root@controller-2:~# cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
> apiVersion: componentconfig/v1alpha1
> kind: KubeSchedulerConfiguration
> clientConnection:
>   kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
> leaderElection:
>   leaderElect: true
> EOF
apiVersion: componentconfig/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
root@controller-2:~# cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
> [Unit]
> Description=Kubernetes Scheduler
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-scheduler \\
>   --config=/etc/kubernetes/config/kube-scheduler.yaml \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --config=/etc/kubernetes/config/kube-scheduler.yaml \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@controller-2:~# {
>   sudo systemctl daemon-reload
>   sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
>   sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
> }
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /etc/systemd/system/kube-apiserver.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /etc/systemd/system/kube-controller-manager.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /etc/systemd/system/kube-scheduler.service.




Using username "root".
Authenticating with public key "rsa-key-20181025"
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-46-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

20 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Fri Mar  8 13:09:58 2019 from 10.0.0.105
root@master:~# gcloud compute ssh controller-0
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.

New release '18.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Mar  8 04:21:50 2019 from 112.171.112.143
root@controller-0:~#
root@controller-0:~# gcloud config get-value
ERROR: (gcloud.config.get-value) argument SECTION/PROPERTY: Must be specified.
Usage: gcloud config get-value SECTION/PROPERTY [optional flags]
  optional flags may be  --help

For detailed information on this command and its flags, run:
  gcloud config get-value --help
root@controller-0:~# gcloud config get-value compute/regin
ERROR: (gcloud.config.get-value) Section [compute] has no property [regin].
root@controller-0:~# gcloud config get-value compute/region
(unset)
root@controller-0:~# gcloud config set compute/zone --help
root@controller-0:~# gcloud config set compute/zone --help
root@controller-0:~# gcloud compute zone list
ERROR: (gcloud.compute) Invalid choice: 'zone'. Did you mean 'zones'?
Usage: gcloud compute [optional flags] <group | command>
  group may be           accelerator-types | addresses | backend-buckets |
                         backend-services | commitments | disk-types | disks |
                         firewall-rules | forwarding-rules | health-checks |
                         http-health-checks | https-health-checks | images |
                         instance-groups | instance-templates | instances |
                         interconnects | machine-types | networks | operations |
                         os-login | project-info | regions | routers | routes |
                         shared-vpc | snapshots | sole-tenancy |
                         ssl-certificates | ssl-policies | target-http-proxies |
                         target-https-proxies | target-instances |
                         target-pools | target-ssl-proxies |
                         target-tcp-proxies | target-vpn-gateways | tpus |
                         url-maps | vpn-tunnels | xpn | zones
  command may be         config-ssh | connect-to-serial-port | copy-files |
                         reset-windows-password | scp | sign-url | ssh

For detailed information on this command and its flags, run:
  gcloud compute --help
root@controller-0:~# gcloud config set compute/zone --help
root@controller-0:~# gcloud compute zones list
NAME                       REGION                   STATUS  NEXT_MAINTENANCE  TURNDOWN_DATE
us-east1-b                 us-east1                 UP
us-east1-c                 us-east1                 UP
us-east1-d                 us-east1                 UP
us-east4-c                 us-east4                 UP
us-east4-b                 us-east4                 UP
us-east4-a                 us-east4                 UP
us-central1-c              us-central1              UP
us-central1-a              us-central1              UP
us-central1-f              us-central1              UP
us-central1-b              us-central1              UP
us-west1-b                 us-west1                 UP
us-west1-c                 us-west1                 UP
us-west1-a                 us-west1                 UP
europe-west4-a             europe-west4             UP
europe-west4-b             europe-west4             UP
europe-west4-c             europe-west4             UP
europe-west1-b             europe-west1             UP
europe-west1-d             europe-west1             UP
europe-west1-c             europe-west1             UP
europe-west3-c             europe-west3             UP
europe-west3-a             europe-west3             UP
europe-west3-b             europe-west3             UP
europe-west2-c             europe-west2             UP
europe-west2-b             europe-west2             UP
europe-west2-a             europe-west2             UP
asia-east1-b               asia-east1               UP
asia-east1-a               asia-east1               UP
asia-east1-c               asia-east1               UP
asia-southeast1-b          asia-southeast1          UP
asia-southeast1-a          asia-southeast1          UP
asia-southeast1-c          asia-southeast1          UP
asia-northeast1-b          asia-northeast1          UP
asia-northeast1-c          asia-northeast1          UP
asia-northeast1-a          asia-northeast1          UP
asia-south1-c              asia-south1              UP
asia-south1-b              asia-south1              UP
asia-south1-a              asia-south1              UP
australia-southeast1-b     australia-southeast1     UP
australia-southeast1-c     australia-southeast1     UP
australia-southeast1-a     australia-southeast1     UP
southamerica-east1-b       southamerica-east1       UP
southamerica-east1-c       southamerica-east1       UP
southamerica-east1-a       southamerica-east1       UP
asia-east2-a               asia-east2               UP
asia-east2-b               asia-east2               UP
asia-east2-c               asia-east2               UP
europe-north1-a            europe-north1            UP
europe-north1-b            europe-north1            UP
europe-north1-c            europe-north1            UP
europe-west6-a             europe-west6             UP
europe-west6-b             europe-west6             UP
europe-west6-c             europe-west6             UP
northamerica-northeast1-a  northamerica-northeast1  UP
northamerica-northeast1-b  northamerica-northeast1  UP
northamerica-northeast1-c  northamerica-northeast1  UP
us-west2-a                 us-west2                 UP
us-west2-b                 us-west2                 UP
us-west2-c                 us-west2                 UP
root@controller-0:~# gcloud config set compute/zone asia-east1-b
Updated property [compute/zone].
root@controller-0:~# gcloud compute forwarding-rules list
Listed 0 items.
root@controller-0:~# gcloud compute target-
target-http-proxies    target-https-proxies   target-instances       target-pools           target-ssl-proxies     target-tcp-proxies     target-vpn-gateways
root@controller-0:~# gcloud compute target-
target-http-proxies    target-https-proxies   target-instances       target-pools           target-ssl-proxies     target-tcp-proxies     target-vpn-gateways
root@controller-0:~# gcloud compute target-
target-http-proxies    target-https-proxies   target-instances       target-pools           target-ssl-proxies     target-tcp-proxies     target-vpn-gateways
root@controller-0:~# gcloud compute target-pools list
NAME                    REGION      SESSION_AFFINITY  BACKUP  HEALTH_CHECKS
kubernetes-target-pool  asia-east1  NONE                      kubernetes
root@controller-0:~# gcloud compute target-pools delete kubernetes-target-pool --region=asia-east1
The following target pools will be deleted:
 - [kubernetes-target-pool] in [asia-east1]

Do you want to continue (Y/n)?  Y

Deleted [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/regions/asia-east1/targetPools/kubernetes-target-pool].
root@controller-0:~# gcloud compute firewall-rules list
NAME                                        NETWORK                  DIRECTION  PRIORITY  ALLOW                         DENY
default-allow-icmp                          default                  INGRESS    65534     icmp
default-allow-internal                      default                  INGRESS    65534     tcp:0-65535,udp:0-65535,icmp
default-allow-rdp                           default                  INGRESS    65534     tcp:3389
default-allow-ssh                           default                  INGRESS    65534     tcp:22
kubernetes-the-hard-way-allow-external      kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp
kubernetes-the-hard-way-allow-health-check  kubernetes-the-hard-way  INGRESS    1000      tcp
kubernetes-the-hard-way-allow-internal      kubernetes-the-hard-way  INGRESS    1000      tcp,udp,icmp

To show all fields of the firewall, please show in JSON format: --format=json
To show all fields in table format, please see the examples in --help.

root@controller-0:~# gcloud compute firewall-rules delete kubernetes-the-hard-way-allow-health-check
The following firewalls will be deleted:
 - [kubernetes-the-hard-way-allow-health-check]

Do you want to continue (Y/n)?  Y

Deleted [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/firewalls/kubernetes-the-hard-way-allow-health-check].
root@controller-0:~# gcloud compute http-health-checks list
NAME        HOST                                  PORT  REQUEST_PATH
kubernetes  kubernetes.default.svc.cluster.local  80    /healthz
root@controller-0:~# gcloud compute http-health-checks delete kubernetes
The following http health checks will be deleted:
 - [kubernetes]

Do you want to continue (Y/n)?  Y

Deleted [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/httpHealthChecks/kubernetes].
root@controller-0:~# gcloud config get-value compute/region
(unset)
root@controller-0:~# gcloud config get-value compute/zone
asia-east1-b
root@controller-0:~# gcloud config get-value compute/region
(unset)
root@controller-0:~# gcloud config set compute/region
ERROR: (gcloud.config.set) argument VALUE: Must be specified.
Usage: gcloud config set SECTION/PROPERTY VALUE [optional flags]
  optional flags may be  --help | --installation

For detailed information on this command and its flags, run:
  gcloud config set --help
root@controller-0:~# gcloud config set compute/region --help
root@controller-0:~# gcloud config set compute/region --help
root@controller-0:~# gcloud config set compute/region asia-east1-b
Updated property [compute/region].
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# {
>   KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
>     --region $(gcloud config get-value compute/region) \
>     --format 'value(address)')
>
>   gcloud compute http-health-checks create kubernetes \
>     --description "Kubernetes Health Check" \
>     --host "kubernetes.default.svc.cluster.local" \
>     --request-path "/healthz"
>
>   gcloud compute firewall-rules create kubernetes-the-hard-way-allow-health-check \
>     --network kubernetes-the-hard-way \
>     --source-ranges 209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 \
>     --allow tcp
>
>   gcloud compute target-pools create kubernetes-target-pool \
>     --http-health-check kubernetes
>
>   gcloud compute target-pools add-instances kubernetes-target-pool \
>    --instances controller-0,controller-1,controller-2
>
>   gcloud compute forwarding-rules create kubernetes-forwarding-rule \
>     --address ${KUBERNETES_PUBLIC_ADDRESS} \
>     --ports 6443 \
>     --region $(gcloud config get-value compute/region) \
>     --target-pool kubernetes-target-pool
> }
ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:
 - Invalid value for field 'region': 'asia-east1-b'. Unknown region.

Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/httpHealthChecks/kubernetes].
NAME        HOST                                  PORT  REQUEST_PATH
kubernetes  kubernetes.default.svc.cluster.local  80    /healthz
Creating firewall...aborted by ctrl-c.
ERROR: (gcloud.compute.firewall-rules.create) Aborted by user.
ERROR: (gcloud.compute.target-pools.create) Could not fetch resource:
 - Invalid value for field 'region': 'asia-east1-b'. Unknown region.

ERROR: (gcloud.compute.target-pools.add-instances) Could not fetch resource:
 - The resource 'projects/velvety-column-233813/regions/asia-east1/targetPools/kubernetes-target-pool' was not found

ERROR: (gcloud.compute.forwarding-rules.create) argument --address: expected one argument
Usage: gcloud compute forwarding-rules create NAME (--backend-service=BACKEND_SERVICE | --target-http-proxy=TARGET_HTTP_PROXY | --target-https-proxy=TARGET_HTTPS_PROXY | --target-instance=TARGET_INSTANCE | --target-pool=TARGET_POOL | --target-ssl-proxy=TARGET_SSL_PROXY | --target-tcp-proxy=TARGET_TCP_PROXY | --target-vpn-gateway=TARGET_VPN_GATEWAY) [optional flags]
  optional flags may be  --address | --address-region | --backend-service |
                         --backend-service-region | --description | --global |
                         --global-address | --global-backend-service | --help |
                         --ip-protocol | --ip-version |
                         --load-balancing-scheme | --network | --network-tier |
                         --port-range | --ports | --region | --subnet |
                         --subnet-region | --target-http-proxy |
                         --target-https-proxy | --target-instance |
                         --target-instance-zone | --target-pool |
                         --target-pool-region | --target-ssl-proxy |
                         --target-tcp-proxy | --target-vpn-gateway |
                         --target-vpn-gateway-region

For detailed information on this command and its flags, run:
  gcloud compute forwarding-rules create --help
root@controller-0:~# gcloud config set compute/region asia-east1
Updated property [compute/region].
root@controller-0:~# {   KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
    --region $(gcloud config get-value compute/region) \
    --format 'value(address)');    gcloud compute http-health-checks create kubernetes     --description "Kubernetes Health Check"     --host "kubernetes.default.svc.cluster.local"     --request-path "/healthz";    gcloud compute firewall-rules create kubernetes-the-hard-way-allow-health-check     --network kubernetes-the-hard-way     --source-ranges 209.85.152.0/22,209.85.204.0/22,35.191.0.0/16     --allow tcp;    gcloud compute target-pools create kubernetes-target-pool     --http-health-check kubernetes;    gcloud compute target-pools add-instances kubernetes-target-pool    --instances controller-0,controller-1,controller-2;    gcloud compute forwarding-rules create kubernetes-forwarding-rule     --address ${KUBERNETES_PUBLIC_ADDRESS}     --ports 6443     --region $(gcloud config get-value compute/region)     --target-pool kubernetes-target-pool; }
ERROR: (gcloud.compute.http-health-checks.create) Could not fetch resource:
 - The resource 'projects/velvety-column-233813/global/httpHealthChecks/kubernetes' already exists

Creating firewall...failed.
ERROR: (gcloud.compute.firewall-rules.create) Could not fetch resource:
 - The resource 'projects/velvety-column-233813/global/firewalls/kubernetes-the-hard-way-allow-health-check' already exists

^C

Command killed by keyboard interrupt


root@controller-0:~# gcloud compute forwarding-rules list
Listed 0 items.
root@controller-0:~# gcloud compute target-pools list
NAME                    REGION      SESSION_AFFINITY  BACKUP  HEALTH_CHECKS
kubernetes-target-pool  asia-east1  NONE                      kubernetes
root@controller-0:~# gcloud compute target-pools delete kubernetes-target-pool
The following target pools will be deleted:
 - [kubernetes-target-pool] in [asia-east1]

Do you want to continue (Y/n)?  Y

Deleted [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/regions/asia-east1/targetPools/kubernetes-target-pool].
root@controller-0:~# gcloud compute firewall-rules list
NAME                                        NETWORK                  DIRECTION  PRIORITY  ALLOW                         DENY
default-allow-icmp                          default                  INGRESS    65534     icmp
default-allow-internal                      default                  INGRESS    65534     tcp:0-65535,udp:0-65535,icmp
default-allow-rdp                           default                  INGRESS    65534     tcp:3389
default-allow-ssh                           default                  INGRESS    65534     tcp:22
kubernetes-the-hard-way-allow-external      kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp
kubernetes-the-hard-way-allow-health-check  kubernetes-the-hard-way  INGRESS    1000      tcp
kubernetes-the-hard-way-allow-internal      kubernetes-the-hard-way  INGRESS    1000      tcp,udp,icmp

To show all fields of the firewall, please show in JSON format: --format=json
To show all fields in table format, please see the examples in --help.

root@controller-0:~# gcloud compute firewall-rules delete kubernetes-the-hard-way-allow-health-check
The following firewalls will be deleted:
 - [kubernetes-the-hard-way-allow-health-check]

Do you want to continue (Y/n)?  Y

Deleted [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/firewalls/kubernetes-the-hard-way-allow-health-check].
root@controller-0:~#
root@controller-0:~# gcloud compute http-health-checks list
NAME        HOST                                  PORT  REQUEST_PATH
kubernetes  kubernetes.default.svc.cluster.local  80    /healthz
root@controller-0:~# gcloud compute http-health-checks delete kubernetes
The following http health checks will be deleted:
 - [kubernetes]

Do you want to continue (Y/n)?  Y

Deleted [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/httpHealthChecks/kubernetes].
root@controller-0:~#
root@controller-0:~# echo $KUBERNETES_PUBLIC_ADDRESS
34.80.89.118
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# {
>   KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
>     --region $(gcloud config get-value compute/region) \
>     --format 'value(address)')
>
>   gcloud compute http-health-checks create kubernetes \
>     --description "Kubernetes Health Check" \
>     --host "kubernetes.default.svc.cluster.local" \
>     --request-path "/healthz"
>
>   gcloud compute firewall-rules create kubernetes-the-hard-way-allow-health-check \
>     --network kubernetes-the-hard-way \
>     --source-ranges 209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 \
>     --allow tcp
>
>   gcloud compute target-pools create kubernetes-target-pool \
>     --http-health-check kubernetes
>
>   gcloud compute target-pools add-instances kubernetes-target-pool \
>    --instances controller-0,controller-1,controller-2
>
>   gcloud compute forwarding-rules create kubernetes-forwarding-rule \
>     --address ${KUBERNETES_PUBLIC_ADDRESS} \
>     --ports 6443 \
>     --region $(gcloud config get-value compute/region) \
>     --target-pool kubernetes-target-pool
> }

Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/httpHealthChecks/kubernetes].
NAME        HOST                                  PORT  REQUEST_PATH
kubernetes  kubernetes.default.svc.cluster.local  80    /healthz
Creating firewall...⠏Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/firewalls/kubernetes-the-hard-way-allow-health-check].
Creating firewall...done.
NAME                                        NETWORK                  DIRECTION  PRIORITY  ALLOW  DENY
kubernetes-the-hard-way-allow-health-check  kubernetes-the-hard-way  INGRESS    1000      tcp
Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/regions/asia-east1/targetPools/kubernetes-target-pool].
NAME                    REGION      SESSION_AFFINITY  BACKUP  HEALTH_CHECKS
kubernetes-target-pool  asia-east1  NONE                      kubernetes
Updated [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/regions/asia-east1/targetPools/kubernetes-target-pool].
Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/regions/asia-east1/forwardingRules/kubernetes-forwarding-rule].
root@controller-0:~#
root@controller-0:~# KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
>   --region $(gcloud config get-value compute/region) \
>   --format 'value(address)')
root@controller-0:~#
root@controller-0:~# curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version
curl: (77) Problem with the SSL CA cert (path? access rights?)
root@controller-0:~# ls
admin.kubeconfig  etcd-v3.3.9-linux-amd64  etcd-v3.3.9-linux-amd64.tar.gz  ystemctl restart etcd
root@controller-0:~#
root@controller-0:~#
root@controller-0:~# ll /var/lib/kubernetes/ca.pem
-rw-r--r-- 1 root root 1367 Mar  8 01:17 /var/lib/kubernetes/ca.pem
root@controller-0:~# curl --cacert /var/lib/kubernetes/ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version
{
  "major": "1",
  "minor": "12",
  "gitVersion": "v1.12.0",
  "gitCommit": "0ed33881dc4355495f623c6f22e7dd0b7632b7c0",
  "gitTreeState": "clean",
  "buildDate": "2018-09-27T16:55:41Z",
  "goVersion": "go1.10.4",
  "compiler": "gc",
  "platform": "linux/amd64"
}root@controller-0:~#

root@master:~# gcloud compute ssh worker-0
Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Fri Mar  8 04:52:10 UTC 2019

  System load:  0.0                Processes:           86
  Usage of /:   0.6% of 193.66GB   Users logged in:     0
  Memory usage: 5%                 IP address for ens4: 10.240.0.20
  Swap usage:   0%

 * Ubuntu's Kubernetes 1.14 distributions can bypass Docker and use containerd
   directly, see https://bit.ly/ubuntu-containerd or try it now with

     snap install microk8s --channel=1.14/beta --classic

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.


root@worker-0:~#
root@worker-0:~#
root@worker-0:~# {
>   sudo apt-get update
>   sudo apt-get -y install socat conntrack ipset
> }
Hit:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic InRelease
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [8570 kB]
Get:5 http://archive.canonical.com/ubuntu bionic InRelease [10.2 kB]
Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Get:7 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/universe Translation-en [4941 kB]
Get:8 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [151 kB]
Get:9 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/multiverse Translation-en [108 kB]
Get:10 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [545 kB]
Get:11 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main Translation-en [203 kB]
Get:12 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [6996 B]
Get:13 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [739 kB]
Get:14 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe Translation-en [191 kB]
Get:15 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [6388 B]
Get:16 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse Translation-en [3452 B]
Get:17 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3472 B]
Get:18 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe Translation-en [1604 B]
Get:19 http://archive.canonical.com/ubuntu bionic/partner amd64 Packages [2304 B]
Get:20 http://archive.canonical.com/ubuntu bionic/partner Translation-en [1272 B]
Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [126 kB]
Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe Translation-en [71.4 kB]
Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [3744 B]
Get:24 http://security.ubuntu.com/ubuntu bionic-security/multiverse Translation-en [1952 B]
Fetched 15.9 MB in 3s (4651 kB/s)
Reading package lists... Done
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following package was automatically installed and is no longer required:
  grub-pc-bin
Use 'sudo apt autoremove' to remove it.
The following additional packages will be installed:
  libipset3
The following NEW packages will be installed:
  conntrack ipset libipset3 socat
0 upgraded, 4 newly installed, 0 to remove and 4 not upgraded.
Need to get 450 kB of archives.
After this operation, 1560 kB of additional disk space will be used.
Get:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 conntrack amd64 1:1.4.4+snapshot20161117-6ubuntu2 [30.6 kB]
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 libipset3 amd64 6.34-1 [43.9 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 ipset amd64 6.34-1 [33.7 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 socat amd64 1.7.3.2-2ubuntu2 [342 kB]
Fetched 450 kB in 3s (179 kB/s)
Selecting previously unselected package conntrack.
(Reading database ... 60751 files and directories currently installed.)
Preparing to unpack .../conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb ...
Unpacking conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Selecting previously unselected package libipset3:amd64.
Preparing to unpack .../libipset3_6.34-1_amd64.deb ...
Unpacking libipset3:amd64 (6.34-1) ...
Selecting previously unselected package ipset.
Preparing to unpack .../ipset_6.34-1_amd64.deb ...
Unpacking ipset (6.34-1) ...
Selecting previously unselected package socat.
Preparing to unpack .../socat_1.7.3.2-2ubuntu2_amd64.deb ...
Unpacking socat (1.7.3.2-2ubuntu2) ...
Setting up conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Setting up socat (1.7.3.2-2ubuntu2) ...
Setting up libipset3:amd64 (6.34-1) ...
Processing triggers for libc-bin (2.27-3ubuntu1) ...
Setting up ipset (6.34-1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...

Get:10 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [545 kB]
Get:11 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main Translation-en [203 kB]
Get:12 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [6996 B]
Get:13 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [739 kB]
Get:14 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe Translation-en [191 kB]
Get:15 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [6388 B]
Get:16 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse Translation-en [3452 B]
Get:17 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3472 B]
Get:18 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe Translation-en [1604 B]
Get:19 http://archive.canonical.com/ubuntu bionic/partner amd64 Packages [2304 B]
Get:20 http://archive.canonical.com/ubuntu bionic/partner Translation-en [1272 B]
Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [126 kB]
Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe Translation-en [71.4 kB]
Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [3744 B]
Get:24 http://security.ubuntu.com/ubuntu bionic-security/multiverse Translation-en [1952 B]
Fetched 15.9 MB in 3s (4651 kB/s)
Reading package lists... Done
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following package was automatically installed and is no longer required:
  grub-pc-bin
Use 'sudo apt autoremove' to remove it.
The following additional packages will be installed:
  libipset3
The following NEW packages will be installed:
  conntrack ipset libipset3 socat
0 upgraded, 4 newly installed, 0 to remove and 4 not upgraded.
Need to get 450 kB of archives.
After this operation, 1560 kB of additional disk space will be used.
Get:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 conntrack amd64 1:1.4.4+snapshot20161117-6ubuntu2 [30.6 kB]
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 libipset3 amd64 6.34-1 [43.9 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 ipset amd64 6.34-1 [33.7 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 socat amd64 1.7.3.2-2ubuntu2 [342 kB]
Fetched 450 kB in 3s (179 kB/s)
Selecting previously unselected package conntrack.
(Reading database ... 60751 files and directories currently installed.)
Preparing to unpack .../conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb ...
Unpacking conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Selecting previously unselected package libipset3:amd64.
Preparing to unpack .../libipset3_6.34-1_amd64.deb ...
Unpacking libipset3:amd64 (6.34-1) ...
Selecting previously unselected package ipset.
Preparing to unpack .../ipset_6.34-1_amd64.deb ...
Unpacking ipset (6.34-1) ...
Selecting previously unselected package socat.
Preparing to unpack .../socat_1.7.3.2-2ubuntu2_amd64.deb ...
Unpacking socat (1.7.3.2-2ubuntu2) ...
Setting up conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Setting up socat (1.7.3.2-2ubuntu2) ...
Setting up libipset3:amd64 (6.34-1) ...
Processing triggers for libc-bin (2.27-3ubuntu1) ...
Setting up ipset (6.34-1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
root@worker-0:~# clear
root@worker-0:~#
root@worker-0:~# wget -q --show-progress --https-only --timestamping \
>   https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.12.0/crictl-v1.12.0-linux-amd64.tar.gz \
>   https://storage.googleapis.com/kubernetes-the-hard-way/runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 \
>   https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \
>   https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \
>   https://github.com/containerd/containerd/releases/download/v1.2.0-rc.0/containerd-1.2.0-rc.0.linux-amd64.tar.gz \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-proxy \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubelet
crictl-v1.12.0-linux-amd64.tar.gz                           100%[========================================================================================================================================>]   7.07M  4.75MB/s    in 1.5s
runsc-50c283b9f56bb7200938d9e207355f05f79f0d17              100%[========================================================================================================================================>]  16.66M  25.7MB/s    in 0.6s
runc.amd64                                                  100%[========================================================================================================================================>]  11.70M  6.33MB/s    in 1.8s
cni-plugins-amd64-v0.6.0.tgz                                100%[========================================================================================================================================>]  14.67M  7.17MB/s    in 2.0s
containerd-1.2.0-rc.0.linux-amd64.tar.gz                    100%[========================================================================================================================================>]  27.55M  9.85MB/s    in 2.8s
kubectl                                                     100%[========================================================================================================================================>]  54.69M  85.3MB/s    in 0.6s
kube-proxy                                                  100%[========================================================================================================================================>]  48.10M  44.5MB/s    in 1.1s
kubelet                                                     100%[========================================================================================================================================>] 168.54M  60.6MB/s    in 2.8s
root@worker-0:~#
root@worker-0:~#
root@worker-0:~# sudo mkdir -p \
>   /etc/cni/net.d \
>   /opt/cni/bin \
>   /var/lib/kubelet \
>   /var/lib/kube-proxy \
>   /var/lib/kubernetes \
>   /var/run/kubernetes
root@worker-0:~#
root@worker-0:~# {
>   sudo mv runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 runsc
>   sudo mv runc.amd64 runc
>   chmod +x kubectl kube-proxy kubelet runc runsc
>   sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/
>   sudo tar -xvf crictl-v1.12.0-linux-amd64.tar.gz -C /usr/local/bin/
>   sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/
>   sudo tar -xvf containerd-1.2.0-rc.0.linux-amd64.tar.gz -C /
> }
crictl
./
./flannel
./ptp
./host-local
./portmap
./tuning
./vlan
./sample
./dhcp
./ipvlan
./macvlan
./loopback
./bridge
bin/
bin/containerd
bin/ctr
bin/containerd-shim-runc-v1
bin/containerd-shim
bin/containerd-release
bin/containerd-stress
root@worker-0:~#
root@worker-0:~#
root@worker-0:~# POD_CIDR=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr)
root@worker-0:~#
root@worker-0:~#
root@worker-0:~# cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
> {
>     "cniVersion": "0.3.1",
>     "name": "bridge",
>     "type": "bridge",
>     "bridge": "cnio0",
>     "isGateway": true,
>     "ipMasq": true,
>     "ipam": {
>         "type": "host-local",
>         "ranges": [
>           [{"subnet": "${POD_CIDR}"}]
>         ],
>         "routes": [{"dst": "0.0.0.0/0"}]
>     }
> }
> EOF
{
    "cniVersion": "0.3.1",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "10.200.0.0/24"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
root@worker-0:~#
root@worker-0:~# cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
> {
>     "cniVersion": "0.3.1",
>     "type": "loopback"
> }
> EOF
{
    "cniVersion": "0.3.1",
    "type": "loopback"
}
root@worker-0:~#
root@worker-0:~# sudo mkdir -p /etc/containerd/
root@worker-0:~# cat << EOF | sudo tee /etc/containerd/config.toml
> [plugins]
>   [plugins.cri.containerd]
>     snapshotter = "overlayfs"
>     [plugins.cri.containerd.default_runtime]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runc"
>       runtime_root = ""
>     [plugins.cri.containerd.untrusted_workload_runtime]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runsc"
>       runtime_root = "/run/containerd/runsc"
>     [plugins.cri.containerd.gvisor]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runsc"
>       runtime_root = "/run/containerd/runsc"
> EOF
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
    [plugins.cri.containerd.untrusted_workload_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runsc"
      runtime_root = "/run/containerd/runsc"
    [plugins.cri.containerd.gvisor]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runsc"
      runtime_root = "/run/containerd/runsc"
root@worker-0:~#
root@worker-0:~# cat <<EOF | sudo tee /etc/systemd/system/containerd.service
> [Unit]
> Description=containerd container runtime
> Documentation=https://containerd.io
> After=network.target
>
> [Service]
> ExecStartPre=/sbin/modprobe overlay
> ExecStart=/bin/containerd
> Restart=always
> RestartSec=5
> Delegate=yes
> KillMode=process
> OOMScoreAdjust=-999
> LimitNOFILE=1048576
> LimitNPROC=infinity
> LimitCORE=infinity
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
root@worker-0:~#
root@worker-0:~# {
>   sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
>   sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
>   sudo mv ca.pem /var/lib/kubernetes/
> }
root@worker-0:~#
root@worker-0:~# cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
> kind: KubeletConfiguration
> apiVersion: kubelet.config.k8s.io/v1beta1
> authentication:
>   anonymous:
>     enabled: false
>   webhook:
>     enabled: true
>   x509:
>     clientCAFile: "/var/lib/kubernetes/ca.pem"
> authorization:
>   mode: Webhook
> clusterDomain: "cluster.local"
> clusterDNS:
>   - "10.32.0.10"
> podCIDR: "${POD_CIDR}"
> resolvConf: "/run/systemd/resolve/resolv.conf"
> runtimeRequestTimeout: "15m"
> tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
> tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
> EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "10.200.0.0/24"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/worker-0.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/worker-0-key.pem"
root@worker-0:~#
root@worker-0:~# cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
> [Unit]
> Description=Kubernetes Kubelet
> Documentation=https://github.com/kubernetes/kubernetes
> After=containerd.service
> Requires=containerd.service
>
> [Service]
> ExecStart=/usr/local/bin/kubelet \\
>   --config=/var/lib/kubelet/kubelet-config.yaml \\
>   --container-runtime=remote \\
>   --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
>   --image-pull-progress-deadline=2m \\
>   --kubeconfig=/var/lib/kubelet/kubeconfig \\
>   --network-plugin=cni \\
>   --register-node=true \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \
  --config=/var/lib/kubelet/kubelet-config.yaml \
  --container-runtime=remote \
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \
  --image-pull-progress-deadline=2m \
  --kubeconfig=/var/lib/kubelet/kubeconfig \
  --network-plugin=cni \
  --register-node=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@worker-0:~#
root@worker-0:~# sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
root@worker-0:~# cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
> kind: KubeProxyConfiguration
> apiVersion: kubeproxy.config.k8s.io/v1alpha1
> clientConnection:
>   kubeconfig: "/var/lib/kube-proxy/kubeconfig"
> mode: "iptables"
> clusterCIDR: "10.200.0.0/16"
> EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
root@worker-0:~#
root@worker-0:~# cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
> [Unit]
> Description=Kubernetes Kube Proxy
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-proxy \\
>   --config=/var/lib/kube-proxy/kube-proxy-config.yaml
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@worker-0:~#
root@worker-0:~# {
>   sudo systemctl daemon-reload
>   sudo systemctl enable containerd kubelet kube-proxy
>   sudo systemctl start containerd kubelet kube-proxy
> }
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /etc/systemd/system/kube-proxy.service.
root@worker-0:~# exit
logout
Connection to 34.80.0.204 closed.
root@master:~# gcloud compute ssh worker-1
Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Fri Mar  8 04:56:03 UTC 2019

  System load:  0.0                Processes:           86
  Usage of /:   0.6% of 193.66GB   Users logged in:     0
  Memory usage: 5%                 IP address for ens4: 10.240.0.21
  Swap usage:   0%

 * Ubuntu's Kubernetes 1.14 distributions can bypass Docker and use containerd
   directly, see https://bit.ly/ubuntu-containerd or try it now with

     snap install microk8s --channel=1.14/beta --classic

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.


root@worker-1:~#
root@worker-1:~#
root@worker-1:~# {
>   sudo apt-get update
>   sudo apt-get -y install socat conntrack ipset
> }
Hit:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic InRelease
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [8570 kB]
Get:5 http://archive.canonical.com/ubuntu bionic InRelease [10.2 kB]
Get:6 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/universe Translation-en [4941 kB]
Get:7 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [151 kB]
Get:8 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/multiverse Translation-en [108 kB]
Get:9 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [545 kB]
Get:10 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main Translation-en [203 kB]
Get:11 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [6996 B]
Get:12 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [739 kB]
Get:13 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe Translation-en [191 kB]
Get:14 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [6388 B]
Get:15 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse Translation-en [3452 B]
Get:16 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3472 B]
Get:17 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe Translation-en [1604 B]
Get:18 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Get:19 http://archive.canonical.com/ubuntu bionic/partner amd64 Packages [2304 B]
Get:20 http://archive.canonical.com/ubuntu bionic/partner Translation-en [1272 B]
Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [126 kB]
Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe Translation-en [71.4 kB]
Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [3744 B]
Get:24 http://security.ubuntu.com/ubuntu bionic-security/multiverse Translation-en [1952 B]
Fetched 15.9 MB in 4s (4370 kB/s)
Reading package lists... Done
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following package was automatically installed and is no longer required:
  grub-pc-bin
Use 'sudo apt autoremove' to remove it.
The following additional packages will be installed:
  libipset3
The following NEW packages will be installed:
  conntrack ipset libipset3 socat
0 upgraded, 4 newly installed, 0 to remove and 4 not upgraded.
Need to get 450 kB of archives.
After this operation, 1560 kB of additional disk space will be used.
Get:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 conntrack amd64 1:1.4.4+snapshot20161117-6ubuntu2 [30.6 kB]
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 libipset3 amd64 6.34-1 [43.9 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 ipset amd64 6.34-1 [33.7 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 socat amd64 1.7.3.2-2ubuntu2 [342 kB]
Fetched 450 kB in 0s (16.5 MB/s)
Selecting previously unselected package conntrack.
(Reading database ... 60751 files and directories currently installed.)
Preparing to unpack .../conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb ...
Unpacking conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Selecting previously unselected package libipset3:amd64.
Preparing to unpack .../libipset3_6.34-1_amd64.deb ...
Unpacking libipset3:amd64 (6.34-1) ...
Selecting previously unselected package ipset.
Preparing to unpack .../ipset_6.34-1_amd64.deb ...
Unpacking ipset (6.34-1) ...
Selecting previously unselected package socat.
Preparing to unpack .../socat_1.7.3.2-2ubuntu2_amd64.deb ...
Unpacking socat (1.7.3.2-2ubuntu2) ...
Setting up conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Setting up socat (1.7.3.2-2ubuntu2) ...
Setting up libipset3:amd64 (6.34-1) ...
Processing triggers for libc-bin (2.27-3ubuntu1) ...
Setting up ipset (6.34-1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
root@worker-1:~# wget -q --show-progress --https-only --timestamping \
>   https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.12.0/crictl-v1.12.0-linux-amd64.tar.gz \
>   https://storage.googleapis.com/kubernetes-the-hard-way/runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 \
>   https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \
>   https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \
>   https://github.com/containerd/containerd/releases/download/v1.2.0-rc.0/containerd-1.2.0-rc.0.linux-amd64.tar.gz \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-proxy \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubelet
crictl-v1.12.0-linux-amd64.tar.gz                           100%[========================================================================================================================================>]   7.07M  3.82MB/s    in 1.9s
runsc-50c283b9f56bb7200938d9e207355f05f79f0d17              100%[========================================================================================================================================>]  16.66M  54.3MB/s    in 0.3s
runc.amd64                                                  100%[========================================================================================================================================>]  11.70M  5.76MB/s    in 2.0s
cni-plugins-amd64-v0.6.0.tgz                                100%[========================================================================================================================================>]  14.67M  7.26MB/s    in 2.0s
containerd-1.2.0-rc.0.linux-amd64.tar.gz                    100%[========================================================================================================================================>]  27.55M  9.97MB/s    in 2.8s
kubectl                                                     100%[========================================================================================================================================>]  54.69M   120MB/s    in 0.5s
kube-proxy                                                  100%[========================================================================================================================================>]  48.10M  96.4MB/s    in 0.5s
kubelet                                                     100%[========================================================================================================================================>] 168.54M   125MB/s    in 1.4s
root@worker-1:~# sudo mkdir -p \
>   /etc/cni/net.d \
>   /opt/cni/bin \
>   /var/lib/kubelet \
>   /var/lib/kube-proxy \
>   /var/lib/kubernetes \
>   /var/run/kubernetes
root@worker-1:~#
root@worker-1:~# {
>   sudo mv runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 runsc
>   sudo mv runc.amd64 runc
>   chmod +x kubectl kube-proxy kubelet runc runsc
>   sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/
>   sudo tar -xvf crictl-v1.12.0-linux-amd64.tar.gz -C /usr/local/bin/
>   sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/
>   sudo tar -xvf containerd-1.2.0-rc.0.linux-amd64.tar.gz -C /
> }
crictl
./
./flannel
./ptp
./host-local
./portmap
./tuning
./vlan
./sample
./dhcp
./ipvlan
./macvlan
./loopback
./bridge
bin/
bin/containerd
bin/ctr
bin/containerd-shim-runc-v1
bin/containerd-shim
bin/containerd-release
bin/containerd-stress
root@worker-1:~#
root@worker-1:~#
root@worker-1:~# POD_CIDR=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr)
root@worker-1:~#
root@worker-1:~# cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
> {
>     "cniVersion": "0.3.1",
>     "name": "bridge",
>     "type": "bridge",
>     "bridge": "cnio0",
>     "isGateway": true,
>     "ipMasq": true,
>     "ipam": {
>         "type": "host-local",
>         "ranges": [
>           [{"subnet": "${POD_CIDR}"}]
>         ],
>         "routes": [{"dst": "0.0.0.0/0"}]
>     }
> }
> EOF
{
    "cniVersion": "0.3.1",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "10.200.1.0/24"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
root@worker-1:~#
root@worker-1:~# cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
> {
>     "cniVersion": "0.3.1",
>     "type": "loopback"
> }
> EOF
{
    "cniVersion": "0.3.1",
    "type": "loopback"
}
root@worker-1:~#
root@worker-1:~# sudo mkdir -p /etc/containerd/
root@worker-1:~# cat << EOF | sudo tee /etc/containerd/config.toml
> [plugins]
>   [plugins.cri.containerd]
>     snapshotter = "overlayfs"
>     [plugins.cri.containerd.default_runtime]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runc"
>       runtime_root = ""
>     [plugins.cri.containerd.untrusted_workload_runtime]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runsc"
>       runtime_root = "/run/containerd/runsc"
>     [plugins.cri.containerd.gvisor]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runsc"
>       runtime_root = "/run/containerd/runsc"
> EOF
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
    [plugins.cri.containerd.untrusted_workload_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runsc"
      runtime_root = "/run/containerd/runsc"
    [plugins.cri.containerd.gvisor]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runsc"
      runtime_root = "/run/containerd/runsc"
root@worker-1:~#
root@worker-1:~# cat <<EOF | sudo tee /etc/systemd/system/containerd.service
> [Unit]
> Description=containerd container runtime
> Documentation=https://containerd.io
> After=network.target
>
> [Service]
> ExecStartPre=/sbin/modprobe overlay
> ExecStart=/bin/containerd
> Restart=always
> RestartSec=5
> Delegate=yes
> KillMode=process
> OOMScoreAdjust=-999
> LimitNOFILE=1048576
> LimitNPROC=infinity
> LimitCORE=infinity
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
root@worker-1:~#
root@worker-1:~# {
>   sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
>   sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
>   sudo mv ca.pem /var/lib/kubernetes/
> }
root@worker-1:~#
root@worker-1:~# cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
> kind: KubeletConfiguration
> apiVersion: kubelet.config.k8s.io/v1beta1
> authentication:
>   anonymous:
>     enabled: false
>   webhook:
>     enabled: true
>   x509:
>     clientCAFile: "/var/lib/kubernetes/ca.pem"
> authorization:
>   mode: Webhook
> clusterDomain: "cluster.local"
> clusterDNS:
>   - "10.32.0.10"
> podCIDR: "${POD_CIDR}"
> resolvConf: "/run/systemd/resolve/resolv.conf"
> runtimeRequestTimeout: "15m"
> tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
> tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
> EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "10.200.1.0/24"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/worker-1.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/worker-1-key.pem"
root@worker-1:~#
root@worker-1:~# cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
> [Unit]
> Description=Kubernetes Kubelet
> Documentation=https://github.com/kubernetes/kubernetes
> After=containerd.service
> Requires=containerd.service
>
> [Service]
> ExecStart=/usr/local/bin/kubelet \\
>   --config=/var/lib/kubelet/kubelet-config.yaml \\
>   --container-runtime=remote \\
>   --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
>   --image-pull-progress-deadline=2m \\
>   --kubeconfig=/var/lib/kubelet/kubeconfig \\
>   --network-plugin=cni \\
>   --register-node=true \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \
  --config=/var/lib/kubelet/kubelet-config.yaml \
  --container-runtime=remote \
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \
  --image-pull-progress-deadline=2m \
  --kubeconfig=/var/lib/kubelet/kubeconfig \
  --network-plugin=cni \
  --register-node=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@worker-1:~#
root@worker-1:~# sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
root@worker-1:~#
root@worker-1:~# cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
> kind: KubeProxyConfiguration
> apiVersion: kubeproxy.config.k8s.io/v1alpha1
> clientConnection:
>   kubeconfig: "/var/lib/kube-proxy/kubeconfig"
> mode: "iptables"
> clusterCIDR: "10.200.0.0/16"
> EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
root@worker-1:~#
root@worker-1:~# cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
> [Unit]
> Description=Kubernetes Kube Proxy
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-proxy \\
>   --config=/var/lib/kube-proxy/kube-proxy-config.yaml
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@worker-1:~#
root@worker-1:~# {
>   sudo systemctl daemon-reload
>   sudo systemctl enable containerd kubelet kube-proxy
>   sudo systemctl start containerd kubelet kube-proxy
> }
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /etc/systemd/system/kube-proxy.service.
root@worker-1:~#
root@worker-1:~# exit
logout
Connection to 34.80.34.179 closed.
root@master:~# gcloud compute ssh worker-2
Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-1028-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Fri Mar  8 04:59:06 UTC 2019

  System load:  0.0                Processes:           87
  Usage of /:   0.6% of 193.66GB   Users logged in:     0
  Memory usage: 5%                 IP address for ens4: 10.240.0.22
  Swap usage:   0%

 * Ubuntu's Kubernetes 1.14 distributions can bypass Docker and use containerd
   directly, see https://bit.ly/ubuntu-containerd or try it now with

     snap install microk8s --channel=1.14/beta --classic

  Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.


root@worker-2:~# {
>   sudo apt-get update
>   sudo apt-get -y install socat conntrack ipset
> }
Hit:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic InRelease
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [8570 kB]
Get:5 http://archive.canonical.com/ubuntu bionic InRelease [10.2 kB]
Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Get:7 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/universe Translation-en [4941 kB]
Get:8 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [151 kB]
Get:9 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/multiverse Translation-en [108 kB]
Get:10 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [545 kB]
Get:11 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/main Translation-en [203 kB]
Get:12 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [6996 B]
Get:13 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [739 kB]
Get:14 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/universe Translation-en [191 kB]
Get:15 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [6388 B]
Get:16 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-updates/multiverse Translation-en [3452 B]
Get:17 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3472 B]
Get:18 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic-backports/universe Translation-en [1604 B]
Get:19 http://archive.canonical.com/ubuntu bionic/partner amd64 Packages [2304 B]
Get:20 http://archive.canonical.com/ubuntu bionic/partner Translation-en [1272 B]
Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [126 kB]
Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe Translation-en [71.4 kB]
Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [3744 B]
Get:24 http://security.ubuntu.com/ubuntu bionic-security/multiverse Translation-en [1952 B]
Fetched 15.9 MB in 4s (4353 kB/s)
Reading package lists... Done
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following package was automatically installed and is no longer required:
  grub-pc-bin
Use 'sudo apt autoremove' to remove it.
The following additional packages will be installed:
  libipset3
The following NEW packages will be installed:
  conntrack ipset libipset3 socat
0 upgraded, 4 newly installed, 0 to remove and 4 not upgraded.
Need to get 450 kB of archives.
After this operation, 1560 kB of additional disk space will be used.
Get:1 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 conntrack amd64 1:1.4.4+snapshot20161117-6ubuntu2 [30.6 kB]
Get:2 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 libipset3 amd64 6.34-1 [43.9 kB]
Get:3 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 ipset amd64 6.34-1 [33.7 kB]
Get:4 http://asia-east1.gce.archive.ubuntu.com/ubuntu bionic/main amd64 socat amd64 1.7.3.2-2ubuntu2 [342 kB]
Fetched 450 kB in 2s (286 kB/s)
Selecting previously unselected package conntrack.
(Reading database ... 60751 files and directories currently installed.)
Preparing to unpack .../conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb ...
Unpacking conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Selecting previously unselected package libipset3:amd64.
Preparing to unpack .../libipset3_6.34-1_amd64.deb ...
Unpacking libipset3:amd64 (6.34-1) ...
Selecting previously unselected package ipset.
Preparing to unpack .../ipset_6.34-1_amd64.deb ...
Unpacking ipset (6.34-1) ...
Selecting previously unselected package socat.
Preparing to unpack .../socat_1.7.3.2-2ubuntu2_amd64.deb ...
Unpacking socat (1.7.3.2-2ubuntu2) ...
Setting up conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...
Setting up socat (1.7.3.2-2ubuntu2) ...
Setting up libipset3:amd64 (6.34-1) ...
Processing triggers for libc-bin (2.27-3ubuntu1) ...
Setting up ipset (6.34-1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
root@worker-2:~#
root@worker-2:~# wget -q --show-progress --https-only --timestamping \
>   https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.12.0/crictl-v1.12.0-linux-amd64.tar.gz \
>   https://storage.googleapis.com/kubernetes-the-hard-way/runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 \
>   https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \
>   https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \
>   https://github.com/containerd/containerd/releases/download/v1.2.0-rc.0/containerd-1.2.0-rc.0.linux-amd64.tar.gz \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-proxy \
>   https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubelet
crictl-v1.12.0-linux-amd64.tar.gz                           100%[========================================================================================================================================>]   7.07M  4.73MB/s    in 1.5s
runsc-50c283b9f56bb7200938d9e207355f05f79f0d17              100%[========================================================================================================================================>]  16.66M  56.2MB/s    in 0.3s
runc.amd64                                                  100%[========================================================================================================================================>]  11.70M  6.38MB/s    in 1.8s
cni-plugins-amd64-v0.6.0.tgz                                100%[========================================================================================================================================>]  14.67M  7.28MB/s    in 2.0s
containerd-1.2.0-rc.0.linux-amd64.tar.gz                    100%[========================================================================================================================================>]  27.55M  9.93MB/s    in 2.8s
kubectl                                                     100%[========================================================================================================================================>]  54.69M   129MB/s    in 0.4s
kube-proxy                                                  100%[========================================================================================================================================>]  48.10M  70.8MB/s    in 0.7s
kubelet                                                     100%[========================================================================================================================================>] 168.54M   116MB/s    in 1.5s
root@worker-2:~#
root@worker-2:~#
root@worker-2:~# sudo mkdir -p \
>   /etc/cni/net.d \
>   /opt/cni/bin \
>   /var/lib/kubelet \
>   /var/lib/kube-proxy \
>   /var/lib/kubernetes \
>   /var/run/kubernetes
root@worker-2:~#
root@worker-2:~# {
>   sudo mv runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 runsc
>   sudo mv runc.amd64 runc
>   chmod +x kubectl kube-proxy kubelet runc runsc
>   sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/
>   sudo tar -xvf crictl-v1.12.0-linux-amd64.tar.gz -C /usr/local/bin/
>   sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/
>   sudo tar -xvf containerd-1.2.0-rc.0.linux-amd64.tar.gz -C /
> }
crictl
./
./flannel
./ptp
./host-local
./portmap
./tuning
./vlan
./sample
./dhcp
./ipvlan
./macvlan
./loopback
./bridge
bin/
bin/containerd
bin/ctr
bin/containerd-shim-runc-v1
bin/containerd-shim
bin/containerd-release
bin/containerd-stress
root@worker-2:~# POD_CIDR=$(curl -s -H "Metadata-Flavor: Google" \
>   http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr)
root@worker-2:~#
root@worker-2:~# cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
> {
>     "cniVersion": "0.3.1",
>     "name": "bridge",
>     "type": "bridge",
>     "bridge": "cnio0",
>     "isGateway": true,
>     "ipMasq": true,
>     "ipam": {
>         "type": "host-local",
>         "ranges": [
>           [{"subnet": "${POD_CIDR}"}]
>         ],
>         "routes": [{"dst": "0.0.0.0/0"}]
>     }
> }
> EOF
{
    "cniVersion": "0.3.1",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "10.200.2.0/24"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
root@worker-2:~#
root@worker-2:~# cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
> {
>     "cniVersion": "0.3.1",
>     "type": "loopback"
> }
> EOF
{
    "cniVersion": "0.3.1",
    "type": "loopback"
}
root@worker-2:~#
root@worker-2:~# sudo mkdir -p /etc/containerd/
root@worker-2:~# cat << EOF | sudo tee /etc/containerd/config.toml
> [plugins]
>   [plugins.cri.containerd]
>     snapshotter = "overlayfs"
>     [plugins.cri.containerd.default_runtime]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runc"
>       runtime_root = ""
>     [plugins.cri.containerd.untrusted_workload_runtime]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runsc"
>       runtime_root = "/run/containerd/runsc"
>     [plugins.cri.containerd.gvisor]
>       runtime_type = "io.containerd.runtime.v1.linux"
>       runtime_engine = "/usr/local/bin/runsc"
>       runtime_root = "/run/containerd/runsc"
> EOF
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
    [plugins.cri.containerd.untrusted_workload_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runsc"
      runtime_root = "/run/containerd/runsc"
    [plugins.cri.containerd.gvisor]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runsc"
      runtime_root = "/run/containerd/runsc"
root@worker-2:~#
root@worker-2:~# cat <<EOF | sudo tee /etc/systemd/system/containerd.service
> [Unit]
> Description=containerd container runtime
> Documentation=https://containerd.io
> After=network.target
>
> [Service]
> ExecStartPre=/sbin/modprobe overlay
> ExecStart=/bin/containerd
> Restart=always
> RestartSec=5
> Delegate=yes
> KillMode=process
> OOMScoreAdjust=-999
> LimitNOFILE=1048576
> LimitNPROC=infinity
> LimitCORE=infinity
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
root@worker-2:~#
root@worker-2:~# {
>   sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
>   sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
>   sudo mv ca.pem /var/lib/kubernetes/
> }
root@worker-2:~#
root@worker-2:~# cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
> kind: KubeletConfiguration
> apiVersion: kubelet.config.k8s.io/v1beta1
> authentication:
>   anonymous:
>     enabled: false
>   webhook:
>     enabled: true
>   x509:
>     clientCAFile: "/var/lib/kubernetes/ca.pem"
> authorization:
>   mode: Webhook
> clusterDomain: "cluster.local"
> clusterDNS:
>   - "10.32.0.10"
> podCIDR: "${POD_CIDR}"
> resolvConf: "/run/systemd/resolve/resolv.conf"
> runtimeRequestTimeout: "15m"
> tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
> tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
> EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "10.200.2.0/24"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/worker-2.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/worker-2-key.pem"
root@worker-2:~#
root@worker-2:~# cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
> [Unit]
> Description=Kubernetes Kubelet
> Documentation=https://github.com/kubernetes/kubernetes
> After=containerd.service
> Requires=containerd.service
>
> [Service]
> ExecStart=/usr/local/bin/kubelet \\
>   --config=/var/lib/kubelet/kubelet-config.yaml \\
>   --container-runtime=remote \\
>   --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
>   --image-pull-progress-deadline=2m \\
>   --kubeconfig=/var/lib/kubelet/kubeconfig \\
>   --network-plugin=cni \\
>   --register-node=true \\
>   --v=2
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \
  --config=/var/lib/kubelet/kubelet-config.yaml \
  --container-runtime=remote \
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \
  --image-pull-progress-deadline=2m \
  --kubeconfig=/var/lib/kubelet/kubeconfig \
  --network-plugin=cni \
  --register-node=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@worker-2:~#
root@worker-2:~# sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
root@worker-2:~# cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
> kind: KubeProxyConfiguration
> apiVersion: kubeproxy.config.k8s.io/v1alpha1
> clientConnection:
>   kubeconfig: "/var/lib/kube-proxy/kubeconfig"
> mode: "iptables"
> clusterCIDR: "10.200.0.0/16"
> EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
root@worker-2:~#
root@worker-2:~# cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
> [Unit]
> Description=Kubernetes Kube Proxy
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-proxy \\
>   --config=/var/lib/kube-proxy/kube-proxy-config.yaml
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@worker-2:~#
root@worker-2:~# {
>   sudo systemctl daemon-reload
>   sudo systemctl enable containerd kubelet kube-proxy
>   sudo systemctl start containerd kubelet kube-proxy
> }
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /etc/systemd/system/kube-proxy.service.
root@worker-2:~# exit
logout
Connection to 34.80.89.105 closed.
root@master:~# gcloud compute ssh controller-0 \
>   --command "kubectl get nodes --kubeconfig admin.kubeconfig"
NAME       STATUS   ROLES    AGE     VERSION
worker-0   Ready    <none>   5m45s   v1.12.0
worker-1   Ready    <none>   2m41s   v1.12.0
worker-2   Ready    <none>   26s     v1.12.0
root@master:~# gcloud compute ssh controller-0   --command "kubectl get nodes
> "
NAME       STATUS   ROLES    AGE     VERSION
worker-0   Ready    <none>   6m1s    v1.12.0
worker-1   Ready    <none>   2m57s   v1.12.0
worker-2   Ready    <none>   42s     v1.12.0
root@master:~#


clusterCIDR: "10.200.0.0/16"
root@worker-2:~#
root@worker-2:~# cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
> [Unit]
> Description=Kubernetes Kube Proxy
> Documentation=https://github.com/kubernetes/kubernetes
>
> [Service]
> ExecStart=/usr/local/bin/kube-proxy \\
>   --config=/var/lib/kube-proxy/kube-proxy-config.yaml
> Restart=on-failure
> RestartSec=5
>
> [Install]
> WantedBy=multi-user.target
> EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
root@worker-2:~#
root@worker-2:~# {
>   sudo systemctl daemon-reload
>   sudo systemctl enable containerd kubelet kube-proxy
>   sudo systemctl start containerd kubelet kube-proxy
> }
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /etc/systemd/system/kube-proxy.service.
root@worker-2:~# exit
logout
Connection to 34.80.89.105 closed.
root@master:~# gcloud compute ssh controller-0 \
>   --command "kubectl get nodes --kubeconfig admin.kubeconfig"
NAME       STATUS   ROLES    AGE     VERSION
worker-0   Ready    <none>   5m45s   v1.12.0
worker-1   Ready    <none>   2m41s   v1.12.0
worker-2   Ready    <none>   26s     v1.12.0
root@master:~# gcloud compute ssh controller-0   --command "kubectl get nodes
> "
NAME       STATUS   ROLES    AGE     VERSION
worker-0   Ready    <none>   6m1s    v1.12.0
worker-1   Ready    <none>   2m57s   v1.12.0
worker-2   Ready    <none>   42s     v1.12.0
root@master:~# clear
root@master:~#
root@master:~# {
>   KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
>     --region $(gcloud config get-value compute/region) \
>     --format 'value(address)')
>
>   kubectl config set-cluster kubernetes-the-hard-way \
>     --certificate-authority=ca.pem \
>     --embed-certs=true \
>     --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443
>
>   kubectl config set-credentials admin \
>     --client-certificate=admin.pem \
>     --client-key=admin-key.pem
>
>   kubectl config set-context kubernetes-the-hard-way \
>     --cluster=kubernetes-the-hard-way \
>     --user=admin
>
>   kubectl config use-context kubernetes-the-hard-way
> }
Cluster "kubernetes-the-hard-way" set.
User "admin" set.
Context "kubernetes-the-hard-way" created.
Switched to context "kubernetes-the-hard-way".
root@master:~#
root@master:~#
root@master:~# kubectl get componentstatuses
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health":"true"}
etcd-2               Healthy   {"health":"true"}
etcd-1               Healthy   {"health":"true"}
root@master:~# kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
worker-0   Ready    <none>   85m   v1.12.0
worker-1   Ready    <none>   82m   v1.12.0
worker-2   Ready    <none>   80m   v1.12.0
root@master:~#
root@master:~# for instance in worker-0 worker-1 worker-2; do
>   gcloud compute instances describe ${instance} \
>     --format 'value[separator=" "](networkInterfaces[0].networkIP,metadata.items[0].value)'
> done
10.240.0.20 10.200.0.0/24
10.240.0.21 10.200.1.0/24
10.240.0.22 10.200.2.0/24
root@master:~#
root@master:~# for i in 0 1 2; do
>   gcloud compute routes create kubernetes-route-10-200-${i}-0-24 \
>     --network kubernetes-the-hard-way \
>     --next-hop-address 10.240.0.2${i} \
>     --destination-range 10.200.${i}.0/24
> done
xCreated [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/routes/kubernetes-route-10-200-0-0-24].
NAME                            NETWORK                  DEST_RANGE     NEXT_HOP     PRIORITY
kubernetes-route-10-200-0-0-24  kubernetes-the-hard-way  10.200.0.0/24  10.240.0.20  1000                                                                                                                                              Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/routes/kubernetes-route-10-200-1-0-24].
NAME                            NETWORK                  DEST_RANGE     NEXT_HOP     PRIORITY
kubernetes-route-10-200-1-0-24  kubernetes-the-hard-way  10.200.1.0/24  10.240.0.21  1000
Created [https://www.googleapis.com/compute/v1/projects/velvety-column-233813/global/routes/kubernetes-route-10-200-2-0-24].
NAME                            NETWORK                  DEST_RANGE     NEXT_HOP     PRIORITY
kubernetes-route-10-200-2-0-24  kubernetes-the-hard-way  10.200.2.0/24  10.240.0.22  1000
root@master:~#
root@master:~# gcloud compute routes list --filter "network: kubernetes-the-hard-way"
NAME                            NETWORK                  DEST_RANGE     NEXT_HOP                  PRIORITY
default-route-289855be5cac8eaf  kubernetes-the-hard-way  0.0.0.0/0      default-internet-gateway  1000
default-route-c2f773c593c64514  kubernetes-the-hard-way  10.240.0.0/24  kubernetes-the-hard-way   1000
kubernetes-route-10-200-0-0-24  kubernetes-the-hard-way  10.200.0.0/24  10.240.0.20               1000
kubernetes-route-10-200-1-0-24  kubernetes-the-hard-way  10.200.1.0/24  10.240.0.21               1000
kubernetes-route-10-200-2-0-24  kubernetes-the-hard-way  10.200.2.0/24  10.240.0.22               1000
root@master:~#
root@master:~# kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns.yaml
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.extensions/coredns created
service/kube-dns created
root@master:~#
root@master:~# kubectl get pods -l k8s-app=kube-dns -n kube-system
NAME                       READY   STATUS              RESTARTS   AGE
coredns-699f8ddd77-4g9st   0/1     ContainerCreating   0          8s
coredns-699f8ddd77-5kjnr   0/1     ContainerCreating   0          8s
root@master:~# kubectl get pods -l k8s-app=kube-dns -n kube-system
NAME                       READY   STATUS    RESTARTS   AGE
coredns-699f8ddd77-4g9st   1/1     Running   0          50s
coredns-699f8ddd77-5kjnr   1/1     Running   0          50s
root@master:~#
root@master:~# kubectl run busybox --image=busybox:1.28 --command -- sleep 3600
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/busybox created
root@master:~# kubectl get pods -l run=busybox
NAME                      READY   STATUS              RESTARTS   AGE
busybox-bd8fb7cbd-ccb65   0/1     ContainerCreating   0          5s
root@master:~# kubectl get pods -l run=busybox -w
NAME                      READY   STATUS    RESTARTS   AGE
busybox-bd8fb7cbd-ccb65   1/1     Running   0          9s
^Croot@master:~#
root@master:~# POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
root@master:~# kubectl exec -ti $POD_NAME -- nslookup kubernetes
Server:    10.32.0.10
Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes
Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local
root@master:~#

root@master:~# kubectl create secret generic kubernetes-the-hard-way \
>   --from-literal="mykey=mydata"
secret/kubernetes-the-hard-way created
root@master:~#
root@master:~# gcloud compute ssh controller-0 \
>   --command "sudo ETCDCTL_API=3 etcdctl get \
>   --endpoints=https://127.0.0.1:2379 \
>   --cacert=/etc/etcd/ca.pem \
>   --cert=/etc/etcd/kubernetes.pem \
>   --key=/etc/etcd/kubernetes-key.pem\
>   /registry/secrets/default/kubernetes-the-hard-way | hexdump -C"
00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6b 75 62 65 72 6e  |s/default/kubern|
00000020  65 74 65 73 2d 74 68 65  2d 68 61 72 64 2d 77 61  |etes-the-hard-wa|
00000030  79 0a 6b 38 73 3a 65 6e  63 3a 61 65 73 63 62 63  |y.k8s:enc:aescbc|
00000040  3a 76 31 3a 6b 65 79 31  3a 53 d3 4b 4d f3 8b e2  |:v1:key1:S.KM...|
00000050  30 d6 bd 9e 1f 27 e5 1f  1a d5 82 fd 10 eb af 13  |0....'..........|
00000060  3a 24 2d c7 1b 43 61 41  dc 28 5e 54 0c 85 5a 8f  |:$-..CaA.(^T..Z.|
00000070  9d 3d ed 9e f0 49 ec cc  e3 86 ff 5f de 52 a6 75  |.=...I....._.R.u|
00000080  49 ef 3d 54 3a 79 d2 00  a6 c9 83 26 63 5f c5 cf  |I.=T:y.....&c_..|
00000090  f3 54 d3 38 4a 0f 10 87  bb c4 b0 fe 1e 71 e9 6a  |.T.8J........q.j|
000000a0  68 c8 31 8f 2a 85 ff 5b  46 57 d5 79 d3 dc 5f f7  |h.1.*..[FW.y.._.|
000000b0  03 4a 14 77 73 e7 7c 3e  13 a4 0f 5a fb ff 79 b2  |.J.ws.|>...Z..y.|
000000c0  57 cc 85 7b 31 82 7a 4d  09 70 ad 3d bb bb 29 78  |W..{1.zM.p.=..)x|
000000d0  d4 0d 51 2d 46 1c dd 5e  d1 4f a1 1d 93 82 e3 3d  |..Q-F..^.O.....=|
000000e0  e0 46 07 0a c1 20 b7 4f  52 0a                    |.F... .OR.|
000000ea




